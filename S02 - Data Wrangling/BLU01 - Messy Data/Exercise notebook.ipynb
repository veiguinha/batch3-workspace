{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b6b75485105e36c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# BLU01 - Exercises Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-26d8a5f531043e66",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Important Note**\n",
    "\n",
    "When reading files, please use `os.path.join()`. Specially if you are a Windows user!\n",
    "The grader is running on Linux, reading a file my be running locally for you, but then don't run in the grader because of all the weird backslashes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0240afddd4fae69d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import chardet\n",
    "import hashlib # for grading purposes\n",
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-93e0b4e1a40ebaaa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q1: Use a shell command to keep in 2 variables the first third and the last third of the lines existent in a file\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-05a3d2245d57305d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Use the file data/exercises/elon_musk.txt\n",
    "# Start by counting the total lines and add the total to the variable count_total.  \n",
    "# count_total = ! type data\\exercises\\elon_musk.txt | find /c /v \"\"\n",
    "count_total = ! wc -l < ./data/exercises/elon_musk.txt\n",
    "count_total = int(count_total[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2e69c6137e73d90c",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "expected_hash = '4ec9599fc203d176a301536c2e091a19bc852759b255bd6818810a42c5fed14a'\n",
    "assert hashlib.sha256(str(count_total).encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14cf2619322a8763",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# NOT GRADED optional exercise!!\n",
    "# Now add the first third and the last third of the lines to the variables first_third and last_third. \n",
    "# Make it work to any files using count_total/3 for instance in the bash commands\n",
    "# first_third = ...\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# last_third = ...\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6556340d1db98d7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Test the `first_third` and `last_third` exercise by running the following (ungraded) asserts.\n",
    "\n",
    "```\n",
    "assert first_third[-1][0] == 'H'\n",
    "assert last_third[0][0] == 'N'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e27cb8588bb5fd40",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q2: Read a file with specific delimiter\n",
    "\n",
    "Read file **data/exercises/euribor_interest_rates.csv** into a pandas DataFrame.\n",
    "\n",
    "First, you should preview the file using a shell command in order to find out the used delimiter, and other properties of this file.\n",
    "\n",
    "Then, you should use function read_csv to read the data into a DataFrame. The resulting DataFrame should have the last column as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-942e0590467e20d9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euribor 3 months|Euribor 6 months|Euribor 12 months|Years\n",
      "3,34|3,52|3,88|1999\n",
      "4,86|4,83|4,75|2000\n",
      "3,29|3,26|3,34|2001\n",
      "2,87|2,80|2,75|2002\n",
      "2,12|2,17|2,31|2003\n",
      "2,16|2,22|2,36|2004\n",
      "2,49|2,64|2,84|2005\n",
      "3,73|3,85|4,03|2006\n",
      "4,68|4,71|4,75|2007\n",
      "2,89|2,97|3,05|2008\n",
      "0,70|0,99|1,25|2009\n",
      "1,01|1,23|1,51|2010\n",
      "1,36|1,62|1,95|2011\n",
      "0,19|0,32|0,54|2012\n",
      "0,29|0,39|0,56|2013\n",
      "0,08|0,17|0,33|2014\n",
      "-0,13|-0,04|0,06|2015\n",
      "-0,32|-0,22|-0,08|2016\n",
      "-0,33|-0,27|-0,19|2017\n"
     ]
    }
   ],
   "source": [
    "# Use a shell command to preview the data\n",
    "!  type data\\exercises\\euribor_interest_rates.csv\n",
    "\n",
    "# Use function read_csv to read the data into a DataFrame\n",
    "df2 = pd.read_csv(os.path.join('data','exercises','euribor_interest_rates.csv'), sep='|', index_col='Years')\n",
    "for column in df2.columns:\n",
    "    df2[column] = df2[column].str.replace(',','.')\n",
    "    df2[column] = df2[column].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0c275c9acaa9c74d",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert df2.loc[2001, 'Euribor 3 months'] == 3.29\n",
    "assert set(df2.columns) == {'Euribor 3 months', 'Euribor 6 months', 'Euribor 12 months'}\n",
    "assert len(df2) == 19\n",
    "assert df2.index[0] == 1999\n",
    "\n",
    "expected_hash = 'b68ca0811f132f73edc68e9d3bebb288ef036c1fef8aabe6d2c63a2b6bfa859c'\n",
    "assert hashlib.sha256(df2.index.name.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6f9efef818e4a83",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q3: Read a csv file with problems\n",
    "\n",
    "Read file **data/exercises/portugal_urban_waste_per_inhabitant.csv** using function `read_csv`. Pay attention to the following:\n",
    "* you might find some trouble when reading the file, at first, then just ignore the problems ;)\n",
    "* use the first column as index\n",
    "* there are some inputs in the file that should be interpreted as NaN, make sure you select the right one when reading the file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2771b707556c08d2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years,Urban waste collection per inhabitant (kg/inhabitant),Selective urban waste collection per inhabitant (kg/inhabitant)\n",
      "1989,no-data,no-data\n",
      "1990,480,78.2,1990,,\n",
      "1991,425.7,1.5\n",
      "1992,999999,1.7\n",
      "1993,357.6,2.3\n",
      "1994,999999,999999\n",
      "1995,352.0,4.0\n",
      "1996,371.7,5.3\n",
      "1997,397.0,6.6\n",
      "1998,413.2,8.1\n",
      "1999,433.3,10.6\n",
      "2000,457.2,15.3\n",
      "2001,454.4,18.3\n",
      "2002,441.0,20.4\n",
      "2003,448.7,21.7\n",
      "2004,445.0,30.5\n",
      "2005,451.8,40.5\n",
      "2006,465.5,48.1\n",
      "2007,471.1,54.6\n",
      "2008,518.3,60.3\n",
      "2009,520.1,66.5\n",
      "2010,516.1,76.2\n",
      "2011,490.4,71.4\n",
      "2012,453.3,63.3\n",
      "2013,439.7,56.3\n",
      "2014,452.9,61.4\n",
      "2015,460.4,70.8\n",
      "2016,460.9,75.1\n",
      "2017,480,78.2,,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 3: expected 3 fields, saw 6\\nSkipping line 30: expected 3 fields, saw 5\\n'\n"
     ]
    }
   ],
   "source": [
    "# Read file data/exercises/portugal_urban_waste_per_inhabitant.csv with read_csv\n",
    "! type data\\exercises\\portugal_urban_waste_per_inhabitant.csv\n",
    "\n",
    "# We have row with an extra element \n",
    "# We have a kind of no_value_data\n",
    "# The column names are quite extensive\n",
    "\n",
    "df3 = pd.read_csv(os.path.join('data','exercises','portugal_urban_waste_per_inhabitant.csv'),\n",
    "                  error_bad_lines=False\n",
    "                  ,index_col='Years',na_values={'Urban waste collection per inhabitant (kg/inhabitant)':['no-data','999999'],\n",
    "                                              'Selective urban waste collection per inhabitant (kg/inhabitant)':['no-data','999999']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-601b354802964776",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isnan(df3.loc[1994, 'Urban waste collection per inhabitant (kg/inhabitant)'])\n",
    "assert df3.loc[2011, 'Selective urban waste collection per inhabitant (kg/inhabitant)'] == 71.4\n",
    "\n",
    "mean_selective_waste = df3['Selective urban waste collection per inhabitant (kg/inhabitant)'].mean()\n",
    "assert math.isclose(35.632, mean_selective_waste, rel_tol=1e-3)\n",
    "\n",
    "expected_hash = 'b68ca0811f132f73edc68e9d3bebb288ef036c1fef8aabe6d2c63a2b6bfa859c'\n",
    "assert hashlib.sha256(df3.index.name.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5cfd5a97b1c49a0e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q4: Repair a csv file after importing\n",
    "Read the same file **data/exercises/portugal_urban_waste_per_inhabitant.csv** using function `read_csv`. But now, be sure you don't miss any lines with relevant information! \n",
    "\n",
    "* use csv module to import everything to a list of lists\n",
    "* create a df with only 3 meaningful columns, where the `Years` should be index\n",
    "* replace garbage values with NaN's \n",
    "* format the columns with the right type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-119041388e17fb72",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Read file data/exercises/portugal_urban_waste_per_inhabitant.csv using  \n",
    "file = open(os.path.join('data','exercises','portugal_urban_waste_per_inhabitant.csv'),'r')\n",
    "n_elements = 3\n",
    "lines = list(csv.reader(file))\n",
    "\n",
    "lines = [i[:n_elements] for i in lines]\n",
    "file.close()\n",
    "# create a dataframe using the line list with only 3 columns\n",
    "df4 = pd.DataFrame(lines[1:], columns=lines[0])\n",
    "\n",
    "#replace invalid values with nan (np.nan)\n",
    "for column in df4.columns:\n",
    "    df4[column] = df4[column].replace('no-data',np.nan)\n",
    "    df4[column] = df4[column].replace('999999',np.nan)\n",
    "    #set types per dataframe column\n",
    "    if column == 'Years':\n",
    "        df4[column] = df4[column].astype('int64')\n",
    "    #set types per dataframe column\n",
    "    else:\n",
    "        df4[column] = df4[column].astype(float)\n",
    "\n",
    "df4[column] = df4[column].astype(float)    \n",
    "#set a new index to the dataframe\n",
    "df4 = df4.set_index('Years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a42299f9047590d5",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isnan(df4.loc[1994, 'Urban waste collection per inhabitant (kg/inhabitant)'])\n",
    "assert df4.loc[2012, 'Selective urban waste collection per inhabitant (kg/inhabitant)'] == 63.3\n",
    "\n",
    "mean_selective_waste = df4['Selective urban waste collection per inhabitant (kg/inhabitant)'].mean()\n",
    "assert math.isclose(38.78518518518518, mean_selective_waste, rel_tol=1e-3)\n",
    "\n",
    "expected_hash = 'b68ca0811f132f73edc68e9d3bebb288ef036c1fef8aabe6d2c63a2b6bfa859c'\n",
    "assert hashlib.sha256(df4.index.name.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-389bc42fe462c70e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q5: Read a JSON file\n",
    "\n",
    "Read file **data/exercises/portugal_production_of_electricity_gwh.json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c2125479f7e50e5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Read file data/exercises/portugal_production_of_electricity_gwh.json with read_json\n",
    "df5 = pd.read_json(os.path.join('data','exercises','portugal_production_of_electricity_gwh.json'),orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3f085fa2cdad9319",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(df5) == 23\n",
    "assert set(df5.columns) == {'Biomass', 'Geothermal power', 'Hydropower < 10MW', 'Hydropower > 10MW', \n",
    "                           'Photovoltaic', 'Total','Total renewable sources','Windpower','Year'}\n",
    "\n",
    "expected_hash = 'c8226c54f1a24ae847c02a3e7a7d6e9fb44c2f82eb6f1fddcee9092c434e67fb'\n",
    "assert hashlib.sha256(str(df5.loc[:,'Hydropower > 10MW'].sum()).encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d4b970f55b3e427",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q6: Read an Excel file\n",
    "\n",
    "Read file **data/exercises/portugal_gas_emissions_per_year.xlsx** using function read_excel. Pay attention to the following:\n",
    "\n",
    "* you should grab the table \"Series\" in sheet \"Metadata\"\n",
    "* use column 'Serie' as index\n",
    "* make sure you keep only the rows with data\n",
    "* set the variable distinct_scales with the number of ... distinct scales found in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-017a7c31b0ddc38e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Read file data/exercises/portugal_gas_emissions_per_year.xlsx with read_excel\n",
    "df6 = pd.read_excel(os.path.join('data','exercises','portugal_gas_emissions_per_year.xlsx'),sheet_name='Metadata',\n",
    "                   skiprows=21,header=1,skipfooter=6, index_col='Serie').drop(['Unnamed: 5','Unnamed: 6','Unnamed: 7'],axis=1)\n",
    "\n",
    "# distinct_scales = df6['Scale'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Measure Unit</th>\n",
       "      <th>Value Type</th>\n",
       "      <th>Scale</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serie</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Carbon dioxide from fossil origin</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>10^3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carbon dioxide from biomass</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>10^3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nitrous oxide</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Methane</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ammonia</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-methane volatile organic compounds</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carbon monoxide</th>\n",
       "      <td>t (tonne)</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nitrogen oxides</th>\n",
       "      <td>t NO2eq</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>t NO2eq = tonne of nitrogen dioxide equivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sulphur oxides</th>\n",
       "      <td>t SO2eq</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>t SO2eq = tonne of sulphur dioxide equivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hydrofluorcarbons</th>\n",
       "      <td>t CO2eq</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>t CO2eq = tonne of carbon dioxide equivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sulphur hexafluoride</th>\n",
       "      <td>t CO2eq</td>\n",
       "      <td>Absolute Value</td>\n",
       "      <td>N.º</td>\n",
       "      <td>t CO2eq = tonne of carbon dioxide equivalent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Measure Unit      Value Type Scale  \\\n",
       "Serie                                                                       \n",
       "Carbon dioxide from fossil origin         t (tonne)  Absolute Value  10^3   \n",
       "Carbon dioxide from biomass               t (tonne)  Absolute Value  10^3   \n",
       "Nitrous oxide                             t (tonne)  Absolute Value   N.º   \n",
       "Methane                                   t (tonne)  Absolute Value   N.º   \n",
       "Ammonia                                   t (tonne)  Absolute Value   N.º   \n",
       "Non-methane volatile organic compounds    t (tonne)  Absolute Value   N.º   \n",
       "Carbon monoxide                           t (tonne)  Absolute Value   N.º   \n",
       "Nitrogen oxides                             t NO2eq  Absolute Value   N.º   \n",
       "Sulphur oxides                              t SO2eq  Absolute Value   N.º   \n",
       "Hydrofluorcarbons                           t CO2eq  Absolute Value   N.º   \n",
       "Sulphur hexafluoride                        t CO2eq  Absolute Value   N.º   \n",
       "\n",
       "                                                                                 Notes  \n",
       "Serie                                                                                   \n",
       "Carbon dioxide from fossil origin                                                  NaN  \n",
       "Carbon dioxide from biomass                                                        NaN  \n",
       "Nitrous oxide                                                                      NaN  \n",
       "Methane                                                                            NaN  \n",
       "Ammonia                                                                            NaN  \n",
       "Non-methane volatile organic compounds                                             NaN  \n",
       "Carbon monoxide                                                                    NaN  \n",
       "Nitrogen oxides                         t NO2eq = tonne of nitrogen dioxide equivalent  \n",
       "Sulphur oxides                           t SO2eq = tonne of sulphur dioxide equivalent  \n",
       "Hydrofluorcarbons                         t CO2eq = tonne of carbon dioxide equivalent  \n",
       "Sulphur hexafluoride                      t CO2eq = tonne of carbon dioxide equivalent  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-91d5f07e40585680",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert distinct_scales == 2\n",
    "assert isinstance(df6, pd.DataFrame)\n",
    "expected_hash = '60c3ad36f77e7366103fe36a3f551a0cde7d64e26d3102ddb8b953d6208a6006'\n",
    "assert hashlib.sha256(\n",
    "        df6.loc[\n",
    "            df6.index==\"Nitrogen oxides\", \n",
    "            \"Measure Unit\"][0].encode()\n",
    "    ).hexdigest() == expected_hash\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a7407517d4d92c3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q7: Find the encoding of a file\n",
    "\n",
    "Find the encoding used in file **data/exercises/cities.csv**, using the method that was shown in the Learning Units.\n",
    "\n",
    "Then, read the data into a DataFrame, using the read_csv method and find the `City` characters that has distance equal to 7503."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bd6944fd63bb38d8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Find the encoding of file data/exercises/mystery_cities.csv\n",
    "encoding = list(chardet.detect(open(os.path.join('data','exercises','cities.csv'),'rb').read()).values())[0]\n",
    "# Read the file into a DataFrame\n",
    "df7 = pd.read_csv('data/exercises/cities.csv',encoding=encoding,index_col='distance')\n",
    "df7.head()\n",
    "\n",
    "# Find the name of the city with distance = 7503\n",
    "city_found = df7.loc[7503,'City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-687fb83c97e03355",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(df7, pd.DataFrame)\n",
    "\n",
    "expected_hash_1 = '969aef39a1d4cb1c5928c774cd7a4e3ccfc064a18fbd43a70193a2631d8a122d'\n",
    "assert hashlib.sha256(encoding.encode()).hexdigest() == expected_hash_1\n",
    "\n",
    "expected_hash_2 = '8086d7adc029756c1b9094df64f6e79017dcc5a99c3c5e55b47beede11179a2b'\n",
    "assert hashlib.sha256(city_found.encode()).hexdigest() == expected_hash_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d103dfae3d5e11fe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q8: Import a Random Sample of a Big File\n",
    "\n",
    "Consider the file **data/exercises/world_percentage_of_literacy.tsv**. Let's imagine this file is really huge, with a lot of rows!  Read the file using a random sample of 7 rows. Count the actual lines with `wc` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb39107093dd73fb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Read file data/exercises/world_percentage_of_literacy.tsv with wc and save the number of lines\n",
    "# in lines_in_file. Notice that the header is not a line..\n",
    "# lines_in_file = ! type data\\exercises\\world_percentage_of_literacy.tsv | find /c /v \"\"\n",
    "lines_in_file = ! wc -l < ./data/exercises/world_percentage_of_literacy.tsv\n",
    "lines_in_file = int(lines_in_file[0])-1\n",
    "lines_in_file\n",
    "\n",
    "# don't forget to close the opened file\n",
    "f = open(os.path.join('data','exercises','world_percentage_of_literacy.tsv'),'r')\n",
    "lines = list(csv.reader(f))\n",
    "f.close()\n",
    "\n",
    "# make parameter rows_to_skip equal to the lines you want to skip loading \n",
    "sample_number = 7\n",
    "# don't forget: \n",
    "# - 7 rows should be fecthed)\n",
    "# - you want to keep the header plus 7 rows in the new dataframe...\n",
    "n_rows_to_skip = lines_in_file - sample_number\n",
    "rows_to_skip = random.sample(range(1,lines_in_file-1),n_rows_to_skip)\n",
    "\n",
    "# Create a df8 file with the sampled values\n",
    "df8 = pd.read_csv('data/exercises/world_percentage_of_literacy.tsv', sep=\"\\t\", skiprows=rows_to_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ec15a8be42137a01",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# getting remaining list of countries, removing rows_to_skip from the file\n",
    "df_helper = pd.read_csv( \n",
    "    'data/exercises/world_percentage_of_literacy.tsv', \n",
    "    sep='\\t',\n",
    "    header=0 \n",
    ")\n",
    "\n",
    "total_indexes = list(df_helper.index)\n",
    "for s in rows_to_skip:\n",
    "    total_indexes.remove(s-1)\n",
    "\n",
    "assert lines_in_file==194\n",
    "assert isinstance(df8, pd.DataFrame)\n",
    "assert list(df8['Country'])==list(df_helper.iloc[total_indexes]['Country'])\n",
    "assert df8.shape[0]==7\n",
    "\n",
    "expected_hash = '7902699be42c8a8e46fbbb4501726517e86b22c56a189f7625a6da49081b2451'\n",
    "assert  hashlib.sha256(str(df8.shape[0]).encode()).hexdigest() == expected_hash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f87e07a7b2cacd9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q9: Loading a Big File\n",
    "\n",
    "Read file **data/exercises/world_percentage_of_literacy.tsv** using chunks keep only the columns `Country` and `Literacy rate (all)`.\n",
    "Note that:\n",
    "* file should be read by chunks of 10 countries\n",
    "* the missing values should be removed (filtered in each chunk)\n",
    "* the `Literacy rate (all)` should be converted to type float (in each chunk)\n",
    "* the index should be incremental starting from 0 (i.e, you don't need to read any column as the index)\n",
    "\n",
    "In the end calculate the average `Literacy rate (all)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5106c25a705296e8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Read file data/exercises/world_percentage_of_literacy.tsv\n",
    "# the chunks should be appended in a list called chunk_arr\n",
    "chunks_iter = pd.read_csv(os.path.join('data','exercises','world_percentage_of_literacy.tsv'),sep = '\\t', chunksize=10)\n",
    "chunk_arr = []\n",
    "\n",
    "for data_chunk in chunks_iter:\n",
    "    data_chunk = data_chunk[['Country','Literacy rate (all)']]\n",
    "    data_chunk['Literacy rate (all)'] = data_chunk['Literacy rate (all)'].replace('not reported by UNESCO 2015',np.nan)\n",
    "    data_chunk = data_chunk.dropna(axis=0)\n",
    "    data_chunk['Literacy rate (all)'] = data_chunk['Literacy rate (all)'].str.replace('%','')\n",
    "    data_chunk['Literacy rate (all)'] = data_chunk['Literacy rate (all)'].str.split('[').str.get(0)\n",
    "    data_chunk['Literacy rate (all)'] = data_chunk['Literacy rate (all)'].str.split(' ').str.get(0)\n",
    "    data_chunk['Literacy rate (all)'] = data_chunk['Literacy rate (all)'].astype(float)\n",
    "    chunk_arr.append(data_chunk)\n",
    "\n",
    "# df9 should be the final dataframe with concatenated chunks\n",
    "# Resulting average should go on lit_avg variable\n",
    "df9 = pd.concat(chunk_arr, axis=0)\n",
    "df9 = df9.set_index(np.arange(len(df9)))\n",
    "lit_avg = df9['Literacy rate (all)'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f60a22c3465d5b83",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert df9.loc[df9['Country']=='World', 'Literacy rate (all)'].values[0] == 86.3\n",
    "assert df9.dtypes['Country'] == np.object\n",
    "assert df9.dtypes['Literacy rate (all)'] == np.float\n",
    "\n",
    "expected_hash = 'f26cd8ca964afd7aaaea0cacb142419676cf9928772f5b5310a036ffdae1586a'\n",
    "assert hashlib.sha256(str([len(c) for c in chunk_arr]).encode()).hexdigest() == expected_hash\n",
    "\n",
    "expected_hash = '94f2bba3a658b5642ebbc9b952af45c3db1a185ae0357e4fe7ced784f0c3fe29'\n",
    "assert hashlib.sha256(str(lit_avg).encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e990312f7cddd0b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q10: Calculate average values using chunks and avoiding a complete data frame in memory.\n",
    "\n",
    "Using chunks, read file **data/exercises/world_percentage_of_literacy.tsv**, avoid incompleted rows and calculate the average of ***Literacy rate (all)*** without loading all data simultaneously. Use a similar approach of the previous question but don't create any dataframe neither any list with chunks;\n",
    "\n",
    "***Hint: Use the average definition***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57fd7700885246eb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Read file data/exercises/world_percentage_of_literacy.tsv\n",
    "dfs = pd.read_csv(os.path.join('data','exercises','world_percentage_of_literacy.tsv'),sep = '\\t', chunksize=10)\n",
    "# the final average should be in the variable final_avg\n",
    "# You should increment 2 variables in each chunk and use them at the end to calculate final_avg. call them lit_a, lit_b\n",
    "lit_a = 0\n",
    "lit_b = 0\n",
    "for data_chunk in dfs:\n",
    "    data_chunk = data_chunk[['Literacy rate (all)']]\n",
    "    data_chunk['Literacy rate (all)'] = data_chunk['Literacy rate (all)'].replace('not reported by UNESCO 2015',np.nan)\n",
    "    data_chunk = data_chunk.dropna(axis=0)\n",
    "    data_chunk['Literacy rate (all)'] = data_chunk['Literacy rate (all)'].apply(lambda x: x.replace('%',''))\n",
    "    data_chunk['Literacy rate (all)'] = data_chunk['Literacy rate (all)'].astype(str).str.split('[').str.get(0)\n",
    "    data_chunk['Literacy rate (all)'] = data_chunk['Literacy rate (all)'].astype(str).str.split(' ').str.get(0)\n",
    "    data_chunk['Literacy rate (all)'] = data_chunk['Literacy rate (all)'].astype(float)\n",
    "    lit_a += data_chunk['Literacy rate (all)'].sum()\n",
    "    lit_b += len(data_chunk['Literacy rate (all)'])\n",
    "\n",
    "final_avg = lit_a/lit_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0edcf1260d7886c9",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(83.24370860927154, final_avg, rel_tol=1e-1)\n",
    "assert lit_b==151 or lit_a==151\n",
    "assert int(lit_b) == 12569 or int(lit_a)==12569"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
