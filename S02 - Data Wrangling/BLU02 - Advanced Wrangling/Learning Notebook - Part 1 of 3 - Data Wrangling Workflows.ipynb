{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU02 - Learning Notebook - Data wrangling workflows - Part 1 of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the BLU \n",
    "\n",
    "## Data wrangling workflows\n",
    "\n",
    "A typical data science workflow goes as follows: you get data from a source, you clean it, and then you continuously iterate on it.\n",
    "\n",
    "![data_transformation_workflow](./media/data_processing_workflow.png)\n",
    "\n",
    "On the previous learning unit in this specialization, we focused mostly on getting and cleaning data (the blue boxes above). \n",
    "\n",
    "At this point, we got a dataset and performed necessary data cleaning. Our data is, therefore, in an interim state:\n",
    "* You have a *tidy dataset* (observations as rows and features as columns), comprised of one or more tables\n",
    "* You know how to import such tables into Pandas, regardless of the format they are stored.\n",
    "\n",
    "Now, to explore, visualize and model the data, we have to perform transformations on it with agility, balancing:\n",
    "* Speed of iteration, testing different hypothesis fast and easy\n",
    "* Consistency, ensuring our pipeline doesn't collapse along the way.\n",
    "\n",
    "In this first part, we'll go into how to transform the dataset and explore it in Pandas.\n",
    "\n",
    "Then, we'll zoom in on a how to combine dataframes.\n",
    "\n",
    "Finally, we'll move to scikit-learn to build efficient pipelines for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the data\n",
    "\n",
    "The New York Philharmonic played its first concert on December 7, 1842.\n",
    "\n",
    "The data documents all known concerts, amounting to more than 20,000 performances. Some considerations:\n",
    "* The Program is the top-most level element in the dataset\n",
    "* A Program is defined as performances in which the repertoire, conductors, and soloists are the same\n",
    "* A Program is associated with an Orchestra (e.g., New York Philharmonic) and a Season (e.g., 1842-43)\n",
    "* A Program may have multiple Concerts with different dates, times and locations\n",
    "* A Program's repertoire may contain various Works (e.g., two different symphonies by Beethoven)\n",
    "* A Work can have multiple Soloists (e.g., Mahler on the harpsichord, Strauss or Bernstein on the piano).\n",
    "\n",
    "**For more information about the dataset, including the data dictionary, please head to the README.**\n",
    "\n",
    "In this unit, we will be using Works and Concerts, imported as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "works = pd.read_csv('./data/works.csv')\n",
    "concerts = pd.read_csv('./data/concerts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concerts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data transformation\n",
    "\n",
    "## 1.1 Transformations as functions \n",
    "\n",
    "Most data transformations operate on dataframes: they receive a local dataframe, transform it and return a new one.\n",
    "\n",
    "In its simplest form, this is the signature of a generic data transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformer(df):\n",
    "    df = df.copy()\n",
    "    # df = ...\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such transformations have no side effects and operate as functions on immutable data (i.e., they keep the original dataframe unchanged).\n",
    "\n",
    "Since the output depends only on the arguments, calling them with the same arguments always produces the same result.\n",
    "\n",
    "Confusing? Not really."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column(df, new_name, old_name):\n",
    "    df = df.copy()\n",
    "    df[new_name] = df[old_name]\n",
    "    df = df.drop(columns=old_name)\n",
    "    return df\n",
    "\n",
    "def test_dataframe():\n",
    "    data = np.random.randn(6, 4)\n",
    "    columns = ['A', 'B', 'C', 'D']\n",
    "    return pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "df = test_dataframe()\n",
    "\n",
    "rename_column(df, 'Z', 'A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_column(df, 'Z', 'A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result, see! The program (or Notebook) remembers nothing but the original data and the function itself: a white canvas!\n",
    "\n",
    "What about the original dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each call, the program *state* is the same as it was before (no new objects, no changes, no nothing!), as if nothing happened.\n",
    "\n",
    "This property is valid for as long as we don't explicitly overwrite the original dataframe outside the function, using an assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_dataframe()\n",
    "df = rename_column(df, 'Z', 'A')\n",
    "\n",
    "try:\n",
    "    rename_column(df, 'Z', 'A')\n",
    "except:\n",
    "    print(\"For some reason this doesn't work. Why is that?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutable data is dangerous because it makes programs unpredictable. And this is why you should avoid modifying objects after creation.\n",
    "\n",
    "Such pitfall is common in Notebooks, especially when you re-run cells, run them in a different order or restart the Kernel. (Am I right?)\n",
    "\n",
    "**Data transformation is a *pipeline***\n",
    "\n",
    "Another problem is that data transformation is about applying multiple, sequential changes to the data (i.e., a multistep process).\n",
    "\n",
    "![data_transformation_pipeline](./media/data_transformation_pipeline.png)\n",
    "\n",
    "*Fig 2. - A data transformation pipeline is a multistep process.*\n",
    "\n",
    "And once we realize this, how do we go about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_dataframe()\n",
    "\n",
    "df_renamed = rename_column(df, 'Z', 'A')\n",
    "# Code happens. Ideas are tested, hours go by.\n",
    "df_renamed_without_b = df_renamed.drop(columns='B')\n",
    "# More code happens. We keep on testing ideas, days go by.\n",
    "df_renamed_without_b_positive = df_renamed_without_b[df_renamed_without_b > 0]\n",
    "# There's a lot of code. Ideas come and go, we've been doing this for a week.\n",
    "df_renamed_without_b_positive_no_nans = df_renamed_without_b_positive.dropna(how='all')\n",
    "# Can we honestly trace back how to get from df to here? Probably not.\n",
    "df_renamed_without_b_positive_no_nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using functions instead, you concisely encapsulate everything. \n",
    "\n",
    "(Also, you spend less time naming things, unless you want to.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformer(df, how_to_dropna):\n",
    "    df = df.copy()\n",
    "    df = rename_column(df, 'Z', 'A')\n",
    "    df = df.drop(columns='B')\n",
    "    df = df[df > 0]\n",
    "    df = df.dropna(how=how_to_dropna)\n",
    "    return df\n",
    "\n",
    "data_transformer(df, how_to_dropna='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is a bad one: names are not explicit, and there are no apparent blocks of logic.\n",
    "\n",
    "Functions should organize and document our codebase (*what* you are doing and how).\n",
    "\n",
    "Using functions, immutable data and avoiding side effects is a smart choice to manage complexity and keep things understandable.\n",
    "\n",
    "Alternatively, we could structure our functions more like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    df = df.copy()\n",
    "    # df = rename_misspelled_columns(df)\n",
    "    # df = drop_unnecessary_columns(df)\n",
    "    # df = keep_only_positive_values(df)\n",
    "    # df = removemissing_values(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data transformation in Pandas\n",
    "\n",
    "Pandas provides convenient methods for most data transformation tasks, with a unified, well-known syntax and consistent interfaces.\n",
    "\n",
    "For example, we don't need to create a `rename_column()` function, since Pandas already provides a `df.rename()` method for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_dataframe()\n",
    "\n",
    "df.rename({'A': 'Z'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a recap: `df.rename()` follows our transformer signature:\n",
    "* It takes a dataframe as input \n",
    "* And returns a new one as output.\n",
    "\n",
    "This predictable input/output is what we mean by consistent interfaces! \n",
    "\n",
    "It seems very promising to build and multistep pipelines, no? What transformations can we perform this way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Subsetting columns or the index\n",
    "\n",
    "#### Take a subset of indexes or columns\n",
    "\n",
    "Pandas implements this functionality, somewhat counterintuitively, as `df.filter()`.\n",
    "\n",
    "Imagine that we want only the columns related to the work itself, excluding IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_related_columns = ['ComposerName', 'WorkTitle', 'Movement']\n",
    "# Select columns by name.\n",
    "works.filter(items=work_related_columns).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use it to subset our dataframe based on the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows containing 'Glass' on the index.\n",
    "works.set_index('ComposerName').filter(like='Glass', axis=0).reset_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Drop columns\n",
    "\n",
    "Additionally, `GUID` and `ProgramID` are pretty redundant. We can get rid of `GUID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works.drop(columns='GUID').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Group By\n",
    "\n",
    "This is a very extensive topic, and we'll just touch it's surface here, so that you know that exists and can explore it further later by your own.\n",
    "\n",
    "In case you've worked with SQL before, you'll find this very familiar :)\n",
    "\n",
    "So, in Pandas there is a process of three chained steps called split-apply-combine:\n",
    "\n",
    "- split: splitting the DataFrame into groups (this is the groupby), we need to specify the column or the columns that will be used to form the groups.\n",
    "- apply: apply a function to each group (aggregation, transformation and filtration)\n",
    "- combine: create a DataFrame with the results\n",
    "\n",
    "With this out of the way, it's time to focus on three types of functions you may want to apply:\n",
    "* Aggregation (e.g., sum, count, mean)\n",
    "* Transformation (e.g., filling missing values)\n",
    "* Filtration (e.g., discard data from underrepresented groups).\n",
    "\n",
    "We will drill-down into each one of them. For this next part, we will use be using the `concerts` data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregation** (e.g., sum, count, mean)\n",
    "\n",
    "We want to start by uncovering the most popular programs, by the number of performances.\n",
    "\n",
    "So the first step is to group our data by `ProgramID`. This returns a DataFrameGroupBy object that by itself doesn't tell us much.\n",
    "\n",
    "However, we can use the group property of the DataFrameGroupBy object to inspect the groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concerts_grouped_by_ProgramID = concerts.groupby('ProgramID')\n",
    "concerts_grouped_by_ProgramID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concerts_grouped_by_ProgramID.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to know the number of performances per program we can simply call `DataFrameGroupBy.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concerts_grouped_by_ProgramID.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result but using the all the operations together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concerts.groupby('ProgramID').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the operation above we obtain a Dataframe with the number of performances per `ProgramID`. If we want to know the most popular ones we need to use the `pd.DataFrame.nlargest()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concerts.groupby('ProgramID').size().nlargest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `GroupBy.size()` to count the number of elements in each group. A list of available methods:\n",
    "* `mean()`\n",
    "* `sum()`\n",
    "* `size()`\n",
    "* `count()`\n",
    "* `std()`\n",
    "* `var()`\n",
    "* `sem()`\n",
    "* `describe()`\n",
    "* `first()`\n",
    "* `last()`\n",
    "* `nth()`\n",
    "* `min()`\n",
    "* `max()`.\n",
    "\n",
    "Alternatively, we can use `GroupBy.agg()` as a more general method. Check out the [docs](http://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.DataFrameGroupBy.agg.html).\n",
    "\n",
    "You could even define your own aggregation function using the lambda nomenclature, we will see an example of it in the transformation topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now imagine that we want to know when it was the first performance of each program.\n",
    "We start by `group-by` the shows by `ProgramID` and then, for each group we take the `min` of the Date column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concerts.groupby('ProgramID').Date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to know when it was the last performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concerts.groupby('ProgramID').Date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformation**\n",
    "\n",
    "The inherent difference between aggregation and transformation is that the later returns an object the same size as the original input.\n",
    "\n",
    "We don't have an excellent example in our dataset, so let's use the [Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset to exemplify a possible use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df = df.assign(Target=iris.target)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we want to standardize the data, but using the statistics for each group:\n",
    "* We take the mean and the standard deviation *inside each group*\n",
    "* We want to standardize each value according to that.\n",
    "\n",
    "We implement it as a lambda function, that will be applied to each group separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = lambda x: (x - x.mean()) / x.std()\n",
    "\n",
    "df.groupby('Target').transform(zscore).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downside is that, since we are returning the entire dataframe, it can't return `Target` as the index, so we lose the column.\n",
    "\n",
    "Let's make sure we got this right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df.groupby('Target').transform(zscore)\n",
    "\n",
    "df_transformed.groupby(df['Target']).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did it! Zero mean and standard deviation equal to one *inside each group*.\n",
    "\n",
    "Now, if this use case doesn't ring a bell, think about replacing missing values with the group mean, for example. Useful, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtration**\n",
    "\n",
    "The method `DataFrameGroupBy.filter()` provides a convenient way to filter out elements that belong to underrepresented groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concerts.groupby('ProgramID').filter(lambda x: x.shape[0] > 15).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns a subset of the original dataframe, depending on a function applied to the group as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Method chaining\n",
    "\n",
    "Now we know about some of the most common individual transformations we can use. But how can we combine them?\n",
    "\n",
    "Data transformation is a pipeline, i.e., some sequential transformations, *chained* together.\n",
    "\n",
    "This chaining means that each transformation returns an object that will be consumed by the next one, and so on, in a pre-defined order.\n",
    "\n",
    "As we've seen, a familiar syntax is to declare intermediate variables for each output, used as input to the next function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let us define a simple function that make the subset of a dataframe based on a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset(df, mask):\n",
    "    return df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = works['Interval'].isnull()\n",
    "df_no_intervals = subset(works,mask)\n",
    "\n",
    "df_exclude_minor_composers = df_no_intervals.groupby('ComposerName').filter(lambda x: x.shape[0] > 10)\n",
    "\n",
    "df_work_related = df_exclude_minor_composers.filter(items=work_related_columns)\n",
    "df_work_related_no_movement = df_work_related.drop(columns='Movement')\n",
    "df_work_related_no_movement_unique = df_work_related_no_movement.drop_duplicates()\n",
    "\n",
    "works_per_composer = df_work_related_no_movement_unique.groupby('ComposerName').size()\n",
    "works_per_composer_sorted = works_per_composer.nlargest()\n",
    "works_per_composer_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These declarations are syntactic sugar: they make it easier to read and express confusing things such as data pipelines. Some downsides:\n",
    "* We need to create an extra variable per intermediate step\n",
    "* Cognitive burden of naming each variable and keeping them in mind\n",
    "* They make the code less fluid\n",
    "* They make it harder to visualize the whole picture of what your program (or Notebook) is doing\n",
    "* They are error-prone and heavily reliant on the state, which is dangerous as we've seen.\n",
    "\n",
    "What if there was an alternative?\n",
    "\n",
    "Method chaining allows invoking multiple method calls chained together in a single statement, each receiving and returning an object.\n",
    "\n",
    "This syntax has always been possible with Pandas, but more and more methods and being added that (try to guess it!):\n",
    "* Receive a dataframe\n",
    "* Return a transformed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_intervals = works['Interval'].isnull()\n",
    "df_no_intervals = subset(works,mask)\n",
    "\n",
    "df_work_related = df_no_intervals.filter(items=work_related_columns)\n",
    "\n",
    "(df_work_related.groupby('ComposerName').filter(lambda x: x.shape[0] > 10)\n",
    "                .drop(columns='Movement')\n",
    "                .drop_duplicates()\n",
    "                .groupby('ComposerName').size()\n",
    "                .nlargest()\n",
    "                .plot(kind='bar'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code flows from top to bottom, and the function parameters are always near the function. \n",
    "\n",
    "Also, you eliminate an extra variable for each intermediate steps.\n",
    "\n",
    "Now, explicitly naming things is good. Ideally, you want to chain functions that make sense together and encapsulate them in logically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_5_composers(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    no_intervals = df['Interval'].isnull()\n",
    "    df = subset(df, no_intervals)\n",
    "    \n",
    "    work_related_columns = ['ComposerName', 'WorkTitle', 'Movement']\n",
    "    \n",
    "    df = (df.filter(work_related_columns)\n",
    "            .groupby('ComposerName').filter(lambda x: x.shape[0] > 10)\n",
    "            .drop(columns='Movement')\n",
    "            .drop_duplicates()\n",
    "            .groupby('ComposerName').size()\n",
    "            .nlargest())\n",
    "    return df\n",
    "\n",
    "top_5_composers = get_top_5_composers(works)\n",
    "top_5_composers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another drawback to excessively long chains is that debugging is harder, as there no intermediate values to inspect.\n",
    "\n",
    "**Segregate your code as to avoid long chains and keep only together what belongs together.**\n",
    "\n",
    "(In case of doubt, read [The Zen of Python](https://www.python.org/dev/peps/pep-0020/) out loud ten times.)\n",
    "\n",
    "### 1.2.5 Custom methods and pipes\n",
    "\n",
    "Now, for the final trick.\n",
    "\n",
    "The function `subset()` has the exact signature we want, again:\n",
    "* It receives a dataframe\n",
    "* It return a transformed dataframe.\n",
    "\n",
    "What if Pandas had a way to include such functions in pipelines? Meet `df.pipe()`!\n",
    "\n",
    "The method `df.pipe()` allows us to include user-defined functions in method chains (aka pipelines).\n",
    "\n",
    "It works like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_work_related.pipe(subset, no_intervals)\n",
    "                .filter(items=work_related_columns)\n",
    "                .groupby('ComposerName').filter(lambda x: x.shape[0] > 10)\n",
    "                .drop(columns='Movement')\n",
    "                .drop_duplicates()\n",
    "                .groupby('ComposerName').size()\n",
    "                .nlargest()\n",
    "                .plot(kind='bar'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_5_composers(df):\n",
    "    no_intervals = df['Interval'].isnull()\n",
    "    work_related_columns = ['ComposerName', 'WorkTitle', 'Movement']\n",
    "    \n",
    "    df = df.copy()\n",
    "    df = (df.pipe(subset, no_intervals)\n",
    "            .filter(items=work_related_columns)\n",
    "            .groupby('ComposerName').filter(lambda x: x.shape[0] > 10)\n",
    "            .drop(columns='Movement')\n",
    "            .drop_duplicates()\n",
    "            .groupby('ComposerName').size()\n",
    "            .nlargest())\n",
    "    \n",
    "    return df\n",
    "\n",
    "top_5_composers = get_top_5_composers(works)\n",
    "top_5_composers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it does!\n",
    "\n",
    "Now, we have all the tools we need to build robust data transformation pipelines in Pandas.\n",
    "\n",
    "In the next Notebook, you will learn how to combine dataframes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
