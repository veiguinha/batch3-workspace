{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b88a7e663ede9739",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# BLU02 - Exercises Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f0cbbbc1eadab9f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-337eee9fcbab05b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1 Read the Programs data (graded)\n",
    "\n",
    "In this first exercise, we aim to create a single dataframe, combining all programs from all seasons.\n",
    "\n",
    "With a caveat though: **we want to include seasons after 1900**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf4694e22046237e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_programs():\n",
    "    files = os.listdir('data/programs/')\n",
    "    # Create a list with the name of all files containing programs from\n",
    "    # 1900 inclusive and onwards (just the filename, no complete path.)\n",
    "    # files_after_1900: \n",
    "    files = [file for file in files if int(file[:4]) >= 1900]\n",
    "    \n",
    "    # Create a list with the name of all .csv files.\n",
    "    # seasons: List[pd.DataFrame] = ...\n",
    "    files = [file for file in files if '.csv' in file]\n",
    "    \n",
    "    # Use pd.concat to create a single dataframe.\n",
    "    # programs: pd.DataFrame = ...\n",
    "    programs = pd.DataFrame()\n",
    "    for file in files:\n",
    "        programs = pd.concat((programs, read_season(file)) ,axis=0, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # Drop the column ProgramID.\n",
    "    programs = programs.drop(columns='ProgramID',axis=1)\n",
    "    \n",
    "    # Set the index to be the column GUID, and sort the dataframe by the index \n",
    "    #( use the DataFrame.sort_index() function).\n",
    "    # Feel free to use method chaining if you want.\n",
    "    programs = (programs.set_index('GUID')\n",
    "               .sort_index())\n",
    "    \n",
    "    return programs\n",
    "\n",
    "\n",
    "def read_season(file):\n",
    "    path = os.path.join('data', 'programs', file)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "programs = make_programs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1aa243b8e5df1a17",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert programs['Season'].min() == '1900-01'\n",
    "\n",
    "shape = str(programs.shape)\n",
    "expected_hash = '16278afb4c2032bcddc35b915f5439ef586333e2723c2ba6cfb9cc1b58eca0e1'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-673495903dcba4a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's preview the `programs` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc75b7c4c275a431",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orchestra</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GUID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0002718f-a7a0-4362-9366-92fabab4ff3c</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1928-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004749e-19e2-4c85-a51e-76a2b0987e4e</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1922-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008995b-f0ce-4bdb-b2f8-2fc9827430fe</th>\n",
       "      <td>New York Symphony</td>\n",
       "      <td>1925-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008fd59-7b87-4e87-8b42-ab5b0f8505cf</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1942-43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000c0467-d7bf-4599-8e37-c856bc13a389</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1991-92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Orchestra   Season\n",
       "GUID                                                                \n",
       "0002718f-a7a0-4362-9366-92fabab4ff3c  New York Philharmonic  1928-29\n",
       "0004749e-19e2-4c85-a51e-76a2b0987e4e  New York Philharmonic  1922-23\n",
       "0008995b-f0ce-4bdb-b2f8-2fc9827430fe      New York Symphony  1925-26\n",
       "0008fd59-7b87-4e87-8b42-ab5b0f8505cf  New York Philharmonic  1942-43\n",
       "000c0467-d7bf-4599-8e37-c856bc13a389  New York Philharmonic  1991-92"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0317b5a17a59d85f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2 Read the Concerts data (graded)\n",
    "\n",
    "Read the concerts data.\n",
    "\n",
    "Although we list all transformations step-by-step for the sake of clarity, we expect you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58bd5fde4e09e377",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_concerts(): \n",
    "    # Read concerts data and drop the ProgramID and ConcertID columns.\n",
    "    # concerts: pd.DataFrame = ...\n",
    "    concerts = (pd.read_csv('data/concerts.csv')\n",
    "               .drop(columns=['ProgramID','ConcertID']))\n",
    "    # Remember to_datetime? We need here. We need to parse the columns Date and \n",
    "    # Time. Use pd.to_datetime(...).dt.date for the Date and pd_to_datetime(..., \n",
    "    # format=%I:%M%p).dt.time for the Time.\n",
    "    concerts['Date'] = pd.to_datetime(concerts['Date']).dt.date\n",
    "    concerts['Time'] = pd.to_datetime(concerts['Time'], format='%I:%M%p').dt.time\n",
    "    return concerts\n",
    "\n",
    "\n",
    "concerts = make_concerts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1e95492b37c4c43f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(concerts.shape)\n",
    "expected_hash = 'c030586e7370b1f2c34307d5de9b921d96efa28c933e44111b121ed819f339da'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "sample = str(concerts.sample(random_state=0))\n",
    "expected_hash = '392a3db01753b02d85173c38cde95112fb5cdf06ca5a45d25f828238d56103be'\n",
    "assert hashlib.sha256(sample.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e22fb5671017b5bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GUID</th>\n",
       "      <th>EventType</th>\n",
       "      <th>Location</th>\n",
       "      <th>Venue</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38e072a7-8fc9-4f9a-8eac-3957905c0002</td>\n",
       "      <td>Subscription Season</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Apollo Rooms</td>\n",
       "      <td>1842-12-07</td>\n",
       "      <td>20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c7b2b95c-5e0b-431c-a340-5b37fc860b34</td>\n",
       "      <td>Subscription Season</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Apollo Rooms</td>\n",
       "      <td>1843-02-18</td>\n",
       "      <td>20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894e1a52-1ae5-4fa7-aec0-b99997555a37</td>\n",
       "      <td>Special</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Apollo Rooms</td>\n",
       "      <td>1843-04-07</td>\n",
       "      <td>20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34ec2c2b-3297-4716-9831-b538310462b7</td>\n",
       "      <td>Subscription Season</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Apollo Rooms</td>\n",
       "      <td>1843-04-22</td>\n",
       "      <td>20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610a4acc-94e4-4cd6-bdc1-8ad020edc7e9</td>\n",
       "      <td>Subscription Season</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Apollo Rooms</td>\n",
       "      <td>1843-11-18</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   GUID            EventType       Location  \\\n",
       "0  38e072a7-8fc9-4f9a-8eac-3957905c0002  Subscription Season  Manhattan, NY   \n",
       "1  c7b2b95c-5e0b-431c-a340-5b37fc860b34  Subscription Season  Manhattan, NY   \n",
       "2  894e1a52-1ae5-4fa7-aec0-b99997555a37              Special  Manhattan, NY   \n",
       "3  34ec2c2b-3297-4716-9831-b538310462b7  Subscription Season  Manhattan, NY   \n",
       "4  610a4acc-94e4-4cd6-bdc1-8ad020edc7e9  Subscription Season  Manhattan, NY   \n",
       "\n",
       "          Venue        Date      Time  \n",
       "0  Apollo Rooms  1842-12-07  20:00:00  \n",
       "1  Apollo Rooms  1843-02-18  20:00:00  \n",
       "2  Apollo Rooms  1843-04-07  20:00:00  \n",
       "3  Apollo Rooms  1843-04-22  20:00:00  \n",
       "4  Apollo Rooms  1843-11-18       NaT  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concerts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orchestra</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GUID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0002718f-a7a0-4362-9366-92fabab4ff3c</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1928-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004749e-19e2-4c85-a51e-76a2b0987e4e</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1922-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008995b-f0ce-4bdb-b2f8-2fc9827430fe</th>\n",
       "      <td>New York Symphony</td>\n",
       "      <td>1925-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008fd59-7b87-4e87-8b42-ab5b0f8505cf</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1942-43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000c0467-d7bf-4599-8e37-c856bc13a389</th>\n",
       "      <td>New York Philharmonic</td>\n",
       "      <td>1991-92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Orchestra   Season\n",
       "GUID                                                                \n",
       "0002718f-a7a0-4362-9366-92fabab4ff3c  New York Philharmonic  1928-29\n",
       "0004749e-19e2-4c85-a51e-76a2b0987e4e  New York Philharmonic  1922-23\n",
       "0008995b-f0ce-4bdb-b2f8-2fc9827430fe      New York Symphony  1925-26\n",
       "0008fd59-7b87-4e87-8b42-ab5b0f8505cf  New York Philharmonic  1942-43\n",
       "000c0467-d7bf-4599-8e37-c856bc13a389  New York Philharmonic  1991-92"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0653d1fbb5f9043",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3 Combine Programs and Concerts data (graded)\n",
    "\n",
    "Let's combine both dataframes into a single dataset, using an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-110744d4ad3ef2ef",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Remember that you want to join on the index of one of the dataframes.\n",
    "concerts_df = concerts.copy()\n",
    "programs_df = programs.copy()\n",
    "nyp = concerts_df.join(programs_df,how='inner',on='GUID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b7730d3cb9832724",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp.shape)\n",
    "expected_hash = 'a75738e37ac4ccf37a893a1009ba624efce9efaa7721d4319e9e078193fe8de6'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash \n",
    "\n",
    "sample = str(nyp.sample(random_state=0))\n",
    "expected_hash = 'd47ed1ab14963bb6e594ebaf8d07fc89e78e83058dc78ced57a5bf5ca200efa7'\n",
    "assert hashlib.sha256(sample.encode()).hexdigest() == expected_hash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac54633ced3ec3ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 4 Read Works and Soloists data (graded)\n",
    "\n",
    "We will read the two remaining pieces of data. \n",
    "\n",
    "Again, albeit the step-by-step description, we encourage you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-030bd11a9b6c76c8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def subset(df,mask):\n",
    "    return df[mask]\n",
    "\n",
    "def make_works():\n",
    "    # Read the works data.\n",
    "    # works: pd.DataFrame = ...\n",
    "    works = pd.read_csv('data/works.csv')\n",
    "    # Remove the Intervals (attention to the values in the Interval column).\n",
    "    remove_intervals = works['Interval'].isnull()\n",
    "    # Select the columns GUID, ComposerName, WorkTitle, Movement and ConductorName.\n",
    "    work_related_columns = ['GUID','ComposerName','WorkTitle','Movement','ConductorName']\n",
    "    works = (works.pipe(subset,remove_intervals)\n",
    "            .filter(items=work_related_columns))\n",
    "    return works\n",
    "\n",
    "\n",
    "def make_soloists():\n",
    "    # Read the soloists data and drop ProgramID, WorkID and MovementID.\n",
    "    soloists = pd.read_csv('data/soloists.csv')\n",
    "    drop_columns =  ['ProgramID','WorkID','MovementID']\n",
    "    soloists = (soloists.drop(drop_columns,axis=1))\n",
    "    return soloists\n",
    "\n",
    "\n",
    "works = make_works()\n",
    "soloists = make_soloists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2a37eb128ea7e979",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(works.shape)\n",
    "expected_hash = 'cad58aa6cd33cfa24c08a0f0f846877178ab31278f212c80b16b952d9416f883'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(soloists.shape)\n",
    "expected_hash = 'a7b0d20a45ff1344e0398eebb162af9afb8805082b0dfdcb70e9a4b78f94dd13'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb54d5d81dda97e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 5 Combine Works and Soloists (graded)\n",
    "\n",
    "Like we did for Programs and Concerts, now we combine Works and Soloists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c2e4becb723a4a3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine both dataframes, again using an inner type of join.\n",
    "# works_and_soloists : pd.DataFrame = ....\n",
    "works_and_soloists = works.merge(soloists, how='inner', on = 'GUID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-13616b91f6c53cad",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(works_and_soloists.shape)\n",
    "expected_hash = 'c0e73877aac4f3916267cb58f2f122ffef32c79039bde2ecb217fda123270d12'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-463cdbc4a6b9ab87",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 6 Combine everything (graded)\n",
    "\n",
    "The final goal here is to create a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-081458c0c0a40de2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine everything into a single dataframe.\n",
    "nyp_merged = nyp.merge(works_and_soloists,on='GUID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a0ca421b4504282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp_merged.shape)\n",
    "expected_hash = '3c25d9867a3c0134a6625087698dac6314f7c225f806e78dd259788bedcfb10b'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76e3b2877f0a18aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 7 Final transformations (graded)\n",
    "\n",
    "Now, we perform the train-test split.\n",
    "\n",
    "We also perform some final transformations on both datasets:\n",
    "* Include some date features: Year, Month, Day and Weekday\n",
    "* Drop Date, Season and GUID\n",
    "* Change the column name Orchestra to OrchestraName, for consistency with other name columns\n",
    "* Filter out composers that appear in less than 100 concerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8ecf5eba7d6c38b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # You should follow these exact steps:\n",
    "    #   1 - add_date_features, ideally using df.pipe\n",
    "    #   2 - drop Date, Season and GUID\n",
    "    #   3 - rename Orchestra to OrchestraName\n",
    "    #   4 - filter out composers with less than 100 concerts (keep the ones with >= 100 rows)\n",
    "    df = (df.pipe(add_date_features)\n",
    "         .drop(['Date','Season','GUID'],axis=1)\n",
    "         .rename(columns = {'Orchestra' : 'OrchestraName'})\n",
    "         .groupby('ComposerName').filter(lambda x: x.shape[0] >= 100))\n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    new_features = ['Year','Month','Day','Weekday']\n",
    "    for feature in new_features:\n",
    "        if feature == 'Year':\n",
    "            df[feature] = df['Date'].dt.year\n",
    "        elif feature == 'Month':\n",
    "            df[feature] = df['Date'].dt.month\n",
    "        elif feature == 'Day':\n",
    "            df[feature] = df['Date'].dt.day\n",
    "        elif feature == 'Weekday':\n",
    "            df[feature] = df['Date'].dt.weekday    \n",
    "    return df\n",
    "\n",
    "\n",
    "nyp_ = preprocess_data(nyp_merged)\n",
    "X_train, X_test = train_test_split(nyp_, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-77f754825659deb6",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp_merged.shape)\n",
    "expected_hash = '3c25d9867a3c0134a6625087698dac6314f7c225f806e78dd259788bedcfb10b'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(nyp_.shape)\n",
    "expected_hash = '31fa2b10222342d4743fa75b3a04c69945106f22fcf7473f5d1daeb84bca88b7'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(nyp_.columns.values)\n",
    "expected_hash = '7d131b98b4d7094443c094603c6db00aa20a79e49661acdefb33bf5fc1c071fa'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6daa8cde1618bc1f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "And, finally, we would be ready to explore modeling.\n",
    "\n",
    "For the next part, however, we will be using the famous [Boston House Prices Dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7cd808ac07305b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 8 Scaling features (graded)\n",
    "\n",
    "About the Boston dataset:\n",
    "\n",
    "> Each record in the database describes a Boston suburb or town. The data is from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970.\n",
    "\n",
    "The features are all numerical (real, positive):\n",
    "* **CRIM** - per capita crime rate by town\n",
    "* **ZN** - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* **INDUS** - proportion of non-retail business acres per town\n",
    "* **CHAS** - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* **NOX** - nitric oxides concentration (parts per 10 million)\n",
    "* **RM** - average number of rooms per dwelling\n",
    "* **AGE** - proportion of owner-occupied units built prior to 1940\n",
    "* **DIS** - weighted distances to five Boston employment centres\n",
    "* **RAD** - index of accessibility to radial highways\n",
    "* **TAX** - full-value property-tax rate per \\$10,000\n",
    "* **PTRATIO** - pupil-teacher ratio by town\n",
    "* **B** - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* **LSTAT** - % lower status of the population\n",
    "* **MEDV** - Median value of owner-occupied homes in \\$1000's.\n",
    "\n",
    "We want to scale all features to the same range, using `sklearn.preprocessing.MinMaxScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-520f1d8e99c15a49",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Initialize the MinMaxScaler to a [0, 5] range.\n",
    "scaler = MinMaxScaler(feature_range=(0, 5))\n",
    "\n",
    "\n",
    "# Fit on the training set and transform X_train. We expect X_train_\n",
    "# to be a dataframe **just like** X_train, only scaled. \n",
    "# X_train_: pd.DataFrame = ...\n",
    "X_train_ = X_train.copy()\n",
    "X_train_ = pd.DataFrame(scaler.fit_transform(X_train_),columns=X_train.columns)\n",
    "\n",
    "# Transform the test set.\n",
    "# X_test_: pd.DataFrame = ...\n",
    "X_test_ = X_test.copy()\n",
    "X_test_ = pd.DataFrame(scaler.transform(X_test_),columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-39a52095c35ce047",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(X_train_.shape)\n",
    "expected_hash = '6f696c7e30c15aae3f0fa4807b596cf15d28cadaf33602d8d20368f7ac921f26'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(X_train_.columns.values)\n",
    "expected_hash = 'c4e20218e7e33f0e771a608bb05ece0152f5a15fc6a0629b6c88cef7790fbfe1'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(X_test_.shape)\n",
    "expected_hash = 'aa2b4e3c1e358b4b9f21c2c86bbf1187020582395419f1a02a949d7a6efac9e4'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(X_test_.columns.values)\n",
    "expected_hash = 'c4e20218e7e33f0e771a608bb05ece0152f5a15fc6a0629b6c88cef7790fbfe1'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-458080a73da057e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 9 Build a ColumnSelector transformer (graded)\n",
    "\n",
    "There's a simple transformer that can be useful, from times to times, when modeling.\n",
    "\n",
    "What we want is to build a transformer that returns the columns we select beforehand. \n",
    "\n",
    "This transformer could be used to determine what features go into modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d8545164af32b84",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    # Implement the __init__ method.\n",
    "    def __init__(self,columns='all'):\n",
    "        self.columns = columns \n",
    "    # Our ColumnSelector must be able to receive a parameter columns.\n",
    "    # The default value for columns must be set to 'all', so we can\n",
    "    # initialize it without any explicit parameters.\n",
    "        \n",
    "    # There's no need for a fit method in this case, it does nothing.\n",
    "    # We should be able to call fit without any explicit parameters.\n",
    "    # Meaning: we should be able to call ColumnSelector.fit().\n",
    "    def fit(self,X=None,y=None):\n",
    "        return self\n",
    "\n",
    "    # Transform should return all columns if the parameter columns we\n",
    "    # passed upon initialization is equal to 'all'. If a column or a\n",
    "    # list of columns are passed, only those should be returned.\n",
    "    def transform(self,X):\n",
    "        if self.columns == 'all':\n",
    "            return X\n",
    "        else:\n",
    "            return X[self.columns]\n",
    "        \n",
    "\n",
    "cols = ['CRIM', 'DIS', 'INDUS', 'RM', 'DIS', 'TAX', 'B']\n",
    "selector = ColumnSelector(columns=cols)\n",
    "X_train__ = selector.fit_transform(X_train_)\n",
    "X_test__ = selector.transform(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6d54a8e7ed69bd97",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(ColumnSelector())\n",
    "assert(selector.fit())\n",
    "\n",
    "shape = str(X_train__.shape)\n",
    "expected_hash = '5d4f688e84beb21ec07f136c16a6cc11318d4f5de7b81bf0232e5282d9834123'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(X_train__.columns.values)\n",
    "expected_hash = '901009bce1feeeccadd8cd499664598ff9319641e55dcda17a650c13c0626604'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(X_test__.shape)\n",
    "expected_hash = '0aba1c19151f76aa2ecb00fd75be05c6f73860573972e967f3d1fe1c44ae2629'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(X_test__.columns.values)\n",
    "expected_hash = '901009bce1feeeccadd8cd499664598ff9319641e55dcda17a650c13c0626604'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f751d80b4180bcd0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 10 Building the pipeline (graded)\n",
    "\n",
    "Finally, we want to use the two transformers together and run a linear regression on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6cf45c70a05aa5c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 44.380398055911535\n",
      "MAE: 4.040328302332138\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline including:\n",
    "#   1 - 'selector', ColumSelector(columns=cols)\n",
    "#   2 - 'min_max', MinMaxScaler() with same range as above\n",
    "#   3 - 'model', LinearRegression\n",
    "pipeline = Pipeline([('selector',ColumnSelector(columns=cols)), #pre-processing\n",
    "                   ('min_max', MinMaxScaler(feature_range=(0,5))), #pre_processing\n",
    "                   ('model',LinearRegression())]) #Instatiation of a model \n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('MSE: {}'.format(mse))\n",
    "print('MAE: {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b520a77a388d20b2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(pipeline) == Pipeline\n",
    "assert type(pipeline.named_steps['selector']) == ColumnSelector\n",
    "assert type(pipeline.named_steps['min_max']) == MinMaxScaler\n",
    "assert pipeline.named_steps['min_max'].get_params()['feature_range'] == (0,5)\n",
    "assert type(pipeline.named_steps['model']) == LinearRegression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-877f9bbc108ee8be",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Exercises complete, congratulations! You are about to become a certified data wrangler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
