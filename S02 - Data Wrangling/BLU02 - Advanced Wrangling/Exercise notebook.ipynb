{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b88a7e663ede9739",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# BLU02 - Exercises Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f0cbbbc1eadab9f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-337eee9fcbab05b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1 Read the Programs data (graded)\n",
    "\n",
    "In this first exercise, we aim to create a single dataframe, combining all programs from all seasons.\n",
    "\n",
    "With a caveat though: **we want to include seasons after 1900**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf4694e22046237e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_programs():\n",
    "    files = os.listdir('data/programs/')\n",
    "    # Create a list with the name of all files containing programs from\n",
    "    # 1900 inclusive and onwards (just the filename, no complete path.)\n",
    "    # files_after_1900: List[str] = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Create a list with the name of all .csv files.\n",
    "    # seasons: List[pd.DataFrame] = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Use pd.concat to create a single dataframe.\n",
    "    # programs: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Drop the column ProgramID.\n",
    "    # programs = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Set the index to be the column GUID, and sort the dataframe by the index \n",
    "    #( use the DataFrame.sort_index() function).\n",
    "    # Feel free to use method chaining if you want.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return programs\n",
    "\n",
    "\n",
    "def read_season(file):\n",
    "    path = os.path.join('data', 'programs', file)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "programs = make_programs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1aa243b8e5df1a17",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert programs['Season'].min() == '1900-01'\n",
    "\n",
    "shape = str(programs.shape)\n",
    "expected_hash = '16278afb4c2032bcddc35b915f5439ef586333e2723c2ba6cfb9cc1b58eca0e1'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-673495903dcba4a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's preview the `programs` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc75b7c4c275a431",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "programs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0317b5a17a59d85f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2 Read the Concerts data (graded)\n",
    "\n",
    "Read the concerts data.\n",
    "\n",
    "Although we list all transformations step-by-step for the sake of clarity, we expect you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58bd5fde4e09e377",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_concerts(): \n",
    "    # Read concerts data and drop the ProgramID and ConcertID columns.\n",
    "    # concerts: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Remember to_datetime? We need here. We need to parse the columns Date and \n",
    "    # Time. Use pd.to_datetime(...).dt.date for the Date and pd_to_datetime(..., \n",
    "    # format=%I:%M%p).dt.time for the Time.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return concerts\n",
    "\n",
    "\n",
    "concerts = make_concerts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1e95492b37c4c43f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(concerts.shape)\n",
    "expected_hash = 'c030586e7370b1f2c34307d5de9b921d96efa28c933e44111b121ed819f339da'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "sample = str(concerts.sample(random_state=0))\n",
    "expected_hash = '392a3db01753b02d85173c38cde95112fb5cdf06ca5a45d25f828238d56103be'\n",
    "assert hashlib.sha256(sample.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e22fb5671017b5bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "concerts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0653d1fbb5f9043",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3 Combine Programs and Concerts data (graded)\n",
    "\n",
    "Let's combine both dataframes into a single dataset, using an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-110744d4ad3ef2ef",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Remember that you want to join on the index of one of the dataframes.\n",
    "# nyp = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b7730d3cb9832724",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp.shape)\n",
    "expected_hash = 'a75738e37ac4ccf37a893a1009ba624efce9efaa7721d4319e9e078193fe8de6'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash \n",
    "\n",
    "sample = str(nyp.sample(random_state=0))\n",
    "expected_hash = 'd47ed1ab14963bb6e594ebaf8d07fc89e78e83058dc78ced57a5bf5ca200efa7'\n",
    "assert hashlib.sha256(sample.encode()).hexdigest() == expected_hash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac54633ced3ec3ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 4 Read Works and Soloists data (graded)\n",
    "\n",
    "We will read the two remaining pieces of data. \n",
    "\n",
    "Again, albeit the step-by-step description, we encourage you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-030bd11a9b6c76c8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_works():\n",
    "    # Read the works data.\n",
    "    # works: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Remove the Intervals (attention to the values in the Interval column).\n",
    "    # works: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Select the columns GUID, ComposerName, WorkTitle, Movement and ConductorName.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return works\n",
    "\n",
    "\n",
    "def make_soloists():\n",
    "    # Read the soloists data and drop ProgramID, WorkID and MovementID.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return soloists\n",
    "\n",
    "\n",
    "works = make_works()\n",
    "soloists = make_soloists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2a37eb128ea7e979",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(works.shape)\n",
    "expected_hash = 'cad58aa6cd33cfa24c08a0f0f846877178ab31278f212c80b16b952d9416f883'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(soloists.shape)\n",
    "expected_hash = 'a7b0d20a45ff1344e0398eebb162af9afb8805082b0dfdcb70e9a4b78f94dd13'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb54d5d81dda97e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 5 Combine Works and Soloists (graded)\n",
    "\n",
    "Like we did for Programs and Concerts, now we combine Works and Soloists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c2e4becb723a4a3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine both dataframes, again using an inner type of join.\n",
    "# works_and_soloists : pd.DataFrame = ....\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-13616b91f6c53cad",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(works_and_soloists.shape)\n",
    "expected_hash = 'c0e73877aac4f3916267cb58f2f122ffef32c79039bde2ecb217fda123270d12'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-463cdbc4a6b9ab87",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 6 Combine everything (graded)\n",
    "\n",
    "The final goal here is to create a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-081458c0c0a40de2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine everything into a single dataframe.\n",
    "# nyp_merged = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a0ca421b4504282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp_merged.shape)\n",
    "expected_hash = '3c25d9867a3c0134a6625087698dac6314f7c225f806e78dd259788bedcfb10b'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76e3b2877f0a18aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 7 Final transformations (graded)\n",
    "\n",
    "Now, we perform the train-test split.\n",
    "\n",
    "We also perform some final transformations on both datasets:\n",
    "* Include some date features: Year, Month, Day and Weekday\n",
    "* Drop Date, Season and GUID\n",
    "* Change the column name Orchestra to OrchestraName, for consistency with other name columns\n",
    "* Filter out composers that appear in less than 100 concerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8ecf5eba7d6c38b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # You should follow these exact steps:\n",
    "    #   1 - add_date_features, ideally using df.pipe\n",
    "    #   2 - drop Date, Season and GUID\n",
    "    #   3 - rename Orchestra to OrchestraName\n",
    "    #   4 - filter out composers with less than 100 concerts (keep the ones with >= 100 rows)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df\n",
    "\n",
    "\n",
    "nyp_ = preprocess_data(nyp_merged)\n",
    "X_train, X_test = train_test_split(nyp_, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-77f754825659deb6",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(nyp_merged.shape)\n",
    "expected_hash = '3c25d9867a3c0134a6625087698dac6314f7c225f806e78dd259788bedcfb10b'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(nyp_.shape)\n",
    "expected_hash = '31fa2b10222342d4743fa75b3a04c69945106f22fcf7473f5d1daeb84bca88b7'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(nyp_.columns.values)\n",
    "expected_hash = '7d131b98b4d7094443c094603c6db00aa20a79e49661acdefb33bf5fc1c071fa'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6daa8cde1618bc1f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "And, finally, we would be ready to explore modeling.\n",
    "\n",
    "For the next part, however, we will be using the famous [Boston House Prices Dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7cd808ac07305b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 8 Scaling features (graded)\n",
    "\n",
    "About the Boston dataset:\n",
    "\n",
    "> Each record in the database describes a Boston suburb or town. The data is from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970.\n",
    "\n",
    "The features are all numerical (real, positive):\n",
    "* **CRIM** - per capita crime rate by town\n",
    "* **ZN** - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* **INDUS** - proportion of non-retail business acres per town\n",
    "* **CHAS** - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* **NOX** - nitric oxides concentration (parts per 10 million)\n",
    "* **RM** - average number of rooms per dwelling\n",
    "* **AGE** - proportion of owner-occupied units built prior to 1940\n",
    "* **DIS** - weighted distances to five Boston employment centres\n",
    "* **RAD** - index of accessibility to radial highways\n",
    "* **TAX** - full-value property-tax rate per \\$10,000\n",
    "* **PTRATIO** - pupil-teacher ratio by town\n",
    "* **B** - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* **LSTAT** - % lower status of the population\n",
    "* **MEDV** - Median value of owner-occupied homes in \\$1000's.\n",
    "\n",
    "We want to scale all features to the same range, using `sklearn.preprocessing.MinMaxScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-520f1d8e99c15a49",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Initialize the MinMaxScaler to a [0, 5] range.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Fit on the training set and transform X_train. We expect X_train_\n",
    "# to be a dataframe **just like** X_train, only scaled. \n",
    "# X_train_: pd.DataFrame = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Transform the test set.\n",
    "# X_test_: pd.DataFrame = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-39a52095c35ce047",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = str(X_train_.shape)\n",
    "expected_hash = '6f696c7e30c15aae3f0fa4807b596cf15d28cadaf33602d8d20368f7ac921f26'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(X_train_.columns.values)\n",
    "expected_hash = 'c4e20218e7e33f0e771a608bb05ece0152f5a15fc6a0629b6c88cef7790fbfe1'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(X_test_.shape)\n",
    "expected_hash = 'aa2b4e3c1e358b4b9f21c2c86bbf1187020582395419f1a02a949d7a6efac9e4'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(X_test_.columns.values)\n",
    "expected_hash = 'c4e20218e7e33f0e771a608bb05ece0152f5a15fc6a0629b6c88cef7790fbfe1'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-458080a73da057e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 9 Build a ColumnSelector transformer (graded)\n",
    "\n",
    "There's a simple transformer that can be useful, from times to times, when modeling.\n",
    "\n",
    "What we want is to build a transformer that returns the columns we select beforehand. \n",
    "\n",
    "This transformer could be used to determine what features go into modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d8545164af32b84",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    # Implement the __init__ method.\n",
    "    # Our ColumnSelector must be able to receive a parameter columns.\n",
    "    # The default value for columns must be set to 'all', so we can\n",
    "    # initialize it without any explicit parameters.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "    # There's no need for a fit method in this case, it does nothing.\n",
    "    # We should be able to call fit without any explicit parameters.\n",
    "    # Meaning: we should be able to call ColumnSelector.fit().\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Transform should return all columns if the parameter columns we\n",
    "    # passed upon initialization is equal to 'all'. If a column or a\n",
    "    # list of columns are passed, only those should be returned.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "\n",
    "cols = ['CRIM', 'DIS', 'INDUS', 'RM', 'DIS', 'TAX', 'B']\n",
    "selector = ColumnSelector(columns=cols)\n",
    "X_train__ = selector.fit_transform(X_train_)\n",
    "X_test__ = selector.transform(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6d54a8e7ed69bd97",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(ColumnSelector())\n",
    "assert(selector.fit())\n",
    "\n",
    "shape = str(X_train__.shape)\n",
    "expected_hash = '5d4f688e84beb21ec07f136c16a6cc11318d4f5de7b81bf0232e5282d9834123'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(X_train__.columns.values)\n",
    "expected_hash = '901009bce1feeeccadd8cd499664598ff9319641e55dcda17a650c13c0626604'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash\n",
    "\n",
    "shape = str(X_test__.shape)\n",
    "expected_hash = '0aba1c19151f76aa2ecb00fd75be05c6f73860573972e967f3d1fe1c44ae2629'\n",
    "assert hashlib.sha256(shape.encode()).hexdigest() == expected_hash\n",
    "\n",
    "columns = str(X_test__.columns.values)\n",
    "expected_hash = '901009bce1feeeccadd8cd499664598ff9319641e55dcda17a650c13c0626604'\n",
    "assert hashlib.sha256(columns.encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f751d80b4180bcd0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 10 Building the pipeline (graded)\n",
    "\n",
    "Finally, we want to use the two transformers together and run a linear regression on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6cf45c70a05aa5c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline including:\n",
    "#   1 - 'selector', ColumSelector(columns=cols)\n",
    "#   2 - 'min_max', MinMaxScaler() with same range as above\n",
    "#   3 - 'model', LinearRegression\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('MSE: {}'.format(mse))\n",
    "print('MAE: {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b520a77a388d20b2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(pipeline) == Pipeline\n",
    "assert type(pipeline.named_steps['selector']) == ColumnSelector\n",
    "assert type(pipeline.named_steps['min_max']) == MinMaxScaler\n",
    "assert pipeline.named_steps['min_max'].get_params()['feature_range'] == (0,5)\n",
    "assert type(pipeline.named_steps['model']) == LinearRegression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-877f9bbc108ee8be",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Exercises complete, congratulations! You are about to become a certified data wrangler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
