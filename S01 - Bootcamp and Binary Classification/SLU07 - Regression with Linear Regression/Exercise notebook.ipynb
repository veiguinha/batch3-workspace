{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-79f2337e7779945a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# SLU7 - Regression With Linear Regression: Exercise notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62ddf765d4352694",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this notebook you will practice the following:\n",
    "     - Simple Linear Regression\n",
    "     - Multiple Linear Regression\n",
    "     - Closed Form Solution\n",
    "     - Using scikit learn linear regression implementations\n",
    "     - Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4cb0eb9c3a32286f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Base imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-74ff3f984bb74106",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a1eba3d354cdb022",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this section, you will use the linear regression to solve the simple problem you have seen in the learning notebooks. Let's start by loading the data we want to model and visualizing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce2d08a1d9e7cb2d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_lin = pd.read_csv('data/linear.csv')\n",
    "df_lin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b60e1b3dc8c60d72",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_lin = df_lin.sort_values('x')\n",
    "plt.xlim((-10, 10))\n",
    "plt.ylim((-20, 20))\n",
    "plt.plot(df_lin['x'], df_lin['y'], 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67395307d5b7356e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.1 - Simple Linear Model\n",
    "\n",
    "As you can see, our data has only one variable (x) and one label (y), so we can try to fit it with a simple linear regression. This model is represented by the following expression:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "where $\\hat{y}$ are the predictions, $\\beta_0$ is the intercept, $\\beta_1$ is the coefficient and $x$ is the input sample. Expanding to several samples, we can write this equation in a vector form:\n",
    "\n",
    "$$\\vec{\\hat{y}} = \\beta_0\\vec{1} + \\beta_1 \\vec{x}$$\n",
    "\n",
    "Implement this linear model below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0658efc6a6964c6e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_linear_model(x, b):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples,) - The input data \n",
    "        b: numpy.array with shape (2,) - The weights of the model [b_0, b_1]\n",
    "    \n",
    "    Returns:\n",
    "        y_hat : numpy.array with shape (num_samples,) - The prediction made by \n",
    "                the simple linear regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-651ae1ad52511300",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(\n",
    "    simple_linear_model(np.arange(0, 10), np.array([-12, 30])), \n",
    "    np.array([-12.,  18.,  48.,  78., 108., 138., 168., 198., 228., 258.])\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    simple_linear_model(np.arange(-5, 5), np.array([1, 1])), \n",
    "    np.array([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    simple_linear_model(np.arange(-10, 10, 2), np.array([0.25, 2.1])), \n",
    "    np.array([-20.75, -16.55, -12.35, -8.15, -3.95, 0.25, 4.45, 8.65, 12.85, 17.05])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc935eebba6de6f9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The first approach you can take is to implement the closed form solution. This is, solving the equation that minimizes the error accross all of the samples - ordinary least squares. For that, however, we need to understand what the error trying to be minimized is. Let's take a look at the error function you learned:\n",
    "\n",
    "### 1.2 Summed Square Error\n",
    "\n",
    "Start by implementing the error function presented - summed squared error:\n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N e_i^2 = \\frac{1}{N} \\sum_{n=1}^N (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "Where the error is the difference between your predictions and the actual sample value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1049431fa099d4b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def summed_squared_error_function(y, y_predict):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        y : numpy.array with shape (num_samples, ) - real target\n",
    "        y_predict : numpy.array  with shape (num_samples, ) - predicted target\n",
    "    \n",
    "    Returns:\n",
    "        error : float\n",
    "    \"\"\"\n",
    "    # Compute the error\n",
    "    # e = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now, square the difference\n",
    "    # s = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Finally, take the mean and return the error\n",
    "    # m = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f2a4da7fe193783",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4510d1dc5df769f3",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple tests\n",
    "assert math.isclose(summed_squared_error_function(np.array([.3412]), np.array([.1231])), 0.04756761000000001)\n",
    "\n",
    "assert math.isclose(summed_squared_error_function(np.array([3.1231]), np.array([4.1313])), 1.0164672400000008)\n",
    "\n",
    "\n",
    "# Error in our dataset with random weights\n",
    "x_rnd = df_lin['x'].values\n",
    "y_rnd = df_lin['y'].values\n",
    "beta_rnd = np.array([1., -1.])\n",
    "y_hat_rnd = simple_linear_model(x_rnd, beta_rnd)\n",
    "\n",
    "assert math.isclose(summed_squared_error_function(y_rnd, y_hat_rnd), 110.62795023298027)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab4aadf231356588",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As you can see from the previous test, picking just random values for our weights will probably yield very high error values. You can even visualize this to see that in fact these random weights don't fit our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3963d719eccbc48",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((-10, 10))\n",
    "plt.ylim((-20, 20))\n",
    "plt.plot(x_rnd, y_rnd, 'b.')\n",
    "plt.plot(x_rnd, y_hat_rnd, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b7d6627f93ad176",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.3 - Closed Form Solution\n",
    "\n",
    "Let's then implement a closed form solution. Remember the solution to minimize the error can be written as:\n",
    "\n",
    "$$ \\beta_1 = \\frac{\\sum_{i}^{N}{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sum_{i}^{N}{(x_i - \\bar{x})^2}} = \\frac{cov(x, y)}{var(x)}$$\n",
    "\n",
    "with cov(x,y) and var(x) are, respectively, the covariance and variance of the samples\n",
    "\n",
    "$$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $$ \n",
    "\n",
    "where $\\bar{y} = \\frac{1}{N}\\sum_{i}^{N}{y_i}$ and $\\bar{x} = \\frac{1}{N}\\sum_{i}^{N}{x_i}$ are the means of the sample.\n",
    "\n",
    "Complete the closed form solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c6179999833a9a8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_closed_form_solution(x, y):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        x : numpy.array with shape (num_samples, ) - input samples \n",
    "        y_predict : numpy.array with shape (num_samples, ) - sample labels\n",
    "    \n",
    "    Returns:\n",
    "        betas: coefficient [beta_0, beta_1] as a numpy.array with shape (2, ) \n",
    "    \"\"\"\n",
    "    # The sample covariance and variance for 1-d arrays in \n",
    "    # numpy for this particular case are computed as follows\n",
    "    # We covered this part so you don't lose too much time on these details\n",
    "    cov_xy = np.cov(x, y, bias=True)[0][1]\n",
    "    var_x = np.var(x)\n",
    "    \n",
    "    # Compute coefficient\n",
    "    # beta_1 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute intersect\n",
    "    # beta_0 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now, return the coefficients (betas)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea54ae86dbd717c8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ed0cf5f5ace73f0a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(\n",
    "    simple_closed_form_solution(np.arange(0, 10), np.arange(0, 20, 2)),\n",
    "    np.array([0., 2.])\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    simple_closed_form_solution(np.arange(-2, 3), np.array([-1.25, -.5, .25, 1., 1.75])),\n",
    "    np.array([.25, .75])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-87eed9f8fb8bc294",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now apply it to our dataset to get the best weights, and measure the error across the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0e90b401208965c6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_cf = df_lin['x'].values\n",
    "y_cf = df_lin['y'].values\n",
    "beta_cf = simple_closed_form_solution(x_cf, y_cf)\n",
    "y_hat_cf = simple_linear_model(x_cf, beta_cf)\n",
    "\n",
    "assert math.isclose(summed_squared_error_function(y_cf, y_hat_cf), 5.8835039829278095)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f51a45d9ec142687",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You can also visualize how good your solution fits the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61a07e39f9f8fdd4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((-10, 10))\n",
    "plt.ylim((-20, 20))\n",
    "plt.plot(x_cf, y_cf, 'b.')\n",
    "plt.plot(x_cf, y_hat_cf, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-323f3707057d5783",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-708eb8c290452c85",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this section, we will expand what we learned to a linear regression with multiple inputs - which we call features of our model. We will use a very specific scenario so we are able to visualize it better - we will try to model a polynomial function, in particular, a cubic function, which can be written as:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^{2} + \\beta_3 x^{3}$$\n",
    "\n",
    "You will basically be considering each power of x as a different feature. To simplify, we already provide the dataset prepared with the powers we want for this, so let's load it to provide some context:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7822f07e635af70",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_pln = pd.read_csv('data/polynomial.csv')\n",
    "df_pln.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e2ef3862e8aae7f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_pln = df_pln.sort_values('x')\n",
    "plt.xlim((0, 10))\n",
    "plt.ylim((0, 10))\n",
    "plt.plot(df_pln['x'], df_pln['y'], 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c9f9d3c78f620b9f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Although this model is non linear in its features, notice that it is linear with respect to the weigths, and the equation above can be rewritten as\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x_0 + \\beta_2 x_1 + \\beta_3 x_2$$\n",
    "\n",
    "where $[x_0, x_1, x_2]$ is our feature vector for a given sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79fedc69b50dbe6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.1  Linear Model Extended\n",
    "\n",
    "The multiple linear regression problem is just the linear regression problem on a linear model with several inputs. This model can be represented by the following expressions:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\sum_{i=1}^K \\beta_i x_i$$\n",
    "\n",
    "We can also write it in matrix form to consider several samples, as before:\n",
    "\n",
    "$$\\vec{\\hat{y}} = \\beta_0\\vec{1} + \\vec{\\beta_{1-k}}X^T$$\n",
    "\n",
    "where X is now a matrix, containing all features for all samples: \n",
    "\n",
    "$$ X = \\begin{bmatrix} \n",
    "x_1^1 & x_1^2 & ... & x_1^k \\\\\n",
    "x_2^1 & x_2^2 & ... & x_2^k \\\\\n",
    "... & ... & ... & ...\\\\\n",
    "x_n^1 & x_n^2 & ... & x_n^k \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "If you go back to the closed form solution you implemented before, you might notice that we had already used matrix form, in particular to concatenate our whole weight vector. We'll follow the same logic, and extend our matrix X to allow a collumn of ones:\n",
    "\n",
    "$$ X' = [\\vec{1} | X] $$\n",
    "\n",
    "and rewrite:\n",
    "\n",
    "$$\\vec{\\hat{y}} = \\vec{\\beta}(X')^T$$\n",
    "\n",
    "Implement below this extended model.\n",
    "\n",
    "Tip: You might want to review the learning notebook and examples to get used to the matrix handling in the following problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10eb3e761476d6d9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def extended_linear_model(x, betas):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        betas : numpy.array with shape (num_features + 1,) - weights of \n",
    "                our model, with the intercept in the first position of \n",
    "                the array\n",
    "    \n",
    "    Returns:\n",
    "        y_pred : numpy.array with shape (num_samples,) - prediction \n",
    "                made by the simple linear regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We do the proper reshaping of weights so you don't have \n",
    "    # to worry about that and focus on the remaining logic\n",
    "    betas = betas.reshape((1, -1))\n",
    "    \n",
    "    # Extend the matrix x with a collumn of ones\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Compute the output of the linear model\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Once again, we reshape your array to get the proper output\n",
    "    return y_pred.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a1c4e3d2ce2183d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Since this is an extension of the simple linear model, it should be able to cover that use case also. Check that your solution still passes the test for the simple linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d853a799cc5992d0",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Same examples as in\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.arange(0, 10).reshape(-1, 1), np.array([-12, 30])), \n",
    "    np.array([-12.,  18.,  48.,  78., 108., 138., 168., 198., 228., 258.])\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.arange(-5, 5).reshape(-1, 1), np.array([1, 1])), \n",
    "    np.array([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.arange(-10, 10, 2).reshape(-1, 1), np.array([0.25, 2.1])), \n",
    "    np.array([-20.75, -16.55, -12.35, -8.15, -3.95, 0.25, 4.45, 8.65, 12.85, 17.05])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check your solution passes the tests for the extended version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8c9cab5fa1ebfc99",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.array([[1., 2.], [3., 4.], [5., 6.]]), np.array([-1., 0., 1.])), \n",
    "    np.array([1., 3., 5.])\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.ones((10, 2)), np.array([1., 2., 3.])), \n",
    "    np.array([6., 6., 6., 6., 6., 6., 6., 6., 6., 6.])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae733317d24d320d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 - Summed Squared Error\n",
    "\n",
    "As before, we can use the summed squared error as our cost. However, since this function does not receive anything other than the predictions and true values, there is no need to reimplement it. As before, let's see how the model would behave and what would be its error if we pick random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ebb1e0761f537021",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Error in our dataset with random weights\n",
    "x_pln_rnd = df_pln['x']\n",
    "X_pln_rnd = df_pln.drop('y', axis=1).to_numpy()\n",
    "y_pln_rnd = df_pln['y'].values\n",
    "beta_pln_rnd = np.array([1., 1., -.5, 0.])\n",
    "y_hat_pln_rnd = extended_linear_model(X_pln_rnd, beta_pln_rnd)\n",
    "\n",
    "plt.xlim((0, 10))\n",
    "plt.ylim((-10, 10))\n",
    "plt.plot(x_pln_rnd, y_pln_rnd, 'b.')\n",
    "plt.plot(x_pln_rnd, y_hat_pln_rnd, 'r-')\n",
    "\n",
    "print('Error: {}'.format(summed_squared_error_function(y_pln_rnd, y_hat_pln_rnd))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e3366d377d3c9b7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As you can see, the solution is clearly not a fit, and the error is very high. So let's move into our closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ff21e3c52fafce2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 - Closed Form Solution\n",
    "\n",
    "Let's now implement the closed form solution for the generic case. When put into matrix form, remember the solution to minimize the error can be written as:\n",
    "\n",
    "$$ \\vec{\\beta} = (X^TX)^{-1}(X^T\\vec{y})$$\n",
    "\n",
    "\n",
    "Where X is our matrix of samples extended to add a 1 component in each sample, $X = [\\vec{1} | X] $ , $\\vec{y}$ is the output vector, and $\\vec{\\beta}$ the weight vector with weights $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "Implement the closed form solution for the multiple linear regression problem below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e87f47eb6ec320ee",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def extended_closed_form_solution(x, y):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        y : numpy.array with shape (num_samples, ) - sample labels\n",
    "    \n",
    "    Returns:\n",
    "        betas : numpy.array with shape (num_features + 1, ) - weight vector \n",
    "    \"\"\" \n",
    "    \n",
    "    # Proper reshaping of the labels\n",
    "    y = y.reshape((-1, 1))\n",
    "\n",
    "    # Extend vector of samples with array of ones\n",
    "    # X_extended = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute betas\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Once again, we reshape your array to get the proper output\n",
    "    return betas.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ec27afe2a6ee23b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-47587567e05c2602",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Old tests\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.arange(0, 10).reshape(10, 1), np.arange(0, 20, 2).reshape(10, 1)),\n",
    "    np.array([0., 2.])\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.arange(-2, 3).reshape(5, 1), np.array([-1.25, -.5, .25, 1., 1.75]).reshape(5, 1)),\n",
    "    np.array([.25, .75])\n",
    ")\n",
    "\n",
    "\n",
    "# Extended test cases\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.array([[1., -1.], [2., 1.], [3., -5.]]), np.array([0., 1., 0.])), \n",
    "    np.array([-0.25, 0.5, 0.25])\n",
    ")\n",
    "\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.array([[10., -2.], [-4., 5.], [-7., -8.]]), np.array([2., 1., -.5])), \n",
    "    np.array([1.019704, 0.115764, 0.08867])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbdc5cd776150269",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now apply it to our dataset to get the best weights, and measure the error across the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd38814320080448",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_pln_cf = df_pln['x'].values\n",
    "y_pln_cf = df_pln['y'].values\n",
    "\n",
    "# All collumns except y\n",
    "X_pln_cf = df_pln.drop('y', axis=1).to_numpy()\n",
    "\n",
    "beta_pln_cf = extended_closed_form_solution(X_pln_cf, y_pln_cf)\n",
    "y_hat_pln_cf = extended_linear_model(X_pln_cf, beta_pln_cf)\n",
    "\n",
    "assert math.isclose(summed_squared_error_function(y_pln_cf, y_hat_pln_cf), 1.1570975215814507)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d204ff04dacb5dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "And finally we'll try to see how well this solution fits the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-33d9aab23eb2dadd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((0, 10))\n",
    "plt.ylim((-5, 10))\n",
    "plt.plot(x_pln_cf, y_pln_cf, 'b.')\n",
    "plt.plot(x_pln_cf, y_hat_pln_cf, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4bbf62d226129f9a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3  ScikitLearn Linear Regression\n",
    "\n",
    "Although implementing the closed form solution and error functions is a good exercise, our implementation is still far from robust. Luckily, ScikitLearn already provides us with a solver for the Linear Regression problem, which implements a closed form solution internally. It also provides already some extra info on the regression, such as the $R^2$ score. \n",
    "\n",
    "For this exercise we'll be using the Boston housing dataset, and try to model the house pricing through the provided features. Start by loading and looking into the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae73b6209a4966ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/boston (scaled).csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features except house price\n",
    "columns_housing = data.drop('MEDV', axis=1)\n",
    "x_housing = columns_housing.to_numpy()\n",
    "\n",
    "# House price \n",
    "y_housing = data['MEDV'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fbd3a2e4e2107803",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Each of the collumns in the table is one of the features our model is going to use, this is, one of the inputs we are going to give it. Use the `sklearn.linear_model.LinearRegression` module that you've learned and implement it in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4896df37a4f5670c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def sklearn_regression(x, y):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args: \n",
    "        x: numpy.ndarray with shape (num_samples, num_features) - samples of our model\n",
    "        y: numpy.array with shape (num_samples, ) - sample labels\n",
    "        \n",
    "    Return:\n",
    "        coefs: numpy array with shape (num_features,) - coefficients vector\n",
    "        intercept: numpy array with shape (1,) - intercept value\n",
    "        score: float - R squared score of regression\n",
    "    \"\"\"\n",
    "    # Fit the linear regressor\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the coefficients\n",
    "    # coefs = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the coefficients\n",
    "    # intercept = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the coefficients\n",
    "    # score = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return coefs, intercept, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25a710d3ea831ce0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's see then what our coefficients are for each of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-edea816492fd3f98",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "coefs_housing, intercept_housing, score_housing = sklearn_regression(x_housing, y_housing)\n",
    "\n",
    "print('Feature coefficients: ')\n",
    "print(pd.Series(coefs_housing, columns_housing.columns))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept: {}'.format(intercept_housing))\n",
    "print('\\n')\n",
    "\n",
    "print('R² score: {}'.format(score_housing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1aca587618ebd00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Finally, check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dfb0a7be54a211de",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "betas_housing = np.concatenate((np.array([intercept_housing]), np.array(coefs_housing)), axis=0)\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(x_housing[:10], betas_housing),\n",
    "    np.array([30.00821269, 25.0298606, 30.5702317, 28.60814055, 27.94288232, \n",
    "              25.25940048, 23.00433994, 19.5347558, 11.51696539, 18.91981483])\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(x_housing[-10:], betas_housing),\n",
    "    np.array([14.01017244, 19.10825534, 21.29720741, 18.45524217, 20.46764235, \n",
    "              23.53261729, 22.37869798, 27.62934247, 26.12983844, 22.34870269])\n",
    ")\n",
    "\n",
    "y_hat_housing = extended_linear_model(x_housing, betas_housing)\n",
    "assert math.isclose(summed_squared_error_function(y_housing, y_hat_housing), 21.8977792176875)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d32d00c1a044a960",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can also use this model to get to the solution for our previous problems. Run the cells below and see the scikitlearn solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79638baa0ce7e0d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_lin = pd.read_csv('data/linear.csv')\n",
    "df_lin = df_lin.sort_values('x')\n",
    "x_lin_skl = df_lin['x'].values.reshape(-1, 1)\n",
    "y_lin_skl = df_lin['y'].values\n",
    "\n",
    "coefs_lin, intercept_lin, _ = sklearn_regression(x_lin_skl, y_lin_skl)\n",
    "betas_lin_skl = np.array([intercept_lin, coefs_lin[0]])\n",
    "y_hat_lin_skl = extended_linear_model(x_lin_skl, betas_lin_skl)\n",
    "\n",
    "plt.xlim((-10, 10))\n",
    "plt.ylim((-20, 20))\n",
    "plt.plot(x_lin_skl, y_lin_skl, 'b.')\n",
    "plt.plot(x_lin_skl, y_hat_lin_skl, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-07a61d128cb2917a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_pln = pd.read_csv('data/polynomial.csv')\n",
    "df_pln = df_pln.sort_values('x')\n",
    "x_pln_skl = df_pln['x'].values.reshape(-1, 1)\n",
    "y_pln_skl = df_pln['y'].values\n",
    "X_pln_skl = df_pln.drop('y', axis=1).to_numpy()\n",
    "\n",
    "coefs_pln, intercept_pln, _ = sklearn_regression(X_pln_skl, y_pln_skl)\n",
    "betas_pln_skl = np.concatenate((np.array([intercept_pln]), np.array(coefs_pln)), axis=0)\n",
    "y_hat_pln_skl = extended_linear_model(X_pln_skl, betas_pln_skl)\n",
    "\n",
    "plt.xlim((0, 10))\n",
    "plt.ylim((-5, 10))\n",
    "plt.plot(x_pln_skl, y_pln_skl, 'b.')\n",
    "plt.plot(x_pln_skl, y_hat_pln_skl, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-24a32ef449262da5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "So now that you've seen the scikitlearn method in action, let's move on to a more generic method - the Gradient Descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent\n",
    "\n",
    "Now we will see how to get to a similar solution through learning methods. In this section, you will implement gradient descent, an algorithm to iteratively update the weights according to the direction of the error.\n",
    "\n",
    "This method is an iterative process that updates the weights in the direction that minimizes our error. For this it makes use of derivatives. The formula follows:\n",
    "\n",
    "$$ \\vec{\\beta}_{i+1} = \\vec{\\beta}_i - \\eta \\Delta_\\vec{\\beta} J$$\n",
    "\n",
    "where $\\Delta_\\vec{\\beta}$ is a vector of the derivatives - also called gradients - of our error function with respect to the weights, $\\beta_{i+1}$ is the updated weight and $\\beta_{i}$ the current weight. We then need to be able to compute these gradients to be able to update the weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e54fd3a31f51054",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 4.1 Multiple Linear Regression partial derivatives\n",
    "\n",
    "The vector $\\Delta_\\vec{\\beta}$ in the formula above is just a vector with the partial derivatives of the error function with respect to each of the weights. The formulas for these partial derivatives with respect to each weigth are defined as follows:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_0} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n) $$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_1} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n)x_{1_n} $$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_K} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n)x_{K_n} $$\n",
    "\n",
    "Since the focus of this notebook is for you to implement the methods to solve linear regression, and you already have quite some work, we'll solve this one for you. Check below the code for the derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-315f5fa32e6031cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_partial_derivatives(x, y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        y : numpy.array with shape (num_samples,) - sample labels\n",
    "        y_hat : numpy.array with shape (num_samples,) - predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        deltas : pandas.Series shape (num_features + 1,)\n",
    "            \n",
    "    \"\"\"    \n",
    "\n",
    "    # Compute the difference between the targets and the predictions.\n",
    "    y_diff = y - y_hat\n",
    "    \n",
    "    # Initialize the numpy array of partial derivatives\n",
    "    deltas = np.zeros((x.shape[1] + 1, ))\n",
    "    \n",
    "    # Compute the partial derivative for b0\n",
    "    deltas[0] = -(2 * y_diff).mean()\n",
    "    \n",
    "    # Extract the partial derivatives of the remaining betas  \n",
    "    for col in range(x.shape[1]): \n",
    "        deltas[col+1] = -((2 * y_diff) * x[:, col]).mean()\n",
    "    \n",
    "    # Return derivatives \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-664e5ef6f8b4ed2e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 4.2 Adjusting  parameters with gradient descent\n",
    "\n",
    "Now we want to adjust the weights with the update rule we presented:\n",
    "\n",
    "$$ \\vec{\\beta}_{i+1} = \\vec{\\beta}_i - \\eta \\Delta_\\vec{\\beta} J$$\n",
    "\n",
    "where $\\eta$ is our learning rate - how fast we want to move in the direction of the gradient. We will be implementing the standard gradient descent, also know as batch gradient descent, where for each iteration we will compute the derivatives by taking in all the dataset:\n",
    "\n",
    "1. _For epoch in 1...epochs:\n",
    "    1. Predict the outputs with current weights $\\hat{y} = \\vec{\\beta}_i X$\n",
    "    2. $\\Delta_{\\beta_0} = \\frac{1}{N} \\sum_{n=1}^N 2 (y - \\hat{y})$\n",
    "    3. $\\Delta_{\\beta_{i=1...N}} = \\frac{1}{N} \\sum_{n=1}^N 2 (y - \\hat{y})x_{i_n} $\n",
    "    4. $\\beta_i = \\beta_i - \\eta \\Delta_{\\beta_i}$\n",
    "\n",
    "Notice that you can get the gradients in steps 1 and 2 with the function implemented above. \n",
    "\n",
    "The number of epochs and learning rate will impact how fast and how good the solution we converge to. Besides the number of epochs there are more clever ways of knowing when to stop this procedure, but for simplicity, we will only use this one here.\n",
    "\n",
    "Implement this gradient descent function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3f64edb60cc15b1a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_gradient_descent(x, y, betas, learning_rate, epochs): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        y : numpy.array with shape (num_samples,)  - sample labels\n",
    "        betas : numpy.array with shape (num_features + 1,) - initial weights\n",
    "        learning_rate : float - factor that will define the size of update step\n",
    "        epochs : int - number of times to run full dataset\n",
    "\n",
    "    Returns:\n",
    "        betas : numpy.array with shape (num_features + 1,) - final weights after algorithm\n",
    "            \n",
    "    \"\"\"    \n",
    "\n",
    "    for epoch in range(epochs): \n",
    "\n",
    "        # Compute estimates for this iteration \n",
    "        # y_hat = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Compute the partial derivatives of the error function \n",
    "        # (hint: check linear_regression_partial_derivatives)\n",
    "        # deltas = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Update betas with Gradient Descent rule \n",
    "        # betas = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9251131ceba84a3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b26ec5b38f334783",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "betas = np.random.rand(x_housing.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "\n",
    "betas_ = linear_regression_gradient_descent(x_housing, y_housing, betas, learning_rate, epochs)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    betas_, \n",
    "    np.array([\n",
    "        20.1536, -0.391 ,  0.7281, -0.1594,  0.9096, -0.4351,  3.3176,\n",
    "        0.2957, -0.9711,  0.5549, -0.8421, -1.4982,  1.0387, -3.0292]), \n",
    "    decimal=4)\n",
    "\n",
    "\n",
    "np.random.seed(84)\n",
    "betas = np.random.rand(x_housing.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 1\n",
    "\n",
    "betas_ = linear_regression_gradient_descent(x_housing, y_housing, betas, learning_rate, epochs)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    betas_, \n",
    "    np.array([ 4.5434, -0.4652,  0.9789, -0.4454,  1.1187, -0.7988,  1.5215,\n",
    "       -0.581 ,  0.9902, -0.7336, -0.745 , -0.5769,  1.5129, -1.1906]), \n",
    "    decimal=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-84a4265ff22c40c5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Finally, run the SGD for the housing problem, and compare the coefficients with the ones from the previous closed form solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "betas = np.random.rand(x_housing.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 200\n",
    "\n",
    "betas_ = linear_regression_gradient_descent(x_housing, y_housing, betas, learning_rate, epochs)\n",
    "\n",
    "intercept_housing_sgd = betas_[0]\n",
    "coefs_housing_sgd = betas_[1:]\n",
    "\n",
    "series_sgd = pd.Series(coefs_housing_sgd, columns_housing.columns, name='SGD')\n",
    "series_ols = pd.Series(coefs_housing, columns_housing.columns, name='OLS')\n",
    "\n",
    "print('Feature coefficients: ')\n",
    "print(pd.concat([series_sgd, series_ols], axis=1))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept SGD: {}'.format(intercept_housing_sgd))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept OLS: {}'.format(intercept_housing))\n",
    "print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
