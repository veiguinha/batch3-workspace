{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-05e844b835aa5580",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# SLU09: Model Selection & Overfitting -- Exercises\n",
    "---\n",
    "\n",
    "*Exercises are graded unless otherwise indicated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e42bb037a88a592b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from utils import generate_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f15b4c639c45f70",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1: Detecting bias and variance in the real world (not graded)\n",
    "\n",
    "For each of the following, identify if they are more likely to be sources of bias or variance:\n",
    "\n",
    "1. Using very flexible models (e.g., non-parametric, non-linear), such as K-nearest neighbors or decision trees (bias/variance)\n",
    "2. Using models with simplistic assumptions, such as linear or logistic regressions (bias/variance)\n",
    "3. Increasing the polynomial degree of our hypothesis function (bias/variance)\n",
    "4. Ignoring important features (bias/variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58a365f406a228fe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2: Create training and test datasets (train-test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34f35a43586a0e85",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def implement_hold_out_method(X, y, test_size=.4, random_state=0):\n",
    "    \"\"\" \n",
    "    Implementing the holdout method, using sklearn.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): a pandas dataframe containing the features\n",
    "        y (pd.Series): a pandas series containing the target variable\n",
    "        test_size (float): proportion of the dataset to include in the test set\n",
    "        random_state (int): the seed used by the random number generator\n",
    "\n",
    "    Returns:\n",
    "        X_train (pd.DataFrame): the features for the training examples\n",
    "        X_test (pd.DataFrame): the features for the test examples\n",
    "        y_train (pd.Series): target for the training set \n",
    "        y_test (pd.Series): target for the test set\n",
    "\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d0784c116c6f5dda",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the solution is correct.\"\"\"\n",
    "\n",
    "X, y = generate_test_data(m=100, n=4)\n",
    "X_train, X_test, y_train, y_test = implement_hold_out_method(X, y)\n",
    "\n",
    "assert X_train.shape == (60, 3)\n",
    "assert X_test.shape == (40, 3)\n",
    "assert y_train.shape == (60,)\n",
    "assert y_test.shape == (40,)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = implement_hold_out_method(X, y, test_size=.1)\n",
    "\n",
    "assert X_train2.shape == (90, 3)\n",
    "assert X_test2.shape == (10, 3)\n",
    "assert y_train2.shape == (90,)\n",
    "assert y_test2.shape == (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af6e4ef9cfe15659",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 3: Creating a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a287c4f6e7371cf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def implement_validation_dataset(X, y, test_size=.25, val_size=.25, random_state=0):\n",
    "    \"\"\" \n",
    "    Implementing the holdout method with validation, using sklearn.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): a pandas dataframe containing the features\n",
    "        y (pd.Series): a pandas series containing the target variable\n",
    "        test_size (float): proportion of the dataset to include in the test set\n",
    "        val_size (float): proportion of the dataset to include in the validation set\n",
    "        random_state (int): the seed used by the random number generator\n",
    "\n",
    "    Returns:\n",
    "        X_train (pd.DataFrame): the features for the training examples\n",
    "        X_test (pd.DataFrame): the features for the test examples\n",
    "        X_val (pd.DataFrame): the features of the validation examples\n",
    "        y_train (pd.Series): target for the training set \n",
    "        y_test (pd.Series): target for the test set\n",
    "        y_val (pd.Series): target for the validation set\n",
    "\n",
    "    \"\"\"\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    the_right_val_size = (val_size*X.shape[0])/(X.shape[0]-X_test.shape[0])\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp,\n",
    "                                                               test_size=the_right_val_size, random_state=random_state)\n",
    "    del X_temp, y_temp\n",
    "    \n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4cd9ffbea2630f66",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the solution is correct.\"\"\"\n",
    "X, y = generate_test_data(m=1000, n=5)\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = implement_validation_dataset(X, y)\n",
    "\n",
    "assert X_train.shape == (500, 4)\n",
    "assert X_test.shape == (250, 4)\n",
    "assert X_val.shape == (250, 4)\n",
    "assert y_train.shape == (500,)\n",
    "assert y_test.shape == (250,)\n",
    "assert y_val.shape == (250,)\n",
    "\n",
    "\n",
    "X_train2, X_test2, X_val2, y_train2, y_test2, y_val2 = implement_validation_dataset(X, y, test_size=.1, val_size=.3)\n",
    "\n",
    "assert X_train2.shape == (600, 4)\n",
    "assert X_test2.shape == (100, 4)\n",
    "assert X_val2.shape == (300, 4)\n",
    "assert y_train2.shape == (600,)\n",
    "assert y_test2.shape == (100,)\n",
    "assert y_val2.shape == (300,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-970ade4434f10e80",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 4: Implementing K-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8247c46e92ae80d6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Get the mean cross validation score for linear regression (with 4 folds) and logistic regression (with 6 folds). \n",
    "\n",
    "Leave the scoring parameter with the default value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b0ed9bb14923e9fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caixi\\Anaconda3\\envs\\SLU09\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\caixi\\Anaconda3\\envs\\SLU09\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\caixi\\Anaconda3\\envs\\SLU09\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\caixi\\Anaconda3\\envs\\SLU09\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\caixi\\Anaconda3\\envs\\SLU09\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\caixi\\Anaconda3\\envs\\SLU09\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# here is your data \n",
    "X_train = pd.read_csv('data/something')\n",
    "y_train_num = pd.read_csv('data/something_else', header=None)\n",
    "y_train_cat = y_train_num.apply(lambda x: x < 50).astype('int')\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# here are your models\n",
    "lin_reg = LinearRegression()\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# store the mean cross val scores as lin_reg_result and log_reg_result\n",
    "# use y_train_num for linear regression\n",
    "# use y_train_cat for logistic regression\n",
    "lin_reg_result = cross_val_score(lin_reg,X_train,y_train_num,cv=4).mean()\n",
    "log_reg_result = cross_val_score(log_reg,X_train,y_train_cat,cv=6).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e89d8d233b70f834",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(lin_reg_result, -0.1477, 3)\n",
    "np.testing.assert_almost_equal(log_reg_result, 0.5333, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c75cf65a2bd703e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 5: Regularization loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-651420ce6ae27bfc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# data \n",
    "X = pd.read_csv('data/something')\n",
    "y = pd.read_csv('data/something_else', header=None)\n",
    "\n",
    "# model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# fit model\n",
    "lr.fit(X, y)\n",
    "\n",
    "# get variables for calculating loss functions\n",
    "y = np.array(y[0])\n",
    "y_hat = lr.predict(X).reshape(60,)\n",
    "betas = np.append(lr.intercept_, lr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da83571625ddaf56",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Compute the $L_1$ and $L_2$ loss functions for the linear regression model trained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-697a22e92af3fe0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "$$J_{L_1} = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_1 \\sum_{k=1}^K \\left|\\beta_k\\right|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-39627df362a78644",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 5.1: MSE\n",
    "\n",
    "First, let's compute the Mean Squared Error part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-add82190e7ce5ca5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        y : numpy array with shape (num_observations,)\n",
    "            The targets.\n",
    "        y_hat : numpy array with shape (num_observations,)\n",
    "            The predictions.\n",
    "            \n",
    "    Returns:\n",
    "        Mean squared error : float\n",
    "    \"\"\"\n",
    "\n",
    "    mse = ((y-y_hat)**2).mean()\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bc3a09760dfe853f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y, y_hat)\n",
    "np.testing.assert_almost_equal(mse, 635.2071, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7913a75cbf17fc97",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 5.2: L1 loss\n",
    "\n",
    "Now, we can compute the $L_1$ loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-692eb1183bd9f57f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def l1_loss(y, y_hat, betas, lamb1):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        y : numpy array with shape (num_observations,)\n",
    "            The targets.\n",
    "        y_hat : numpy array with shape (num_observations,)\n",
    "            The predictions.\n",
    "        betas : numpy array with shape (num_features+1,)\n",
    "            The parameters of your regression model. \n",
    "            The first value is the intercept and the \n",
    "            remaining ones are the feature coefficients.\n",
    "        lamb1 : float\n",
    "            The strength of the L1 regularizer.\n",
    "            \n",
    "    Returns:\n",
    "        loss : float\n",
    "    \"\"\"\n",
    "        \n",
    "    # Compute the mean squared error loss part of the general loss function.\n",
    "    # Hint: use the function we created above\n",
    "    mse =  mean_squared_error(y, y_hat)\n",
    "    \n",
    "    # Compute the L1 part of the general loss function.\n",
    "    # Do not forget that we are not supposed to include the intercept in the L1 regularization \n",
    "    l1 = lamb1*(np.absolute(betas[1:]).sum())\n",
    "    \n",
    "    # Compute the total loss by combining the parts.\n",
    "    L = mse + l1\n",
    "    \n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c7a7f5cfc2871021",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "lamb1 = 2\n",
    "loss = l1_loss(y, y_hat, betas, lamb1)\n",
    "np.testing.assert_almost_equal(loss, 635.8783, 3)\n",
    "\n",
    "lamb1_2 = 6\n",
    "loss2 = l1_loss(y, y_hat, betas, lamb1_2)\n",
    "np.testing.assert_almost_equal(loss2, 637.2206, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7b97af89e87edf14",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 5.3: L2 loss\n",
    "\n",
    "Finally, let's compute the $L_2$ loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6fb97fab632b4f7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "$$J_{L_2} = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_2 \\sum_{k=1}^K \\beta_k^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6c5def23f7f4d0a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def l2_loss(y, y_hat, betas, lamb2):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        y : numpy array with shape (num_observations,)\n",
    "            The targets.\n",
    "        y_hat : numpy array with shape (num_observations,)\n",
    "            The predictions.\n",
    "        betas : numpy array with shape (num_features+1,)\n",
    "            The parameters of your regression model. \n",
    "            The first value is the intercept and the \n",
    "            remaining ones are the feature coefficients.\n",
    "        lamb2 : float\n",
    "            The strength of the L2 regularizer.\n",
    "            \n",
    "    Returns:\n",
    "        loss : float\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the mean squared error loss part of the general loss function.\n",
    "    # Hint: use the function we created above\n",
    "    mse = mean_squared_error(y,y_hat)\n",
    "    \n",
    "    # Compute the L2 part of the general loss function.\n",
    "    # Do not forget that we are not supposed to include the intercept in l2 regularization\n",
    "    l2 = lamb2*((betas[1:]**2).sum())\n",
    "    \n",
    "    # Compute the total loss by combining the parts.\n",
    "    L = mse + l2 \n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fecd3123c3630fb1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "lamb2 = 2\n",
    "loss = l2_loss(y, y_hat, betas, lamb2)\n",
    "np.testing.assert_almost_equal(loss, 635.2940, 3)\n",
    "\n",
    "lamb2_2 = 6\n",
    "loss2 = l2_loss(y, y_hat, betas, lamb2_2)\n",
    "np.testing.assert_almost_equal(loss2, 635.4676, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8b13848309c9d4c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 6: Regularized linear regression in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ef765293921bf50",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 6.1: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea5ea253d5d19cca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# create data\n",
    "X_temp = pd.read_csv('data/something')\n",
    "y = pd.read_csv('data/something_else', header=None)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X = pd.DataFrame(poly.fit_transform(X_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-041c01ab12c152ea",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso \n",
    "\n",
    "def lasso_regression(X, y, lamb):\n",
    "    \"\"\"\n",
    "    Implements lasso (L1) regression, using sklearn\n",
    "    \n",
    "    Args: \n",
    "        X (pd.DataFrame): a pandas dataframe containing the features\n",
    "        y (pd.Series): a pandas series containing the target variable\n",
    "        lamb (float): the strength of the regularizer\n",
    "            \n",
    "    Returns:\n",
    "        lasso : a Lasso regression model fitted to X and y\n",
    "    \"\"\"\n",
    "    \n",
    "    # instantiate an instance of sklearn's Lasso model\n",
    "    # use named arguments, such as alpha=... and random_state=...\n",
    "    # use a random state of 42\n",
    "    \n",
    "    # We should normalize the data in order to measure and compare the coefficients, \n",
    "    # however for the purpose of doing the exercise we did not do it \n",
    "    lasso = Lasso(alpha = lamb,random_state = 42)\n",
    "    \n",
    "    lasso.fit(X,y)\n",
    "    \n",
    "    return lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-529b23f08a82851b",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "lamb = 0.1\n",
    "check_lasso = lasso_regression(X, y, lamb)\n",
    "np.testing.assert_almost_equal(np.sum(check_lasso.coef_), 0.396970, 6)\n",
    "np.testing.assert_almost_equal(check_lasso.score(X, y), 0.169879, 6)\n",
    "\n",
    "lamb2 = 100\n",
    "check_lasso2 = lasso_regression(X, y, lamb2)\n",
    "np.testing.assert_almost_equal(np.sum(check_lasso2.coef_), 0.003354, 6)\n",
    "np.testing.assert_almost_equal(check_lasso2.score(X, y), 0.146606, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-172f3f73e2ec3f44",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 6.2: Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-189ee85987940647",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_regression(X, y, lamb):\n",
    "    \"\"\"\n",
    "    Implements ridge (L2) regression, using sklearn\n",
    "    \n",
    "    Args: \n",
    "        X (pd.DataFrame): a pandas dataframe containing the features\n",
    "        y (pd.Series): a pandas series containing the target variable\n",
    "        lamb (float): the strength of the regularizer\n",
    "            \n",
    "    Returns:\n",
    "        ridge : a Ridge regression model fitted to X and y\n",
    "    \"\"\"\n",
    "    \n",
    "    # instantiate an instance of sklearn's Ridge model\n",
    "    # use named arguments, such as alpha=... and random_state=...\n",
    "    # use a random state of 42\n",
    "    ridge = Ridge(alpha = lamb, random_state = 42)\n",
    "    \n",
    "    ridge.fit(X,y)\n",
    "    \n",
    "    return ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-619c183e9b281470",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "lamb = 0.1\n",
    "check_ridge = ridge_regression(X, y, lamb)\n",
    "np.testing.assert_almost_equal(np.sum(check_ridge.coef_), 0.401361, 6)\n",
    "np.testing.assert_almost_equal(check_ridge.score(X, y), 0.169880, 6)\n",
    "\n",
    "lamb2 = 100\n",
    "check_ridge2 = ridge_regression(X, y, lamb2)\n",
    "np.testing.assert_almost_equal(np.sum(check_ridge2.coef_), 0.373489, 6)\n",
    "np.testing.assert_almost_equal(check_ridge2.score(X, y), 0.169831, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59b4156f2d6a7299",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 6.3: Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0dd9fa04c84ff594",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "def elasticnet_regression(X, y, lamb, l1_ratio):\n",
    "    \"\"\"\n",
    "    Implements elastic net regression, using sklearn\n",
    "    \n",
    "    Args: \n",
    "        X (pd.DataFrame): a pandas dataframe containing the features\n",
    "        y (pd.Series): a pandas series containing the target variable\n",
    "        lamb (float): total weight of regularization terms\n",
    "        l1_ratio: the ratio of l1 to l2 loss terms\n",
    "            \n",
    "    Returns:\n",
    "        elasticnet : an Elastic Net regression model fitted to X and y\n",
    "    \"\"\"\n",
    "    \n",
    "    # instantiate an instance of sklearn's Elastic Net model\n",
    "    # use named arguments, such as alpha=..., l1_ratio=..., and random_state=...\n",
    "    # use a random state of 42\n",
    "    elasticnet = ElasticNet(alpha = lamb, l1_ratio=l1_ratio, random_state = 42)\n",
    "    \n",
    "    elasticnet.fit(X,y)\n",
    "    \n",
    "    return elasticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-018bf67f10314dce",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "lamb = 0.001\n",
    "l1_ratio = 0.3\n",
    "check_enet = elasticnet_regression(X, y, lamb, l1_ratio)\n",
    "np.testing.assert_almost_equal(np.sum(check_enet.coef_), 0.401365, 6)\n",
    "np.testing.assert_almost_equal(check_enet.score(X, y), 0.169880, 6)\n",
    "\n",
    "lamb2 = 10\n",
    "l1_ratio2 = 0.9\n",
    "check_enet2 = elasticnet_regression(X, y, lamb2, l1_ratio2)\n",
    "np.testing.assert_almost_equal(np.sum(check_enet2.coef_), 0.039402, 6)\n",
    "np.testing.assert_almost_equal(check_enet2.score(X, y), 0.159236, 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
