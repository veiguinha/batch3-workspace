{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU08 - Classification: Learning notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will cover the following: \n",
    "\n",
    "- Practical Introduction to Classification\n",
    "- Classification with Logistic Regression\n",
    "- Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some quick imports to get us started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "# The following is to make some plots along the way \n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# Some quick utilts to avoid filling this notebook with support code \n",
    "from utils import (get_data_iris, \n",
    "                   plot_pair_plots, \n",
    "                   plot_line,\n",
    "                   plot_line_and_annot,\n",
    "                   draw_logit_curve,\n",
    "                   get_sepal_vs_petal_width,\n",
    "                   super_simple_classifier_plot, \n",
    "                   linear_separation_plot, \n",
    "                   predict_probability_point, \n",
    "                   final_classification_plot, \n",
    "                   gradient_descent_classification_plot, \n",
    "                   plot_maximum_log_likelihood)\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_sepal_vs_petal_width' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-62f23c008f4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sepal_vs_petal_width\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data_iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_sepal_vs_petal_width' is not defined"
     ]
    }
   ],
   "source": [
    "df = get_sepal_vs_petal_width()\n",
    "X, y = get_data_iris()\n",
    "X_full, y_full = X.copy(), y.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you just learned about Regression. Now let's tackle a different problem, Classification! You could be asking yourself the following.\n",
    "> _What is Classification and what are its uses?_\n",
    "\n",
    "In a nutshell, Classification is the problem of assigning one of a finite set of classes. By being able to accurately classify a data point without knowing its class, allows us to take specific actions towards it. These actions mostly aim to optimize a certain aspect. For instance, an insurance company would like to predict whether a client will churn. If a client is classified as prone to churn, then the company could offer that same client some exclusive discounts or services. Thus, minimizing the client's odds of actually churning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depdending on the number of existing classes to classify, the problem can be one of two:\n",
    "- **Binary Classification**: A classification task with two possible classes. *Eg*: Hot dog classification (Hot Dog / Not Hot Dog)\n",
    "- **Multiclass Classification**: A classification task with more than two possible classes. *Eg*: Animal classification (Cat / Dog / Bird / ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concepts behind Classification are relatively simple, but performing accurate and generalizable classification models can get quite complex. There are a lot of different algorithms and approaches for this task, but in this learning unit we will look at the simpler (but still useful!) approaches, and introduce some cool new concepts along the way.\n",
    "\n",
    "For this notebook, we will use the tiny and famous Iris database:\n",
    "\n",
    "<img src=\"https://kasperfred.com/media/posts/creating-your-first-machine-learning-classification-model-in-sklearn/cover2_Zz8UOwj.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we made a plot for each pairwise feature relationship and distinguished each iris species through color. As you can see, it is actually very useful to look at data visualizations in classification problems. \n",
    "\n",
    "Now, we could easily make very complex classification that separates the Species 0 from the other two species by making a separating line where $PETAL\\_WIDTH = 0.8$. We will mark all Species 0 iris with the number 0 and the other two as 1. Let's see how the sepal length by petal width plot would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'super_simple_classifier_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e4b65490c3e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msuper_simple_classifier_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'super_simple_classifier_plot' is not defined"
     ]
    }
   ],
   "source": [
    "super_simple_classifier_plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, we just observed a very simple classification approach! However this took a lot of work and it would be great if we could build and use an algorithm that automatically detects the separation lines, like the one we just did, automatically. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maybe we can use a linear regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find a line that separates the two classes, using a simple linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_sepal_vs_petal_width()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='PETAL_WIDTH', \n",
    "        y='SEPAL_WIDTH', c=df['SPECIES'], \n",
    "        cmap='bwr', s=45, figsize=(12, 8), sharex=False, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is a diagonal line that will do a decent job. \n",
    "\n",
    "We could try to treat this as a simple regression problem, right? \n",
    "\n",
    "It's just that instead of having many different targets, we'd just have 0s and 1s. \n",
    "\n",
    "Let's try that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression \n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "# fit (yay scikit!)\n",
    "features = ['PETAL_WIDTH', 'SEPAL_WIDTH']\n",
    "target = 'SPECIES'\n",
    "linear_reg.fit(df[features], df[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's fit. So what are our parameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y = (%0.2f) x PETAL_WIDTH + (%0.2f) x SEPAL_WIDTH + (%0.2f)' % (linear_reg.coef_[0], \n",
    "                                                                 linear_reg.coef_[1], \n",
    "                                                                 linear_reg.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predictions_linreg'] = linear_reg.predict(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='PETAL_WIDTH', \n",
    "        y='SEPAL_WIDTH', c=df['predictions_linreg'], \n",
    "        cmap='seismic', s=45, figsize=(12, 8), sharex=False, );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! So if we want to figure out where our line is, we could simply say it's where our linear regression marks 0.5, right? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are saying that \n",
    "$$ y = \\beta_0 + \\beta_1 * PetalWidth + \\beta_2 * SepalWidth  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we can minimize this error, in a clever way, we will find the best $\\beta_1$ and $\\beta_2$. Cool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separation line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('(%0.2f) + (%0.2f) x PETAL_WIDTH + (%0.2f) x SEPAL_WIDTH = 0.5' % (linear_reg.intercept_,\n",
    "                                                                         linear_reg.coef_[0], \n",
    "                                                                         linear_reg.coef_[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line(df, x='PETAL_WIDTH', y='SEPAL_WIDTH', c=df['predictions_linreg'], linear_reg=linear_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So It looks as if we could (kind of) use this for classification. There are, however, a number of problems with this: \n",
    "\n",
    "- Our model doesn't have any concept of probability. If we just use the line, something is either _\"definitely 0\"_ or _\"definitely 1\"_, and we don't like that. \n",
    "\n",
    "- If we try to use the colors to indicate probability, they are way too conservative. Some of those reds are clearly red, yet they get given numbers such as 0.6, or 0.7.\n",
    "\n",
    "- Also, we don't like the fact that it is predicting \"-0.2\" and \"1.3\". What does that even mean? \n",
    "\n",
    "### What we would like \n",
    "\n",
    "What we would like would be for it to be \"50-50\" on the border, and then to start gaining confidence as the margin increases. In other words, we'd like it to return _\"the probabilty of being 1.\"_\n",
    "\n",
    "Something like... this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_line_and_annot(df, linear_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we move away from the border, we'd get quite confident pretty fast. This curve would look something like this:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_logit_curve(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That is where Logistic Regression comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://image.ibb.co/fbBhEH/download.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its name, the [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) is a classification algorithm. \n",
    "\n",
    "The idea is very simple: you have a linear model, and then apply a logit or sigmoid function. \n",
    "\n",
    "Visually, this it the difference between logistic regression and linear regression: \n",
    "Let's see what this means: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/logit.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, this was linear regression (slightly simplified): \n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 * PetalWidth + \\beta_2 * SepalWidth $$\n",
    "\n",
    "And with logistic regression, we have the following: \n",
    "\n",
    "$$ y = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 * PetalWidth + \\beta_2 * SepalWidth)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is called the sigmoid or logit function. This may look intimidating, but it's actually quite simple. The \"weird\" looking bit is just the linear regression, right? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = \\frac{1}{1 + e^{-(Cool\\ this\\ is\\ just\\ the\\ linear\\ regression!)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"weird\" looking bit can be simply defined as $z$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be asking yourself the following:\n",
    "\n",
    "> _But how does Logistic Regression use this linear boundary to quantify the probability of a data point belonging to a certain class?_\n",
    "\n",
    "Well...\n",
    "<img src=\"https://pics.me.me/its-magic-memegen-com-19255903.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "Not really, once the model is trained, the boundary function is quite simple! Let $x_1$ and $x_2$ be two variables, such as $SEPAL\\_LENGTH$ and $PETAL\\_WIDTH$ as we've been using until now. Also, let $\\hat{p}$ be the probability of an event or class. It should be noted again that Logistic Regression can only classify between two classes at a time, a positive class $+$ and negative class $-$. For example, the $-$ and $+$ classes could represent species 0 or 1, respectively. The resulting function is as follows:\n",
    "\n",
    "$$ln\\left ( \\frac{\\hat{p}}{1-\\hat{p}} \\right ) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\n",
    "\n",
    "Consider a point $(a, b)$. Inputting the values of $x_1$ and $x_2$ into the boundary function, we will get its output $\\beta_0 + \\beta_1 a + \\beta_2 b$. Now depending on the location of $(a, b)$, there are three possibilities to consider:\n",
    "\n",
    "* $(a, b)$ lies in the region defined by points of the $+$ class. As a result, $\\beta_0 + \\beta_1 a + \\beta_2 b$ will be positive, lying somewhere in $(0, \\infty)$. Mathematically, the higher the magnitude of this value, the greater is the distance between the point and the boundary. Intuitively speaking, the greater is the probability that $(a, b)$ belongs to the $+$ class. Therefore, $\\hat{p}$ will lie in $(0.5, 1]$. Let's try this out for a classification task with Logistic Regression between species 0 and 1 with variables $SEPAL\\_LENGTH$ and $PETAL\\_WIDTH$. By observation, we'll choose point $(5,1)$, but you can change it to another one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probability_point(X_full, y_full, (5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $(a, b)$ lies in the region defined by points of the $-$ class. Now, $\\beta_0 + \\beta_1 a + \\beta_2 b$ will be negative, lying in $(-\\infty, 0)$. But like in the positive case, higher the absolute value of the function output, greater the probability that $(a, b)$ belongs to the $-$ class. $\\hat{p}$ will now lie in $[0, 0.5)$. Let's choose such a point for the same classification task as before! By observation, we'll choose point $(5,0.2)$, but you can change it to another one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probability_point(X_full, y_full, (5,0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $(a, b)$ lies ON the decision boundary. In this case, $\\beta_0 + \\beta_1 a + \\beta_2 b = 0$. This means that our model cannot really say whether $(a, b)$ belongs to the $+$ or $-$ class. As a result, $\\hat{p}$ will be exactly $0.5$. Such a point would be $(5,0.7)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probability_point(X_full, y_full, (5,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgflip.com/286p5c.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other side, the $\\frac{\\hat{p}}{1-\\hat{p}}$ function is called the the $odds \\ ratio$ function $OR(X)$, which is essentially the ratio of the probability of an observation belonging to a certain class vs. it not belonging. Probability and odds convey the exact same information, but as $\\hat{p}$ ranges $[0, 1]$, $OR(X)$ ranges $(0, \\infty)$.\n",
    "\n",
    "That was enough theory. Imagine that we want to classify iris flowers between species 0 and 1 by only using $SEPAL\\_LENGTH$ and $PETAL\\_WIDTH$ as $x_1$ and $x_2$, respectively. Suppose we already have a logistic regression model trained which coefficients are:\n",
    "\n",
    "$$\\beta_0 = -0.92210397 \\\\ \\beta_1 = -0.4441176 \\\\ \\beta_2 = 4.47663504$$ \n",
    "\n",
    "If we want to obtain the probability $\\hat{p}$ of a sample $(5,1)$ belonging to class $+$ we do the following.  \n",
    "\n",
    "* Compute the boundary function $\\beta_0 + \\beta_1 a + \\beta_2 b$ which output we will simply call $z$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = -0.92210397 + -0.4441176 * 5 + 4.47663504 * 1\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the $odds \\ ratio$, through $OR(X) = e^z$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OR = np.exp(z)\n",
    "print(OR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute $\\hat{p}$ = $\\frac{e^z}{1 + e^z} = \\frac{1}{1 + e^{-z}}$, called the $logistic \\ function$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = OR / (1 + OR)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was exactly what the $predict\\_probability\\_point$ function did for point $(5,1)$ previously!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Probability Threshold to Perform Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how Logistic Regression estimates class probabilities $\\hat{p}$, but how do we define if an iris plant belongs to the + or - class? \n",
    "\n",
    "For this binary classification task we can simply say that if for a given iris plant $\\hat{p} >= threshold$ then it belongs to the + class else it belongs to the - class. By default, binary classification algorithms use $threshold=0.5$.\n",
    "\n",
    "However, $threshold$ can be modified to have values between $[0, 1]$. This can be useful in cases where one of the classes has many fewer data samples compared to the other. Such problems are said to have a class imbalance, which is not the case for this problem.\n",
    "\n",
    "Let's try to classify between species 0 and 1 using several different $threshold$ values and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_classification_plot(X_full, y_full, threshold_values=[0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the results vary by modifying the threshold and that the results are the best when $threshold=0.5$ for this particular case.\n",
    "\n",
    "Now that we understood how to use the logistic regression formula to obtain probabilities and defining thresholds, we can now learn how to train the model and obtain the coefficients and intercept. That is where cost functions come in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the focus is on learning from data, right? And how can a model learn from data like an individual can learn to play an instrument? Through minimizing a cost function! An aspiring new musician would like to minimize the number of times he plays out of tune, which would be his cost function. He will learn how to play the instrument correctly through trial and error as well as feedback, thus minimizing his cost function. \n",
    "\n",
    "A machine learning model does just the same, it aims to minimize a cost function which is a measure of how wrong it is in terms of its ability to estimate the relationship between $X$ and $Y$. Therefore, the aim of a machine learning model is to find the coefficients, parameters, weights or a structure that minimizes a cost function, which it will iteratively do.\n",
    "\n",
    "<img src=\"https://i.imgflip.com/288yvb.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we shouldn't use the same cost function Mean Squared Error as we did for linear regression. Why? Because our prediction function is non-linear (due to the logistic function). Squaring this prediction as we do in MSE results in a non-convex function with many local minima. If our cost function has many local minima, gradient descent may not find the optimal global minima! So let's just trust in our old friend Boromir.\n",
    "\n",
    "<img src=\"https://i.imgflip.com/289uki.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of Mean Squared Error, we use a cost function called Maximum Likelihood, adapted to log-likelihood ($i.e.$ Maximum Log-Likelihood). For a dataset with $N$ observations, $y_N$ true labels and $\\hat{p}_N$ predictions it is defined as follows:\n",
    "\n",
    "$$H_{\\hat{p}}(y) = - \\frac{1}{N}\\sum_{i=1}^{N} \\left [{ y_i \\ \\log(\\hat{p}_i) + (1-y_i) \\ \\log (1-\\hat{p}_i)} \\right ]$$\n",
    "\n",
    "It's better to first observe its behaviour. Let's plot two charts with the predicted probability $\\hat{p}$ going from $0$ to $1$, but for two different real labels $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_maximum_log_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the key thing to note is that the cost function penalizes confident and wrong predictions more than it rewards confident and right predictions! Now that we know the cost function that is mostly used for Logistic Regression let's now look at how a loss function can be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know the concept behind the cost function, you may be asking to yourself how we can minimize it. That is where gradient descent comes in! It is one of the most efficient optimization algorithms which attempts to find a local or global minima of a function. \n",
    "\n",
    "Gradient descent allows a machine learning model to learn the gradient (*aka* the direction) that the model should take in order to reduce errors and thus minimizing the cost function. \n",
    "\n",
    "In a simplified way, gradient descent evaluates a point, and finds the \"direction\" where the cost function goes down the most. The easiest metaphor is a river going down a mountain. It will always go down the \"steepest\" possible route. \n",
    "\n",
    "![](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent.png)\n",
    "\n",
    "In the Logistic Regression example, direction refers to how the model parameters $\\beta_0$, $\\beta_1$ and $\\beta_2$ should be tweaked or corrected as to reduce the cost function. As the model iterates, it tends to a minimum where any further changes to the parameters produces little or any changes in the cost function. In such a case, it is said that it converged. We have an example below where we can see gradient descent in action.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ZmzSnV6xluGa42wtU7KYVA.gif\" style=\"width: 400px;\"/>\n",
    "\n",
    "An alternative to gradient descent would be brute forcing a potentially infinite combination of parameters until the set of parameters that minimizes the cost is found. However, this is not feasible and, thus, gradient descent enables the learning process to make corrective updates to the learned estimates that move the model towards an optimal combination of parameters.\n",
    "\n",
    "How interesting would it be to actually observe Logistic Regression do gradient descent and change its decision boundary? Let's do just that! Let's observe 25 iterations of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this cell takes a bit of time to run, but is pretty \n",
    "gradient_descent_classification_plot(X_full, y_full, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the decision boundary starts quite wide and somewhat horizontal and then starts to shrink and move between the two iris species!\n",
    "\n",
    "Now you should be asking yourself how can you apply gradient descent to modify the model's parameters and minimize the cost function. For that, let's refresh our memory on how we can predict a probability only with an intercept $\\beta_0$ and coefficient $\\beta_1$. \n",
    "\n",
    "$$ \\hat{p} = \\frac{e^{\\beta_0 + \\beta_1 x_1}}{1 + e^{\\beta_0 + \\beta_1 x_1}}$$\n",
    "\n",
    "This can be further simplified into\n",
    "\n",
    "$$ \\hat{p} = \\frac{1.0}{1.0 + e^{(-(\\beta_0 + \\beta_1 x_1))}}$$\n",
    "\n",
    "But wait! We still don't know what Gradient Descent really means! Basically, is it the process of minimizing a function by following the gradients of the cost function. This involves knowing the form of the cost as well as the derivative so that from a given point you know the gradient and can move in that direction, e.g. downhill towards the minimum value.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "In machine learning, we can use a technique that evaluates and updates the coefficients every iteration called stochastic gradient descent to minimize the error of a model on our training data. The way this optimization algorithm works is that each training instance is shown to the model one at a time. The model makes a prediction for a training instance, the error is calculated and the model is updated in order to reduce the error for the next prediction. This procedure can be used to find the set of coefficients in a model that result in the smallest error for the model on the training data. Each iteration, the coefficients ($\\beta$) in machine learning language are updated using the equation:\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t - learning\\_rate \\frac{\\partial H_{\\hat{p}}(y)}{\\partial \\beta_t}$$\n",
    "\n",
    "which we can simplify to\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t + learning\\_rate \\left [(y - \\hat{p}) \\ \\hat{p} \\ (1 - \\hat{p}) \\ x \\right]$$\n",
    "\n",
    "where $learning\\_rate$ is a learning rate that you must configure (*e.g*. 0.1). You think of learning rate as how big a step you want to take in each iteration. Small steps converge slower, while big steps are faster but might not converge. \n",
    "\n",
    "Exceptionally, the intercept is updated without an observation as it is not associated with a specific observation.\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} - learning\\_rate \\frac{\\partial H_{\\hat{p}}(y)}{\\partial \\beta_{0(t)}}$$\n",
    "\n",
    "which we can also simplify to\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} + learning\\_rate \\left [(y - \\hat{p}) \\ \\hat{p} \\ (1 - \\hat{p})\\right]$$\n",
    "\n",
    "We can see that each sample has a big impact on the convergence. What happens if the data is sorted by class or in this case species? In the beginning the algorithm would only use data from Species 0 plants which could make it converge before even using data from other species! This is why we should always shuffle the data prior performing Stochastic. Gradient Descent\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "Stochastic gradient descent can be very noisy which makes it difficult to converge properly to the global minima. This happens because the model parameters are updated with each individual sample. A simple approach that reduces the noise could be to average the gradient over all training samples. This approach exists and it is called **Batch Gradient Descent**. The downside of using Batch instead of Stochastic is that it is slower. We can update a specific coefficient ($\\beta$) as follows:\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t + learning\\_rate \\frac{1}{N} \\sum_{i=1}^{N} \\left [(y_i - \\hat{p}_i) \\ \\hat{p}_i \\ (1 - \\hat{p}_i) \\ x_i \\right]$$\n",
    "\n",
    "And the intercept:\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} + learning\\_rate \\frac{1}{N} \\sum_{i=1}^{N} \\left [(y_i - \\hat{p}_i) \\ \\hat{p}_i \\ (1 - \\hat{p}_i) \\right]$$\n",
    "\n",
    "It should be noted that the formulas were already simplified to leave most of the math out. If you run this iterative process many times, you should obtain the coefficients that converge to a minimum of the cost function and thus, your logistic regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "It is crucial that your variables are adjusted between $[0;1]$ (normalized) or standardized so that you can correctly analyse some logistic regression coefficients for your possible future employer. Additionally, it is also important to normalize your data if your algorithm is doing gradient descent which will lead to a much faster convergence. \n",
    "\n",
    "To adjust your variables between $[0;1]$, you would apply the following formula:\n",
    "\n",
    "$$ x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "which is called normalization. It is a simple formula, but Scikit-Learn has this transformation already implemented called `MinMaxScaler`. To do so, you would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sclr = MinMaxScaler().fit(X_full)\n",
    "normalized_X = sclr.transform(X_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from then, you could confidently apply Logistic Regression to the normalized data and interpret the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sklearn Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "We now know most of what we need to know about Logistic Regression. If you understood everything until know then you should have an idea of its strengths and weaknesses:\n",
    "* Strengths: Outputs have a probabilistic interpretation, and can be updated easily with new data using stochastic gradient descent. Additionally, the algorithm can be regularized to avoid overfitting. \n",
    "* Weaknesses: Logistic regression tends to underperform when there are multiple or non-linear decision boundaries. They are not flexible enough to naturally capture more complex relationships.\n",
    "\n",
    "Fortunately, there are already a lot of logistic regression implementations that are already quite computationally efficient. So, there's no need for you to code them from scratch. You can find one of those implementations in sklearn! You can import it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation can use these optimization methods:\n",
    "* liblinear: default;\n",
    "* newton-cg;\n",
    "* lbfgs; \n",
    "* sag;\n",
    "* saga.\n",
    "\n",
    "Now that you've loaded the algorithm, you need to first create the classifier object. We will set `random_state` to $0$ just to make sure we obtain the exact same results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_clf = LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now you just created the algorithm object that is ready to learn from some juicy data. Now you have to give it the input data $X$ as well as the corresponding labels $y$ so that it can train and converge to the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_clf.fit(normalized_X, y_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voilà! You just created your first logistic regression model. Now, it would be interesting to know how good the model is! And we can do so, by checking the percentage of correctly predicted observations on the data by doing the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_clf.score(normalized_X, y_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh we were right 84.7% of the time, not bad! What if we want to get the predictions for each sample instead of the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logit_clf.predict(normalized_X)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! These are the `argmax` of the predicted probabilities, in other words, the class with highest predicted probability for ech sample. However, it would be more interesting to obtain and observe the probabilities of each class! We can do it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = logit_clf.predict_proba(normalized_X)\n",
    "probas[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can do whatever we want with the predicted probabilities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may now be legitimately asking... \n",
    "\n",
    "> _\"Why didn't the classifier just return probability of being 1, instead of making me jump through hoops?\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgflip.com/2ntgpm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that instead of having 2 types of flower, we have 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the SPECIES column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([X_full, y_full], axis=1)\n",
    "display(df_full.sample(5, random_state=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some predictions with this dataset. Same logic as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciate\n",
    "logit = LogisticRegression()\n",
    "\n",
    "# fit \n",
    "logit.fit(normalized_X, y_full)\n",
    "\n",
    "# predict (optional)\n",
    "predictions = logit.predict(normalized_X)\n",
    "\n",
    "# predict probabilities (generally what we want)\n",
    "probability_predictions = logit.predict_proba(normalized_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first 5 predicted probabilities \n",
    "probability_predictions[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we read this... The first flower can be class 0 or 1, but if very unlikely to be class 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another flower, the one at position 50: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_predictions[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so this flower: \n",
    "- is likely to be of class 2 (53% probability)\n",
    "- may still be of class 2 (38% probability) \n",
    "- is almost certainly not of class 0. \n",
    "\n",
    "The algorithm has a lot more arguments that you can change to modify its behaviour as well as other methods that you can explore. Feel free to explore them here: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
