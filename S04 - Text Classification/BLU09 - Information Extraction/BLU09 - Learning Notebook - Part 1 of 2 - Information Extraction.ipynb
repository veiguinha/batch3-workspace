{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Information Extraction\n",
    "\n",
    "Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. Information Extraction may be presented in three subtasks:\n",
    "\n",
    "* **Named Entity Recognition**, retrieve entities (like persons, location, etc.) in the text. \n",
    "* **Relation Extraction**, find the relation between two entities in the text.\n",
    "* **Template Filling**, find the correct entity to fill a certain template, for instance.\n",
    "\n",
    "In this BLU we are going to learn some of the basic techniques to extract specific (pre-specified) information from textual sources. From the three specified task, we are going to **focus on the task of named-entity recognition (NER)** where our objective is to **retrieve all the mentions** of entities like persons, locations, time, among others. The other two are mentioned for the sake of completeness and you should definitely research more about them, specially if you're eager to learn more about NLP.\n",
    "\n",
    "![robot entities](./media/robot_entities.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work in a corpus containing forum discussions. We extracted a sample from Reddit for this use. For more interesting examples, you may find more textual data available at https://files.pushshift.io/reddit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I read 1000 documents\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "with open('./datasets/sample_data.json') as fp:\n",
    "    for line in fp:\n",
    "        entry = json.loads(line)\n",
    "        docs.append(entry['body'])\n",
    "        \n",
    "print('I read {} documents'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction with Regular Expressions\n",
    "\n",
    "In BLU7, we became pros of regular expressions. We're going to try to use them to our task of recognizing entities. Take a moment to think about all the possibilities of Entities that we can find in a text. Do you think such a task will be achievable using only regular expressions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regex](./media/regex.gif \"regex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, let's say that your boss asked you to retrieve all the **dates** mentioned in our sample corpus. We learned in BLU7 that it is easy to use a regular expression for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14/09/30', '7/12/2007', '4/16/2007', '3/27/2007', '2/28/2007']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find all possible dates in the format xx/xx/xxxx\n",
    "data = ' '.join(docs)\n",
    "re.findall('\\d{1,2}/\\d{1,2}/\\d{2,4}', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this looks like it's going to be a breeze. However, now your boss decides to ask you to retrieve all the **country names** which appear in the corpus instead. \n",
    "\n",
    "One possible approach is to retrieve a list of all countries that exist and look for the occurence of such elements in the corpus. Let's try that, shall we?\n",
    "\n",
    "![alt text](./media/countries_meme.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = []\n",
    "with open('./datasets/countries.txt') as fp:\n",
    "    for line in fp:\n",
    "        countries.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use again regular expressions for this. Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afghanistan',\n",
       " 'Albania',\n",
       " 'Algeria',\n",
       " 'American\\\\ Samoa',\n",
       " 'Andorra',\n",
       " 'Angola',\n",
       " 'Anguilla',\n",
       " 'Antarctic\\\\ Lands',\n",
       " 'Antarctica',\n",
       " 'Antigua',\n",
       " 'Antigua\\\\ and\\\\ Barbuda',\n",
       " 'Argentina',\n",
       " 'Armenia',\n",
       " 'Aruba',\n",
       " 'Ashmore\\\\ Islands',\n",
       " 'Ashmore\\\\ and\\\\ Cartier\\\\ Islands',\n",
       " 'Australia',\n",
       " 'Austria',\n",
       " 'Azerbaijan',\n",
       " 'Bahamas',\n",
       " 'Bahrain',\n",
       " 'Baker\\\\ Island',\n",
       " 'Bangladesh',\n",
       " 'Barbados',\n",
       " 'Barbuda',\n",
       " 'Bassas\\\\ da\\\\ India',\n",
       " 'Belarus',\n",
       " 'Belgium',\n",
       " 'Belize',\n",
       " 'Benin',\n",
       " 'Bermuda',\n",
       " 'Bhutan',\n",
       " 'Bolivia',\n",
       " 'Borneo',\n",
       " 'Bosnia',\n",
       " 'Bosnia\\\\ Herzegovina',\n",
       " 'Bosnia\\\\ and\\\\ Herzegovina',\n",
       " 'Botswana',\n",
       " 'Bouvet\\\\ Island',\n",
       " 'Brazil',\n",
       " 'Britain',\n",
       " 'British\\\\ Indian\\\\ Ocean\\\\ Territory',\n",
       " 'British\\\\ Virgin\\\\ Islands',\n",
       " 'Brunei',\n",
       " 'Bulgaria',\n",
       " 'Burkina\\\\ Faso',\n",
       " 'Burma',\n",
       " 'Burundi',\n",
       " 'Caicos\\\\ Islands',\n",
       " 'Cambodia',\n",
       " 'Cameroon',\n",
       " 'Canada',\n",
       " 'Cape\\\\ Verde',\n",
       " 'Cartier\\\\ Islands',\n",
       " 'Cayman\\\\ Islands',\n",
       " 'Central\\\\ African\\\\ Republic',\n",
       " 'Chad',\n",
       " 'Chile',\n",
       " 'China',\n",
       " 'Christmas\\\\ Island',\n",
       " 'Clipperton',\n",
       " 'Clipperton\\\\ Island',\n",
       " 'Cocos',\n",
       " 'Colombia',\n",
       " 'Comoros',\n",
       " 'Congo',\n",
       " 'Democratic\\\\ Republic\\\\ of\\\\ the\\\\ Congo',\n",
       " 'Republic\\\\ of\\\\ the\\\\ Congo',\n",
       " 'Cook\\\\ Islands',\n",
       " 'Coral\\\\ Sea\\\\ Islands',\n",
       " 'Costa\\\\ Rica',\n",
       " \"Cote\\\\ D\\\\ 'Ivoire\",\n",
       " \"Cote\\\\ d\\\\ 'Ivoire\",\n",
       " 'Croatia',\n",
       " 'Cuba',\n",
       " 'Cyprus',\n",
       " 'Czech\\\\ Republic',\n",
       " 'Czechoslovakia',\n",
       " 'Democratic\\\\ Republic\\\\ of\\\\ the\\\\ Congo',\n",
       " 'Denmark',\n",
       " 'Djibouti',\n",
       " 'Dominica',\n",
       " 'Dominican\\\\ Republic',\n",
       " 'East\\\\ Timor',\n",
       " 'Ecuador',\n",
       " 'Egypt',\n",
       " 'El\\\\ Salvador',\n",
       " 'England',\n",
       " 'Equatorial\\\\ Guinea',\n",
       " 'Eritrea',\n",
       " 'Estonia',\n",
       " 'Ethiopia',\n",
       " 'Europa\\\\ Island',\n",
       " 'European\\\\ Union',\n",
       " 'Falkland\\\\ Islands',\n",
       " 'Faroe\\\\ Islands',\n",
       " 'Federated\\\\ States\\\\ of\\\\ Micronesia',\n",
       " 'Fiji',\n",
       " 'Finland',\n",
       " 'Former\\\\ Yugoslav\\\\ Republic\\\\ of\\\\ Macedonia',\n",
       " 'France',\n",
       " 'French\\\\ Guiana',\n",
       " 'French\\\\ Polynesia',\n",
       " 'French\\\\ Southern',\n",
       " 'French\\\\ Southern\\\\ and\\\\ Antarctic\\\\ Lands',\n",
       " 'FRG',\n",
       " 'Gabon',\n",
       " 'Gambia',\n",
       " 'Georgia',\n",
       " 'Germany',\n",
       " 'Ghana',\n",
       " 'Gibraltar',\n",
       " 'Glorioso',\n",
       " 'Glorioso\\\\ Islands',\n",
       " 'Greece',\n",
       " 'Greenland',\n",
       " 'Grenadan',\n",
       " 'Guadeloupe',\n",
       " 'Guam',\n",
       " 'Guatemala',\n",
       " 'Guernsey',\n",
       " 'Guinea',\n",
       " 'Guinea\\\\-Bissau',\n",
       " 'Guyana',\n",
       " 'Haiti',\n",
       " 'Heard\\\\ Island',\n",
       " 'Heard\\\\ Island\\\\ and\\\\ McDonald\\\\ Islands',\n",
       " 'Herzegovina',\n",
       " 'Holland',\n",
       " 'Holy\\\\ See',\n",
       " 'Honduras',\n",
       " 'Hong\\\\ Kong',\n",
       " 'Howland\\\\ Island',\n",
       " 'Hungary',\n",
       " 'Iceland',\n",
       " 'India',\n",
       " 'Indonesia',\n",
       " 'Iran',\n",
       " 'Iraq',\n",
       " 'Ireland',\n",
       " 'Islas\\\\ Malvinas',\n",
       " 'Isle\\\\ of\\\\ Man',\n",
       " 'Israel',\n",
       " 'Italy',\n",
       " 'Ivory\\\\ Coast',\n",
       " 'Jamaica',\n",
       " 'Jan\\\\ Mayen',\n",
       " 'Japan',\n",
       " 'Jarvis\\\\ Island',\n",
       " 'Jersey',\n",
       " 'Johnston\\\\ Atoll',\n",
       " 'Jordan',\n",
       " 'Juan\\\\ de\\\\ Nova\\\\ Island',\n",
       " 'Kazakhstan',\n",
       " 'Keeling\\\\ Islands',\n",
       " 'Kenya',\n",
       " 'Kingman\\\\ Reef',\n",
       " 'Kiribati',\n",
       " 'Korea',\n",
       " 'Kuwait',\n",
       " 'Kyrgyzstan',\n",
       " 'Laos',\n",
       " 'Latvia',\n",
       " 'Lebanon',\n",
       " 'Lesotho',\n",
       " 'Liberia',\n",
       " 'Libya',\n",
       " 'Liechtenstein',\n",
       " 'Lithuania',\n",
       " 'Luxembourg',\n",
       " 'Macau',\n",
       " 'Macedonia',\n",
       " 'Madagascar',\n",
       " 'Malawi',\n",
       " 'Malaysia',\n",
       " 'Maldives',\n",
       " 'Mali',\n",
       " 'Malta',\n",
       " 'Man\\\\ ,\\\\ Isle\\\\ of',\n",
       " 'Marshall\\\\ Islands',\n",
       " 'Martinique',\n",
       " 'Mauritania',\n",
       " 'Mauritius',\n",
       " 'Mayotte',\n",
       " 'McDonald\\\\ Islands',\n",
       " 'Mexico',\n",
       " 'Micronesia',\n",
       " 'Midway\\\\ Islands',\n",
       " 'Moldova',\n",
       " 'Monaco',\n",
       " 'Mongolia',\n",
       " 'Montenegro',\n",
       " 'Montserrat',\n",
       " 'Morocco',\n",
       " 'Mozambique',\n",
       " 'Myanmar',\n",
       " 'Namibia',\n",
       " 'Nauru',\n",
       " 'Navassa\\\\ Island',\n",
       " 'Nepal',\n",
       " 'Netherlands',\n",
       " 'Netherlands\\\\ Antilles',\n",
       " 'Nevis',\n",
       " 'New\\\\ Caledonia',\n",
       " 'New\\\\ Guinea',\n",
       " 'New\\\\ Zealand',\n",
       " 'Nicaragua',\n",
       " 'Niger',\n",
       " 'Nigeria',\n",
       " 'Niue',\n",
       " 'Norfolk\\\\ Island',\n",
       " 'North\\\\ Korea',\n",
       " 'North\\\\ Vietnam',\n",
       " 'Northern\\\\ Mariana\\\\ Islands',\n",
       " 'Norway',\n",
       " 'Oman',\n",
       " 'PRC',\n",
       " 'Pakistan',\n",
       " 'Palau',\n",
       " 'Palestine',\n",
       " 'Palmyra\\\\ Atoll',\n",
       " 'Panama',\n",
       " 'Papua',\n",
       " 'Papua\\\\ New\\\\ Guinea',\n",
       " 'Paracel\\\\ Islands',\n",
       " 'Paraguay',\n",
       " 'People\\\\ Republic\\\\ of\\\\ China',\n",
       " 'Persia',\n",
       " 'Peru',\n",
       " 'Philippines',\n",
       " 'Pitcairn\\\\ Islands',\n",
       " 'Poland',\n",
       " 'Portugal',\n",
       " 'Puerto\\\\ Rico',\n",
       " 'Qatar',\n",
       " 'Republic\\\\ of\\\\ the\\\\ Congo',\n",
       " 'Republic\\\\ of\\\\ Chile',\n",
       " 'Reunion',\n",
       " 'Romania',\n",
       " 'Russia',\n",
       " 'Rwanda',\n",
       " 'Saint\\\\ Helena',\n",
       " 'Saint\\\\ Kitts',\n",
       " 'Saint\\\\ Kitts\\\\ and\\\\ Nevis',\n",
       " 'Saint\\\\ Lucia',\n",
       " 'Saint\\\\ Pierre\\\\ and\\\\ Miquelon',\n",
       " 'Saint\\\\ Vincent\\\\ and\\\\ the\\\\ Grenadines',\n",
       " 'Samoa',\n",
       " 'San\\\\ Marino',\n",
       " 'Sao\\\\ Tome\\\\ and\\\\ Principe',\n",
       " 'Saudi\\\\ Arabia',\n",
       " 'Senegal',\n",
       " 'Serbia',\n",
       " 'Serbia\\\\ and\\\\ Montenegro',\n",
       " 'Seychelles',\n",
       " 'Sierra\\\\ Leone',\n",
       " 'Singapore',\n",
       " 'Slovakia',\n",
       " 'Slovenia',\n",
       " 'Solomon\\\\ Islands',\n",
       " 'Somalia',\n",
       " 'South\\\\ Africa',\n",
       " 'South\\\\ Georgia',\n",
       " 'South\\\\ Georgia\\\\ and\\\\ the\\\\ South\\\\ Sandwich\\\\ Islands',\n",
       " 'South\\\\ Korea',\n",
       " 'South\\\\ Sandwich\\\\ Isklands',\n",
       " 'South\\\\ Sandwich\\\\ Islands',\n",
       " 'South\\\\ Vietnam',\n",
       " 'Soviet',\n",
       " 'soviet',\n",
       " 'Soviet\\\\ Union',\n",
       " 'Spain',\n",
       " 'Spratly\\\\ Islands',\n",
       " 'Sri\\\\ Lanka',\n",
       " 'Sudan',\n",
       " 'Suriname',\n",
       " 'Svalbard',\n",
       " 'Swaziland',\n",
       " 'Sweden',\n",
       " 'Switzerland',\n",
       " 'Syria',\n",
       " 'Taiwan',\n",
       " 'Tajikistan',\n",
       " 'Tanzania',\n",
       " 'Thailand',\n",
       " 'Tibet',\n",
       " 'Tobago',\n",
       " 'Togo',\n",
       " 'Tokelau',\n",
       " 'Tonga',\n",
       " 'Trinidad',\n",
       " 'Trinidad\\\\ and\\\\ Tobago',\n",
       " 'Tromelin\\\\ Island',\n",
       " 'Tunisia',\n",
       " 'Turkey',\n",
       " 'Turkmenistan',\n",
       " 'Turks',\n",
       " 'Turks\\\\ and\\\\ Caicos\\\\ Islands',\n",
       " 'Tuvalu',\n",
       " 'U\\\\.K\\\\.',\n",
       " 'U\\\\.S\\\\.',\n",
       " 'U\\\\.k\\\\.',\n",
       " 'UK',\n",
       " 'US',\n",
       " 'USA',\n",
       " 'U\\\\.S\\\\.A\\\\.',\n",
       " 'USSR',\n",
       " 'Uganda',\n",
       " 'Ukraine',\n",
       " 'United\\\\ Arab\\\\ Emirates',\n",
       " 'United\\\\ Kingdom',\n",
       " 'United\\\\ States',\n",
       " 'United\\\\ States\\\\ of\\\\ America',\n",
       " 'Uruguay',\n",
       " 'Uzbekistan',\n",
       " 'Vanuatu',\n",
       " 'Vatican\\\\ City',\n",
       " 'Vatican',\n",
       " 'Venezuela',\n",
       " 'Vietnam',\n",
       " 'Virgin\\\\ Islands',\n",
       " 'Wake\\\\ Island',\n",
       " 'Wallis\\\\ and\\\\ Futuna',\n",
       " 'Western\\\\ Sahara',\n",
       " 'Yemen',\n",
       " 'Yugoslav\\\\ Republic\\\\ of\\\\ Macedonia',\n",
       " 'Yugoslavia',\n",
       " 'Zaire',\n",
       " 'Zambia',\n",
       " 'Zimbabwe',\n",
       " 'Wales']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.escape(c) for c in countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('us', 763, 765)\n",
      "('United States', 827, 840)\n",
      "('UK', 6971, 6973)\n",
      "('US', 7000, 7002)\n",
      "('Puerto rico', 8026, 8037)\n",
      "('us', 8638, 8640)\n",
      "('France', 19815, 19821)\n",
      "('us', 21563, 21565)\n",
      "('Puerto Rico', 27659, 27670)\n",
      "('Puerto Rico', 27754, 27765)\n",
      "('US', 28101, 28103)\n",
      "('Canada', 29439, 29445)\n",
      "('USA', 32880, 32883)\n",
      "('Norway', 34749, 34755)\n",
      "('Korea', 34837, 34842)\n",
      "('USA', 35738, 35741)\n",
      "('United States', 41060, 41073)\n",
      "('us', 42290, 42292)\n",
      "('us', 42403, 42405)\n",
      "('Soviet', 44563, 44569)\n",
      "('us', 49625, 49627)\n",
      "('Chad', 51352, 51356)\n"
     ]
    }
   ],
   "source": [
    "# Sort country list by length. This is important to match longer spans before short \n",
    "# ones (like in 'Papua New Guinea' vs. 'Papua')\n",
    "countries.sort(key=len, reverse=True)\n",
    "\n",
    "# Make a regex to recognize all possible names.\n",
    "# '|' creates the or operation in regex\n",
    "# \\b means word boundaries (punctuation or white spaces)\n",
    "# re.escape is used to escape regex operators like '.'    \n",
    "countries_regex = r'\\b(' + '|'.join([re.escape(c) for c in countries]) + r')\\b'\n",
    "\n",
    "# finditer is similar to findall\n",
    "# the flag re.I means to ignore casing (accept both lowercase and uppercase letters as the same)\n",
    "for i, m in enumerate(re.finditer(countries_regex, data, flags=re.I)):\n",
    "    print( (m.group(), m.start(), m.end()) )\n",
    "    # just show the first 20\n",
    "    if i > 20:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this approach working?**\n",
    "\n",
    "It seems like the word **'us'**, for example, has caused some confusion. It could be the country _U.S._, or just the pronoun _us_. In this case, just comparing the word form we are not able to disambiguate the two forms. We will need either more **context** or more **linguistic information** and regular expression won't give us none of that.\n",
    "\n",
    "Luckily, you already know an NLP library which can provide you the correct information to disambiguate the word 'us'. In the next examples, we will use SpaCy as our NLP toolkit to give us just that.\n",
    "\n",
    "## Deeper look in information extraction using SpaCy\n",
    "![Spacy](./media/spacy.jpg)\n",
    "\n",
    "If you remember BLU8, we used SpaCy to understand word vectors (aka word embeddings). We will make use of the medium sized SpaCy english model once again. In case you haven't downloaded it yet, here's the command once again:\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_md\n",
    "```\n",
    "    \n",
    "But of course we could have used any english model (en_core_web_sm, en_core_web_md, en_core_web_lg) provided by SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are disabling the synctatic parser from pipeline to improve speed.\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SpaCy, we will process the documents with the complete NLP pipeline using [pipe](https://spacy.io/usage/processing-pipelines). This means that `pipe` will process our text, tokenize it and extract information from it using all the CPU cores from our machine. Concretely, it will Part-of-Speech tag (more on that later), parse and extract entities.\n",
    "\n",
    "We won't get into details on how SpaCy does this -- what matters is that it uses fast machine learning models with good enough accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use the function pipe to process all documents.\n",
    "# One of the strenghts for SpaCy is the parallel processing using all your computer cores.\n",
    "# In this step, SpaCy performs the NLP pipeline for all the docs, so it may take a while.\n",
    "docs = list(nlp.pipe(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we want to do NER (Named Entity Extraction) in a piece of text. We can get an example sentence from our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRR Tolkien. Gandalf, Aragorn, Frodo, Bilbo Baggins, Gollum...\n"
     ]
    }
   ],
   "source": [
    "example = docs[631]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SpaCy, it's really easy to extract entities - we can simply use `.ents` in our previously processed text, and SpaCy will use its built-in model to get the entities present in the text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRR Tolkien 0 11 PERSON\n",
      "Gandalf 13 20 PERSON\n",
      "Aragorn 22 29 PERSON\n",
      "Frodo 31 36 PERSON\n",
      "Bilbo Baggins 38 51 PERSON\n",
      "Gollum 53 59 PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in example.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example sentence, SpaCy correctly labels all these LOTR characters with the Person entity. You could further argue that Gandalf is a wizard and Frodo/Bilbo are hobbits, but let's not penalize SpaCy on that one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our text is processed and we know how to get entities, let's build a `Matcher` in SpaCy.\n",
    "\n",
    "A `Matcher` is SpaCy's version of a regular expression - it searches for patterns in your text, according to the rules you give it. However, it is much more powerful since it has access to the outputs of the aforementioned NLP pipeline. That means we can search patterns that include certain entities or Part-of-Speech tags. \n",
    "\n",
    "In this `Matcher` we will define templates which we will use later to match elements in the text (thus using it to do information extraction). The `Matcher` is initialized using the vocabulary object, which must be shared with the documents the matcher will operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab) # Pass the vocabulary object to Matcher.__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a similar matcher as we did above with regular expressions. We are going to get each country name and add it as a pattern to the `matcher`. To add a pattern, we can simply use `.add()`. It receives:\n",
    "\n",
    "- an ID (the name we want to give our pattern)\n",
    "- a callable function that is called when there is a match (we're not going to use anything)\n",
    "- the pattern itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    # Build a pattern from the country name. For example: United States -> [{'LOWER': 'united'}, {'LOWER': 'states'}]\n",
    "    # LOWER means to match the words in the lowercased token.\n",
    "    pattern = [{'LOWER': c.lower()} for c in country.split()]\n",
    "    matcher.add(country, None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 2 us\n",
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "64 18 20 Puerto rico\n",
      "69 50 51 us\n",
      "146 4 5 France\n",
      "167 29 30 us\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 78 79 Norway\n",
      "263 101 102 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "320 35 36 us\n",
      "320 58 59 us\n",
      "335 38 39 Soviet\n",
      "335 38 39 Soviet\n",
      "349 4 5 us\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "369 41 42 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "# for screen economy, let's just show the matches for the first 400 documents.\n",
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned, in order to disambiguate the retrieval of 'U.S.' vs 'us' we need to add more linguistic information to the `matcher`. Let's play with Part of Speech (PoS).\n",
    "\n",
    "## But what is Part-of-Speech?\n",
    "\n",
    "If you remember from your language classes, you could categorize words in a sentence according to the role they have in it. In NLP, we call this Part of Speech tags. For the English language, common PoS tags are: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection.\n",
    "\n",
    "SpaCy adopts the Universal PoS tagset where any language has a common subset of PoS defined. The list of all possible values can be consulted [here](https://spacy.io/api/annotation#pos-tagging).\n",
    "\n",
    "In this case, we are interested in matching the country names that were tagged as **Proper Nouns** ('PROPN' tag obtained from the tagset list).\n",
    "\n",
    "![Pronoun meme](./media/pronoun.jpg)\n",
    "\n",
    "In SpaCy, just as entities of a document are inside `doc.ents`, for each token of a document we can find its assigned POS tag by using `.pos_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRR PROPN\n",
      "Tolkien PROPN\n",
      ". PUNCT\n",
      "Gandalf PROPN\n",
      ", PUNCT\n",
      "Aragorn PROPN\n",
      ", PUNCT\n",
      "Frodo PROPN\n",
      ", PUNCT\n",
      "Bilbo PROPN\n",
      "Baggins PROPN\n",
      ", PUNCT\n",
      "Gollum PROPN\n",
      "... PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in example:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `Matcher` is pretty smart, so we only really need to add to a `'POS'` entry in the pattern dictionary and the tag we are looking for as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new matcher instance\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "for country in countries:\n",
    "    # same as before, but now with one more restriction: the Part-of-speech should be a Pronoun.\n",
    "    pattern = [{'LOWER': c.lower(), 'POS': 'PROPN'} for c in country.split()]    \n",
    "    matcher.add(country, None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "146 4 5 France\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 78 79 Norway\n",
      "263 101 102 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "369 41 42 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id] \n",
    "        span = doc[start:end]\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly the PoS tagger is based on a machine learning method, so it is prone to errors. Notice how it causes _Puerto rico_ of document 64 to be out of this list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting using complex patterns\n",
    "\n",
    "Let's now look into other types of information extraction methods which use complex structures. For example, let's say we want to extract places. Usually, places come up in text in structures similar to:\n",
    "\n",
    "* go to xx\n",
    "* went from xxx\n",
    "* going to xx\n",
    "\n",
    "**Note**: Notice that such patterns could be interesting to the task of relation extraction we mentioned in the intro. But that's something we will leave up to you to look further into.\n",
    "\n",
    "In order to build a SpaCy pattern for the proposed sentence structure, we are going to use the lemma word 'go' (remember lemmatization from BLU07? We can do this in SpaCy pretty easily as well!), which is invariant for all possible verb inflexitions, a preposition (POS tag name - ADP) and a proper noun (POS tag name - PROPN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'LEMMA': 'go'}, {'POS': 'ADP'}, {'POS': 'PROPN'}]\n",
    "matcher.add('LOC', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 27 goes to GTA\n",
      "246 249 going to Osaka\n",
      "81 84 gone to Irvine\n",
      "91 94 going with Robbie\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        span_text = span.text  # the span as a string\n",
    "        print(start, end, span_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sure aren't all the locations that are present in our corpus! Not what we expected then :( \n",
    "\n",
    "Once again, we are finding out that it is very difficult to build patterns to match these type of ocurrences in the text. Addressing all possible patterns for person, location, etc. this way is very inneficient and difficult. \n",
    "\n",
    "Another possible way to go is to annotate examples in a corpus. We can train machine learning systems to automatically extract patterns from annotated corpora. Such class of machine learning methods are known as sequencial labeling and the most famous approaches are [CRFs](https://people.cs.umass.edu/~wallach/technical_reports/wallach04conditional.pdf) and [Seq2seq](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).\n",
    "\n",
    "Fortunately, as explained above, Spacy already contains pre-trained models for standard named-entities. Besides _Person_ (PER) entities like _Bilbo_ and _Organization_ (ORG) entities like _PayPal_ , we can also extract _Location_ entities with the code GPE!\n",
    "\n",
    "Let's try to extract all Locations using the built-in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 New York 0 8\n",
      "30 Portland 5 13\n",
      "30 Portland 402 410\n",
      "58 UK 117 119\n",
      "58 US 146 148\n",
      "64 Puerto rico 81 92\n",
      "117 /battleground 353 366\n",
      "156 VT 3 5\n",
      "202 Tennessee 107 116\n",
      "209 ^you 76 80\n",
      "213 the Puerto Ricans 380 397\n",
      "213 Puerto Rico 467 478\n",
      "213 Puerto Rico 562 573\n",
      "213 US 909 911\n",
      "218 Neyland 11 18\n",
      "226 North Moorhead 16 30\n",
      "226 Fargo 185 190\n",
      "243 Miralo 0 6\n",
      "246 Fünf 12 16\n",
      "255 anti-USA 378 386\n",
      "263 California 363 373\n",
      "267 USA 9 12\n",
      "303 Texas 21 26\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'GPE':   \n",
    "            print(i, e.text, e.start_char, e.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, let's not forget that, as in any machine learning model, we are also prone to errors in our prediction.\n",
    "\n",
    "Could we train a better model? Sure! Given a good corpus for training and the right tools we could achieve a very high accuracy. However, as this is not the objective of this BLU we are going to leave you some links if you want to learn more about this.\n",
    "\n",
    "https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another handy link - https://spacy.io/usage/linguistic-features - you can find here all kind of features SpaCy can extract for you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
