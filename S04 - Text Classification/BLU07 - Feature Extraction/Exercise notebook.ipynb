{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12bea12324c032d8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\caixi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f07b8631beb0508c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q1. Country names\n",
    "\n",
    "For the first question you will be making use of regex. In particular you have a list of countries and you'll have to answer some very specific questions about that list.\n",
    "\n",
    "Start by loading the defining the path to this list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe61fa6cbbdef77d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "path = \"data/countries.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52faf4f77a990570",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The first thing you will build is a wrapper that will apply a regex pattern into a given file, and return a list of results found matching that pattern. Implement it in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Afghanistan\\nAlbania\\nAlgeria\\nAndorra\\nAngola\\nAntigua & Deps\\nArgentina\\nArmenia\\nAustralia\\nAustria\\nAzerbaijan\\nBahamas\\nBahrain\\nBangladesh\\nBarbados\\nBelarus\\nBelgium\\nBelize\\nBenin\\nBhutan\\nBolivia\\nBosnia Herzegovina\\nBotswana\\nBrazil\\nBrunei\\nBulgaria\\nBurkina\\nBurundi\\nCambodia\\nCameroon\\nCanada\\nCape Verde\\nCentral African Rep\\nChad\\nChile\\nChina\\nColombia\\nComoros\\nCongo\\nCongo {Democratic Rep}\\nCosta Rica\\nCroatia\\nCuba\\nCyprus\\nCzech Republic\\nDenmark\\nDjibouti\\nDominica\\nDominican Republic\\nEast Timor\\nEcuador\\nEgypt\\nEl Salvador\\nEquatorial Guinea\\nEritrea\\nEstonia\\nEthiopia\\nFiji\\nFinland\\nFrance\\nGabon\\nGambia\\nGeorgia\\nGermany\\nGhana\\nGreece\\nGrenada\\nGuatemala\\nGuinea\\nGuinea-Bissau\\nGuyana\\nHaiti\\nHonduras\\nHungary\\nIceland\\nIndia\\nIndonesia\\nIran\\nIraq\\nIreland {Republic}\\nIsrael\\nItaly\\nIvory Coast\\nJamaica\\nJapan\\nJordan\\nKazakhstan\\nKenya\\nKiribati\\nKorea North\\nKorea South\\nKosovo\\nKuwait\\nKyrgyzstan\\nLaos\\nLatvia\\nLebanon\\nLesotho\\nLiberia\\nLibya\\nLiechtenstein\\nLithuania\\nLuxembourg\\nMacedonia\\nMadagascar\\nMalawi\\nMalaysia\\nMaldives\\nMali\\nMalta\\nMarshall Islands\\nMauritania\\nMauritius\\nMexico\\nMicronesia\\nMoldova\\nMonaco\\nMongolia\\nMontenegro\\nMorocco\\nMozambique\\nMyanmar, {Burma}\\nNamibia\\nNauru\\nNepal\\nNetherlands\\nNew Zealand\\nNicaragua\\nNiger\\nNigeria\\nNorway\\nOman\\nPakistan\\nPalau\\nPanama\\nPapua New Guinea\\nParaguay\\nPeru\\nPhilippines\\nPoland\\nPortugal\\nQatar\\nRomania\\nRussian Federation\\nRwanda\\nSt Kitts & Nevis\\nSt Lucia\\nSaint Vincent & the Grenadines\\nSamoa\\nSan Marino\\nSao Tome & Principe\\nSaudi Arabia\\nSenegal\\nSerbia\\nSeychelles\\nSierra Leone\\nSingapore\\nSlovakia\\nSlovenia\\nSolomon Islands\\nSomalia\\nSouth Africa\\nSouth Sudan\\nSpain\\nSri Lanka\\nSudan\\nSuriname\\nSwaziland\\nSweden\\nSwitzerland\\nSyria\\nTaiwan\\nTajikistan\\nTanzania\\nThailand\\nTogo\\nTonga\\nTrinidad & Tobago\\nTunisia\\nTurkey\\nTurkmenistan\\nTuvalu\\nUganda\\nUkraine\\nUnited Arab Emirates\\nUnited Kingdom\\nUnited States\\nUruguay\\nUzbekistan\\nVanuatu\\nVatican City\\nVenezuela\\nVietnam\\nYemen\\nZambia\\nZimbabwe'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(path,\"r\")\n",
    "read_file = file.read()\n",
    "read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a88528290dd35f7d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_all_in_file(pattern, path):\n",
    "    \"\"\"\n",
    "    Function that returns all matches of a certain pattern in a certain text.\n",
    "    \n",
    "    Args:\n",
    "    pattern - regex pattern\n",
    "    path - path to the file\n",
    "    \"\"\"\n",
    "    file = open(path,\"r\")\n",
    "    read_file = file.read()\n",
    "    \n",
    "    return re.findall(pattern,read_file,re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef7991b98e752180",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Make sure this function is working with the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a056266aa5ada57",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert find_all_in_file(pattern=\"^P.+?$\", path=path)[8] == \"Portugal\"\n",
    "assert find_all_in_file(pattern=\"^.+?a$\", path=path)[18] == \"Croatia\"\n",
    "assert len(find_all_in_file(pattern=\"^.+?ca$\", path=path)) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df341d1f1838c29e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.a)\n",
    "\n",
    "Now that you prepared your wrapper, let's move on to the actual expressions. The first thing we are looking for is for countries with loooong names. In particular we want you to find all the countries with more than 15 letters. Use the wrapper you defined above and assign its return to a variable `ret`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7cddd8e2e48afb31",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "ret_long = find_all_in_file(pattern=\"^.{15,}$\", path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5a358ed83c473214",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries with more than 15 or more letters:  16\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries with more than 15 or more letters: \", len(ret_long))\n",
    "assert len(ret_long) == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6727f8213243afc0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.b)\n",
    "\n",
    "Now, find out how many countries:\n",
    "* Start with a vowel\n",
    "* Start with a consonant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-447eb675481e47fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "ret_vowel = find_all_in_file(pattern=\"^[AEIOU][a-z].*\", path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-535072609d84d577",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries that start with vowels:  36\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries that start with vowels: \" , len(ret_vowel))\n",
    "assert len(ret_vowel) == 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66af8912995d91a2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "ret_consonant = find_all_in_file(pattern=\"^[^AEIOU][a-z].*\", path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1c17b943b63b52ae",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries that start with consonants:  160\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries that start with consonants: \" , len(ret_consonant))\n",
    "assert len(ret_consonant) == 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e3cc736b933d71e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.c)\n",
    "\n",
    "Next, find how many countries are composed by only one word and end in `ia`. You'll want to have a list with countries such as `Croatia`, `Serbia`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bf14a2df3dd3e4e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "ret_ia = find_all_in_file(pattern=(\"^[a-zA-Z0-9]*[ia]{2}$\"), path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-19b61469674be299",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variants of countries ending in \"ia\":  35\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of variants of countries ending in \\\"ia\\\": \" , len(ret_ia))\n",
    "assert \"Serbia\" in ret_ia\n",
    "assert \"Croatia\" in ret_ia\n",
    "assert len(ret_ia) == 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6bb573c7736db8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.d)\n",
    "\n",
    "Finally, find the countries which have at least four consecutive consonants, without taking into account the first letter (Hint: you can assume the first letter is capitalized). So, it should match things like `Abcdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e92132a5b63400f3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "ret_bcdf = find_all_in_file(pattern='^[A-Za-z].*[bcdfghjklmnpqrstvwxyz]{4,}[A-Za-z].*$|^[A-Za-z].*[bcdfghjklmnpqrstvwxyz]{4,}$'\n",
    "                            , path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5150c254b592778e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries matched:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries matched: \" , len(ret_bcdf))\n",
    "assert len(ret_bcdf) == 3\n",
    "assert hashlib.sha256(' '.join(ret_bcdf).encode()).hexdigest() == '7da1a15074b9245ae3b88fb92fc5c484243003a084d03280bf11d9346d768869'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0409818769bbf3d0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q2. A Study in Scarlet\n",
    "\n",
    "For this following questions we will be looking at Sir Arthur Conan Doyle's [\"A Study in Scarlet\"](https://en.wikipedia.org/wiki/A_Study_in_Scarlet) (which you might have seen adapted to tv in [\"A Study in Pink\"](https://en.wikipedia.org/wiki/A_Study_in_Pink)). We will be performing common preprocessing operations on this text, as it is a common task in Natural Language Processing. Start by downloading the data and loading it into a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abce95c72c255d16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "path = \"data/sherlock.txt\"\n",
    "data =  [line.strip('\\n') for line in open(path, 'r', encoding='utf8') if len(line)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-addc0c904c359402",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.a)\n",
    "\n",
    "First tokenize the data. Implement the function to receive an NLTK-style tokenizer and return the token list for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b7930a61806e32",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_tokenizer(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with the tokens of given text. I.e\n",
    "    for an input ['Abc def', 'Ghi jkl mn'] it returns [['Abc', 'def'], ['Ghi', 'jkl', 'mn']]\n",
    "    \n",
    "    Args:\n",
    "    data - list with the data\n",
    "    tokenizer - nltk tokenizer\n",
    "    \"\"\"\n",
    "    token = tokenizer\n",
    "    vector = []\n",
    "    for sentence in data:\n",
    "        words = token.tokenize(sentence)\n",
    "        vector.append(words)\n",
    "    \n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f829c5a222c54690",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "data_tok = apply_tokenizer(data=data, tokenizer=tokenizer)\n",
    "\n",
    "assert len(data_tok) == 3770\n",
    "assert len([w for s in data_tok for w in s]) == 51648\n",
    "assert data_tok[8] == ['I','could','join','it',',','the','second','Afghan','war','had','broken','out','.','On','landing','at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b76d100b971a777f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.b)\n",
    "\n",
    "The second step you will implement is lowercasing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee47fb5a45fbd622",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_lowercase(data):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with all the tokens lowecased.\n",
    "    \n",
    "    Args:\n",
    "    data - list with tokenized data\n",
    "    \"\"\"\n",
    "    vector = []\n",
    "    for sentence in data:\n",
    "        words = list(map(lambda x: x.lower(), sentence))\n",
    "        vector.append(words)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7979e12840663ea2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_lc = apply_lowercase(data=apply_tokenizer(data=data, tokenizer=tokenizer))\n",
    "\n",
    "assert len(data_lc) == 3770\n",
    "assert len([w for s in data_lc for w in s]) == 51648\n",
    "assert data_lc[8] == ['i','could','join','it',',','the','second','afghan','war','had','broken','out','.','on','landing','at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8044c14c20583cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.c)\n",
    "\n",
    "Now implement a function that filters the stopwords.\n",
    "\n",
    "NOTE: Stopwords adapted from [here](https://gist.github.com/sebleier/554280). (Notice what we added some specific things, like ?\" and .\" to the stopwords. This was shown to be a limitation of the nltk tokenizer so it will be removed that way, instead of the more conventional way. This goes to show that there are more powerful tokenizers that you should use in the case you have to perform tokenization in the future.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42ecb29c8fe117f1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_filter_stopwords(data, stopwords_fp):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with no stopwords.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    stopwords_fp - path to the stopwords file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the list of stopwords from the file\n",
    "    stopwords = [line.strip(\"\\n\") for line in open(stopwords_fp, \"r\")]\n",
    "    \n",
    "    # Filter the stopwords from the text\n",
    "    vector = []\n",
    "    for sentence in data:\n",
    "        words = [n for n in sentence if n not in stopwords]\n",
    "        vector.append(words)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f70da0255ea6e291",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stopwords_fp = \"data/english_stopwords.txt\"\n",
    "data_filt_sw = apply_filter_stopwords(data=apply_lowercase(apply_tokenizer(data, tokenizer)), \n",
    "                                      stopwords_fp=stopwords_fp)\n",
    "assert len(data_filt_sw) == 3770\n",
    "assert len([w for s in data_filt_sw for w in s]) == 27733\n",
    "assert data_filt_sw[8] == ['could', 'join', ',', 'second', 'afghan', 'war', 'broken', '.', 'landing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d0cb317596faa2f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.d)\n",
    "\n",
    "After filtering stopwords, we want to remove punctuation from the text as well. Make use of `string.punctuation` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0cd3cf5cc97a8f2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_filter_punkt(data):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with no punctuation.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    \"\"\"\n",
    "    vector = []\n",
    "    for sentence in data:\n",
    "        words = [n for n in sentence if n not in string.punctuation]\n",
    "        vector.append(words)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-650a677a3a01d1bf",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_filt_punkt = apply_filter_punkt(data=apply_tokenizer(data, tokenizer))\n",
    "\n",
    "assert len(data_filt_punkt) == 3770\n",
    "assert len([w for s in data_filt_punkt for w in s]) == 46362\n",
    "assert data_filt_punkt[8] == ['I','could','join','it','the','second','Afghan','war','had','broken','out','On','landing','at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38d66dd89731e114",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.e)\n",
    "\n",
    "The last preprocessing step you are going to implement is stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a831ba989f3e50e6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_stemmer(data, stemmer):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with stemmed data.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    stemmer - instance of stemmer to use\n",
    "    \"\"\"\n",
    "    stemm = stemmer\n",
    "    vector = []\n",
    "    for sentence in data:\n",
    "        words = list(map(stemm.stem,sentence))\n",
    "        vector.append(words)\n",
    "    \n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3596a6510ebbda3d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "data_stems = apply_stemmer(data=apply_lowercase(apply_tokenizer(data, tokenizer)),\n",
    "                           stemmer=stemmer)\n",
    "\n",
    "assert len(data_stems) == 3770\n",
    "assert len([w for s in data_stems for w in s]) == 51648\n",
    "assert data_stems[8][-2] == 'land'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bfe5aed6ebdbb26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.f)\n",
    "\n",
    "Finally, join everything in a function, that applies the steps in the following order, in :\n",
    "* Tokenization\n",
    "* Lowercasing\n",
    "* Filtering stopwords\n",
    "* Filtering punctuation\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea5b2305431c20dd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, lower=True, remove_punct=True, stopwords=[],regex_list=[]):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def clean_sentences(self, sentences):\n",
    "                \n",
    "        # Split sentence into list of words\n",
    "        sentences_tokens = apply_tokenizer(data=sentences,tokenizer=self.tokenizer)\n",
    "        \n",
    "        # Lowercase\n",
    "        if self.lower:\n",
    "            sentences_tokens = apply_lowercase(sentences_tokens)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if self.stopwords:\n",
    "            sentences_tokens = apply_filter_stopwords(data=sentences_tokens,stopwords_fp=self.stopwords)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            sentences_tokens = apply_filter_punkt(sentences_tokens)\n",
    "    \n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            sentences_tokens = apply_stemmer(data=sentences_tokens,stemmer=self.stemmer)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentences_prep = [\" \".join(tokens).strip() for tokens in sentences_tokens]\n",
    "        return sentences_prep\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4a87d0c9b1f20f7e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text_cleaner = TextCleanerTransformer(\n",
    "    regex_list=[],\n",
    "    tokenizer=tokenizer, \n",
    "    stemmer=stemmer,\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords_fp\n",
    ")\n",
    "\n",
    "data_preprocessed = text_cleaner.clean_sentences(data)\n",
    "assert len(data_preprocessed) == 3770\n",
    "assert len([w for s in data_preprocessed for w in s.split()]) == 22447\n",
    "assert data_preprocessed[8] == 'could join second afghan war broken land'\n",
    "assert data_preprocessed[15] == 'noth misfortun disast remov brigad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f48abbdab8acc3d1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q3. Movie reviews\n",
    "\n",
    "We will now use what we've learned to explore movie reviews. We will start by analysing the dataset, then we will apply the preprocessing you implemented above, and finally we will see how it affects a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d6a39b05ecd310c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.a)\n",
    "\n",
    "To get some stats on the dataset, we will start by implementing your own function to get the list of n-grams from a list of tokens. Complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43601c5f562f5867",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ngrams(data, n):\n",
    "    \"\"\"\n",
    "    Returns list of tuples for all the n-grams\n",
    "    \n",
    "    Args:\n",
    "    data - list of tokenized data (flattened)\n",
    "    n - the n in n-grams\n",
    "    \"\"\"\n",
    "    from nltk import ngrams\n",
    "\n",
    "    n_grams = list(ngrams(data, n))\n",
    "        \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f601c9c7d4b35092",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert ngrams(\"The actress won the oscar\".split(), 2) == [('The', 'actress'), ('actress', 'won'), ('won', 'the'), ('the', 'oscar')]\n",
    "assert ngrams(\"The actress won the oscar\".split(), 3) == [('The', 'actress', 'won'), ('actress', 'won', 'the'), ('won', 'the', 'oscar')]\n",
    "assert ngrams(\"The actress won the oscar\".split(), 4) == [('The', 'actress', 'won', 'the'), ('actress', 'won', 'the', 'oscar')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8cdc8657fcd5f3c4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.b)\n",
    "\n",
    "We will now see in our dataset what are the most common n-grams. Load the data and find how many unique bi-grams, tri-grams and four-grams we have. Also, take advantage of `Counter` and `most_common()` to find the most common tri-gram. Merge together the words of the most common trigram to get one single string. (Hint: look at python's `join` function, exemplefied below when joining the full text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21e130445a725501",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/imdb_sentiment.csv')\n",
    "\n",
    "# Get the text and split into full list of words\n",
    "docs = df['text']\n",
    "full_text = ' '.join([d.strip() for d in docs])\n",
    "words = full_text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-caa9453210806809",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement below the code to get the sets of unigrams, bigrams, trigrams and fourgrams, and to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-539dba0b0aabb366",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "unigrams = Counter(ngrams(words,1))\n",
    "bigrams = Counter(ngrams(words,2))\n",
    "trigrams = Counter(ngrams(words,3))\n",
    "fourgrams = Counter(ngrams(words,4))\n",
    "most_common_trigram = ' '.join([' '.join(n[0]).strip() for n in trigrams.most_common(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-91094315aa4e5abf",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102712 unigrams\n",
      "Found 544165 bigrams\n",
      "Found 962749 trigrams\n",
      "Found 1132643 fourgrams\n",
      "Most common trigram is \"one of the\"\n"
     ]
    }
   ],
   "source": [
    "n_unigrams = str(len(unigrams))\n",
    "n_bigrams = str(len(bigrams))\n",
    "n_trigrams = str(len(trigrams))\n",
    "n_fourgrams = str(len(fourgrams))\n",
    "\n",
    "print('Found {} unigrams'.format(n_unigrams))\n",
    "assert hashlib.sha256(n_unigrams.encode()).hexdigest() == '1ae2d8247d3ad491c79aed034828ba78b21e25438a6e9a61f252eb566e39e877'\n",
    "\n",
    "print('Found {} bigrams'.format(n_bigrams))\n",
    "assert hashlib.sha256(n_bigrams.encode()).hexdigest() == '7d2d487bcdf890f05578da49f574e3e8f22f7420f752071a24eb49759de5adf8'\n",
    "\n",
    "print('Found {} trigrams'.format(n_trigrams))\n",
    "assert hashlib.sha256(n_trigrams.encode()).hexdigest() == '8c54e3c7087ab053a77d56c60408fd47837081fdea817b7cc9e68f134cef969d'\n",
    "\n",
    "print('Found {} fourgrams'.format(n_fourgrams))\n",
    "assert hashlib.sha256(n_fourgrams.encode()).hexdigest() == '8df23d7f0d27298e7a7f77bdce4d15bb401098175c36514ba94b5350177b1593'\n",
    "\n",
    "print('Most common trigram is \"{}\"'.format(most_common_trigram))\n",
    "assert hashlib.sha256(most_common_trigram.encode()).hexdigest() == '28b6f04107ef3f1120975abf58ca8d08d20243beea929999b203f0add941fe16'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d2477efb5c417c3d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.c)\n",
    "\n",
    "Let's now process a sample of our dataset with the previous Q2 preprocessing, and get a Bag of Words representation. Start by using your text cleaner to get a preprocessed version of this dataset.\n",
    "\n",
    "Note: if you didn't finish the text cleaner above, jump to the TF-IDF implementation directly, where you can load the BoW from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-306c1207ab2a8c1d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text_cleaner = TextCleanerTransformer(\n",
    "    regex_list=[],\n",
    "    tokenizer=tokenizer, \n",
    "    stemmer=None,\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords_fp\n",
    ")\n",
    "\n",
    "docs_preprocessed = text_cleaner.clean_sentences(docs[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-11aa60fa130d8eeb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can get a vocabulary, vectorize our dataset and convert it into a BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-23a2f9da897ab002",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>br</th>\n",
       "      <th>/&gt;&lt;</th>\n",
       "      <th>/&gt;</th>\n",
       "      <th>movie</th>\n",
       "      <th>film</th>\n",
       "      <th>.&lt;</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>endearing</th>\n",
       "      <th>cortney</th>\n",
       "      <th>fatal</th>\n",
       "      <th>incidents</th>\n",
       "      <th>erupts</th>\n",
       "      <th>semblance</th>\n",
       "      <th>thirds</th>\n",
       "      <th>miserable</th>\n",
       "      <th>shoes</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7441 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   br  /><  />  movie  film  .<  one  like  time  even  ...  endearing  \\\n",
       "0   0    0   0      3     5   0    1     0     2     0  ...          0   \n",
       "1   0    0   0      1     0   0    0     0     0     0  ...          0   \n",
       "2   5    2   4      6     0   2    2     1     0     1  ...          0   \n",
       "3   0    0   0      4     0   0    1     1     1     1  ...          0   \n",
       "4   8    3   6      1     0   2    2     0     1     0  ...          0   \n",
       "\n",
       "   cortney  fatal  incidents  erupts  semblance  thirds  miserable  shoes  \\\n",
       "0        0      0          0       0          0       0          0      0   \n",
       "1        0      0          0       0          0       0          0      0   \n",
       "2        0      0          0       0          0       0          0      0   \n",
       "3        0      0          0       0          0       0          0      0   \n",
       "4        0      0          0       0          0       0          0      0   \n",
       "\n",
       "   mail  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 7441 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocabulary(docs):\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "def vectorize(docs):\n",
    "    vocabulary = build_vocabulary(docs)\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vector = np.array([doc.count(word) for word in vocabulary])\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return (vocabulary, vectors)\n",
    "\n",
    "def build_df(docs):\n",
    "    vocab, vectors = vectorize(docs)\n",
    "    return pd.DataFrame(vectors, columns=vocab)\n",
    "\n",
    "BoW = build_df(docs_preprocessed)\n",
    "BoW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c9951e7e69d2e1c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You will now implement one of TF-IDFs variation to compute from the bag of words the more relevant words. The formulation you should use is one you've learned before:\n",
    "\n",
    "$$ tfidf _{t, d} =(tf_{t,d})*(log_2{(1 + \\frac{N}{df_{t}})})  $$\n",
    "\n",
    "Implement the TF-IDF below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb6996cb41762895",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tfidf(BoW_df):\n",
    "    \"\"\"\n",
    "    Returns pandas dataframe of a tfidf representation from a BoW representation dataframe.\n",
    "\n",
    "    Args:\n",
    "    BoW_df - dataframe with document word counts (Bag of Words)\n",
    "    \"\"\"\n",
    "\n",
    "    tf = BoW.div(BoW.sum(axis=1), axis=0)\n",
    "    # What this function does is to divide each word by the total number of words in a document\n",
    "    \n",
    "    def _idf(column):\n",
    "        # In here we are finding the inverse of the term frequency  to give a higher\n",
    "        # ponderation to the words that are more rare\n",
    "        # In addition we normalize the data\n",
    "        return np.log2(1 + len(column) / sum(column > 0))\n",
    "\n",
    "    # This represents the multplication of the two matrices \n",
    "    # In here we just need to normalize the parameter tf\n",
    "    tf_idf = (np.log2(1 + tf)).multiply(tf.apply(_idf))\n",
    "    \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ab99eef3c657a91",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's now apply it to our previous BoW (note: load the BoW first if you could not use your text cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffdac98448888edf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>br</th>\n",
       "      <th>/&gt;&lt;</th>\n",
       "      <th>/&gt;</th>\n",
       "      <th>movie</th>\n",
       "      <th>film</th>\n",
       "      <th>.&lt;</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>endearing</th>\n",
       "      <th>cortney</th>\n",
       "      <th>fatal</th>\n",
       "      <th>incidents</th>\n",
       "      <th>erupts</th>\n",
       "      <th>semblance</th>\n",
       "      <th>thirds</th>\n",
       "      <th>miserable</th>\n",
       "      <th>shoes</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7441 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   br  /><  />  movie  film  .<  one  like  time  even  ...  endearing  \\\n",
       "0   0    0   0      3     5   0    1     0     2     0  ...          0   \n",
       "1   0    0   0      1     0   0    0     0     0     0  ...          0   \n",
       "2   5    2   4      6     0   2    2     1     0     1  ...          0   \n",
       "3   0    0   0      4     0   0    1     1     1     1  ...          0   \n",
       "4   8    3   6      1     0   2    2     0     1     0  ...          0   \n",
       "\n",
       "   cortney  fatal  incidents  erupts  semblance  thirds  miserable  shoes  \\\n",
       "0        0      0          0       0          0       0          0      0   \n",
       "1        0      0          0       0          0       0          0      0   \n",
       "2        0      0          0       0          0       0          0      0   \n",
       "3        0      0          0       0          0       0          0      0   \n",
       "4        0      0          0       0          0       0          0      0   \n",
       "\n",
       "   mail  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 7441 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW = pd.read_csv('data/imdb_sentiment_bow_sample.csv')\n",
    "BoW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5b8f988a69678772",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "relevance = tfidf(BoW)\n",
    "\n",
    "assert(math.isclose(relevance['movie'][0], 0.009717385023827248),\n",
    "       math.isclose(relevance['film'][10], 0.019778475747522496),\n",
    "       math.isclose(relevance['nice'][16], 0.010851136310680626),\n",
    "       math.isclose(relevance['good'][128], 0.00989061193998239))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-779badbcfbbc934e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.d)\n",
    "\n",
    "Now, let's use scikit-learn to get to a similar matrix and relevance numbers. Load the full processed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40fe043ee73e9a82",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_preprocessed = pd.read_csv('data/imdb_sentiment_processed.csv')\n",
    "\n",
    "# Get the processed text \n",
    "docs = df_preprocessed['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61ec1d1af055ee98",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Start by transforming your documents into a matrix of tf-idf scores using sklearn. Make use of the `CountVectorizer` and the `TfidfTransformer` provided by scikitlearn. Implement a function that provided with a list of documents returns the word term frequency matrix and the corresponding vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8300a0a7bebf4efd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_word_term_frequency_matrix(docs):\n",
    "    \"\"\"\n",
    "    Returns the matrix of word and tf-idf scores \n",
    "    \n",
    "    Args:\n",
    "    docs - list of documents in dataset\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(docs)\n",
    "    word_count_matrix = vectorizer.transform(df['text'].values)\n",
    "    vocabulary =  vectorizer.vocabulary_\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf.fit(word_count_matrix)\n",
    "    word_term_frequency_matrix = tfidf.transform(word_count_matrix)\n",
    "\n",
    "    return (word_term_frequency_matrix, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b548aa7912824b9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now get the corresponding string of the most important word of this document (with index `321`) according to TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d00746278a646fe0",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "index = 321\n",
    "\n",
    "word_term_frequency_matrix, vocabulary = build_word_term_frequency_matrix(docs)\n",
    "\n",
    "max_word_idx = word_term_frequency_matrix[index].argmax()\n",
    "inv_vocab = {v: k for k, v in vocabulary.items()}\n",
    "most_relevant_word = inv_vocab[max_word_idx]\n",
    "\n",
    "assert(most_relevant_word == 'dull')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab54cc6297c36ee2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.e)\n",
    "\n",
    "Finally, let's try to classify the sentiment of these movie reviews. \n",
    "\n",
    "Build a Pipeline to classify a review as positive or negative. Use `MultinomialNB` as your final classifier, train it and get an accuracy score above 86% on the imdb validation dataset, by choosing the best set of parameters of `TextCleanerTransformer()`, `CountVectorizer()` and `TfidfTransformer()`, according to what we learned in Part III.\n",
    "\n",
    "Hint: Try to use more than unigrams! Also, remember what we said about stopwords and feature space size in Part III of the Learning Notebooks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca9cc0a788f3d721",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Split in train and validation\n",
    "train_df, validation_df = train_test_split(df_preprocessed, test_size=0.3, random_state=42)\n",
    "\n",
    "# Encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['sentiment'].values)\n",
    "\n",
    "train_df['sentiment'] = le.transform(train_df['sentiment'].values)\n",
    "validation_df['sentiment'] = le.transform(validation_df['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0712ceb628e6ed6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_and_validate(train_df, validation_df):\n",
    "    \"\"\"\n",
    "    Train a model using sklearn's Pipeline and return it along with its \n",
    "    current accuracy in the validation set. Assume the documents are already \n",
    "    preprocessed\n",
    "    \n",
    "    Args:\n",
    "    train_df - dataframe with training docs\n",
    "    validation_df - dataframe with validation docs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the pipeline\n",
    "    text_clf = Pipeline([('vect', CountVectorizer('english',ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "    \n",
    "#     Train the classifier\n",
    "    text_clf.fit(train_df['text'], train_df['sentiment'])\n",
    "\n",
    "    predicted = (text_clf.predict(map(str, validation_df['text'].values)))\n",
    "\n",
    "    acc = (np.mean(predicted == validation_df['sentiment']))\n",
    "    \n",
    "    return text_clf, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-07b6f3694941ac81",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.862\n"
     ]
    }
   ],
   "source": [
    "_, acc = train_and_validate(train_df, validation_df)\n",
    "print(\"Accuracy: {}\".format(acc))\n",
    "assert(acc >= 0.86)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
