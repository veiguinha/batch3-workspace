{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-92910fa39f92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import math\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/spam_cleaned.tsv', sep='\\t')\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['email'] = df['email'].map(simple_tokenizer)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['email'], df['label'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9bab744c327dc14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1.\n",
    "\n",
    "The year is 1998 and your hotmail account is starting to fill up with garbage! Luckily you know some NLP and have a handy dataset to train a spam/not spam classifier with. (But beware, the dataset is very imbalanced.) To start, you'll train a baseline classifier using features from TFIDF.\n",
    "\n",
    "\n",
    "### Q1.a)\n",
    "\n",
    "Don't forget your computer is also from 1998 and your computing power is pretty bad, so we'll have to limit this classifier to only using 50 features.\n",
    "Implement a function that returns the fitted vectorizer, and precision and recall, respectively, on the spam class from a K Nearest Neighbors classifier using default TFIDF features (except max_features=50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5802f58ce8de2c27",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_of_spam_tfidf(X_train, y_train, X_test, y_test):\n",
    "    '''Returns a fitted TfidfVectorizer and the test precision and recall on the 'spam' class\n",
    "       from a KNeighborsClassifier trained on the inputted train data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (TfidfVectorizer): TfidfVectorizer with max_features == 50, fitted to X_train\n",
    "        precision (float): The precision score of the spam class on the test data from a\n",
    "                           KNeighborsClassifier fitted to the vectorized training data\n",
    "        recall (float): The recall score of the spam class on the test data from a\n",
    "                        KNeighborsClassifier fitted to the vectorized training data\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    vectorizer = TfidfVectorizer(max_features=50)\n",
    "    vectorizer.fit(X_train)\n",
    "    \n",
    "    X_train_vec = vectorizer.transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X_train_vec,y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    \n",
    "    precision = precision_score(y_test,y_pred,pos_label='spam')\n",
    "    recall = recall_score(y_test,y_pred,pos_label='spam')\n",
    "    \n",
    "    return vectorizer, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-32e077c8cb1aea12",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, precision, recall = get_precision_recall_of_spam_tfidf(X_train, y_train, X_test, y_test)\n",
    "\n",
    "assert math.isclose(precision, 0.8395061728395061)\n",
    "assert math.isclose(recall, 0.6210045662100456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4dd063a2fd408be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.b)\n",
    "\n",
    "Let's see what we can do to get both precision and recall up. We don't want to be bombarded with spam but we also don't want to miss too many real emails.\n",
    "\n",
    "First, look at the selected features of the fitted tfidf vectorizer, sorted by their inverse document frequency (in descending order). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4c04022dcbf77d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_ngrams_sorted_by_idf(fitted_vectorizer):\n",
    "    '''Returns the features of a fitted TfidfVectorizer ordered by their idf score\n",
    "    \n",
    "    Parameters:\n",
    "        fitted_vectorizer (TfidfVectorizer): A fitted TfidfVectorizer\n",
    "    \n",
    "    Returns:\n",
    "        ngrams_sorted (list): The features of fitted_vectorizer sorted in ascending order\n",
    "                              by their idf score\n",
    "    '''\n",
    "    df = pd.DataFrame(vectorizer.idf_, index=vectorizer.get_feature_names(),columns=[\"idf_weights\"])\n",
    "    ngrams_sorted = df.sort_values(by=['idf_weights'],ascending=False)['idf_weights'].index.tolist()\n",
    "    \n",
    "    return ngrams_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-df8fce77a9a2c576",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(hashlib.sha256(get_ngrams_sorted_by_idf(vectorizer)[5].encode()).hexdigest() == \"5ef5ef0364b6939c4ca61f34b393f7b368d1be8619647aaf83d5b395919ab629\")\n",
    "assert(hashlib.sha256(get_ngrams_sorted_by_idf(vectorizer)[48].encode()).hexdigest() == \"bb0347a468d97e98a9c00e37cebec1ab930f6f1221cae0f1fbb92b07e1900ba2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show most specifc 30 features of the 50 selected features\n",
    "get_ngrams_sorted_by_idf(vectorizer)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.c)\n",
    "\n",
    "Let's also check the number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52cf310a681078ae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_number_of_features(fitted_vectorizer):\n",
    "    '''Returns the number of features of a fitted TfidfVectorizer\n",
    "    \n",
    "    Parameters:\n",
    "        fitted_vectorizer (TfidfVectorizer): A fitted TfidfVectorizer\n",
    "    \n",
    "    Returns:\n",
    "        vocab_size (int): The number of features of fitted_vectorizer \n",
    "    '''\n",
    "    vocab_size = len(vectorizer.get_feature_names())\n",
    "    return vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6cf4827c6a138b1b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_features = get_number_of_features(vectorizer)\n",
    "\n",
    "assert num_features == 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-31f534c93e127893",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.d)\n",
    "\n",
    "Hmm, looks like we might need to do some feature selection!\n",
    "\n",
    "To try something simple, reimplement the KNN function from 1a, but avoiding English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-124715539ede6b28",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_of_spam_tfidf_no_stopwords(X_train, y_train, X_test, y_test):\n",
    "    '''Returns a fitted TfidfVectorizer with English stopwords removed and the test precision\n",
    "       and recall on the 'spam' class from a KNeighborsClassifier trained on the inputted train data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (TfidfVectorizer): TfidfVectorizer with max_features=50 and stopwords='english',\n",
    "                                      fitted to X_train\n",
    "        precision (float): The precision score of the spam class on the test data from a\n",
    "                           KNeighborsClassifier fitted to the vectorized training data\n",
    "        recall (float): The recall score of the spam class on the test data from a\n",
    "                        KNeighborsClassifier fitted to the vectorized training data\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    vectorizer = TfidfVectorizer(max_features=50,stop_words='english')\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train_vec = vectorizer.transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X_train_vec,y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    \n",
    "    precision = precision_score(y_test,y_pred,pos_label='spam')\n",
    "    recall = recall_score(y_test,y_pred,pos_label='spam')\n",
    "    \n",
    "    return vectorizer, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-89cb64b5c51affeb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer_stopwords, precision_stopwords, recall_stopwords = \\\n",
    "                    get_precision_recall_of_spam_tfidf_no_stopwords(X_train, y_train, X_test, y_test)\n",
    "\n",
    "assert math.isclose(precision_stopwords, 0.8636363636363636)\n",
    "assert math.isclose(recall_stopwords, 0.6073059360730594)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b41a6d2d1f3ff2da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q2.\n",
    "\n",
    "Not bad, not great, after some simple feature selection we have better precision but slightly worse recall. Still you are confident you can push it a bit further.\n",
    "\n",
    "Repeat Q1, but this time, in addition to stopwords, use the chi-squared method to get the relevant 50 features, and output the fitted vectorizer, fitted SelectKBest, precision, and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35e5bcb64966cd8f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_of_spam_chi_squared(X_train, y_train, X_test, y_test):\n",
    "    '''Returns a fitted TfidfVectorizer, a fitted SelectKBest and the test precision and recall on\n",
    "       the 'spam' class from a KNeighborsClassifier trained on the inputted train data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (TfidfVectorizer): TfidfVectorizer with  stopwords='english', fitted to X_train\n",
    "        ch2 (SelectKBest): SelectKBest with score function chi2 and k=50, fitted to vectorized X_train\n",
    "        precision (float): The precision score of the spam class on the test data from a\n",
    "                           KNeighborsClassifier fitted to the feature-selected training data\n",
    "        recall (float): The recall score of the spam class on the test data from a KNeighborsClassifier\n",
    "                        fitted to the feature-selected training data\n",
    "    '''\n",
    "    vectorizer_ch2 = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    vectorizer_ch2.fit(X_train)\n",
    "    X_train_vec = vectorizer_ch2.transform(X_train)\n",
    "    X_test_vec = vectorizer_ch2.transform(X_test)\n",
    "    \n",
    "    \n",
    "#     chi_values, p_values = chi2(X_train_vec, y_train)\n",
    "    ch2_train = SelectKBest(chi2, k=50)\n",
    "    \n",
    "    ch2 = ch2_train.fit(X_train_vec, y_train)\n",
    "    \n",
    "#     feature_names = vectorizer_ch2.get_feature_names()\n",
    "#     most_important_features = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "    \n",
    "    X_train_chi = ch2_train.transform(X_train_vec)\n",
    "    X_test_chi = ch2_train.transform(X_test_vec)\n",
    "    \n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X_train_chi,y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_chi)\n",
    "    \n",
    "    precision_ch2 = precision_score(y_test,y_pred,pos_label='spam')\n",
    "    recall_ch2 = recall_score(y_test,y_pred,pos_label='spam')\n",
    "    \n",
    "    return vectorizer_ch2, ch2, precision_ch2, recall_ch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9ec326f8b2022b2d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer_ch2, ch2, precision_ch2, recall_ch2 = \\\n",
    "                    get_precision_recall_of_spam_chi_squared(X_train, y_train, X_test, y_test)\n",
    "\n",
    "assert math.isclose(precision_ch2, 0.9101123595505618)\n",
    "assert math.isclose(recall_ch2, 0.7397260273972602)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we'll check the most important features\n",
    "most_important_features = [vectorizer_ch2.get_feature_names()[i] for i in ch2.get_support(indices=True)]\n",
    "print(most_important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14b0595927829765",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q3.\n",
    "\n",
    "Chi-squared feature selection helped get both metrics up a little! \n",
    "\n",
    "You now feel confident enough to verify if these features are in fact meaningful for classifying your data, so you decide to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_punct = lambda s: re.sub(r'\\s+\\W*[a-z]?\\W*\\s+', ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in most_important_features:\n",
    "    print('Documents that contains the word(s) \"%s\"' % feature)\n",
    "    print('----')\n",
    "    docs = X_train.apply(sub_punct).str.lower().str.contains(feature)\n",
    "    print(str(y_train[docs].value_counts()) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9173a0b6dfe2e4d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q4.\n",
    "\n",
    "You now want dimensionality reduction techniques like SVD and PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for random\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-20da2f33c118bba8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.a)\n",
    "\n",
    "Transform the training data with tfidf avoiding stopwords, and calculate the total variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee2c9e7a3e39a7b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectorizer.fit(X_train)\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "# The total variance was calculated only on the X_train after the transformation of the vectorizer\n",
    "total_variance = np.var((X_train_vec).toarray(),axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-75e25a3bbae50fd6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(total_variance, 0.9897797614643888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a2188266694d43ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.b)\n",
    "\n",
    "Write a function that fits sklearn's TruncatedSVD with 50 components to the tfidf data, trains a KNN classifier, and outputs the total explained variance, precision, and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43a51c434987d044",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_of_svd(X_train_vec, y_train, X_test_vec, y_test):\n",
    "    '''Returns the total explained variance of SVD fitted to the Tfidf-vectorized input data,\n",
    "       and the test precision and recall on the 'spam' class from a KNeighborsClassifier\n",
    "       trained on the dimensionality-reduced train data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train_vec (Series): Text data for training vectorized by Tfidf\n",
    "        y_train (Series): Labels corresponding to X_train_vec\n",
    "        X_test_vec (Series): Text data for testing vectorized by Tfidf\n",
    "        y_test (Series): Labels corresponding to X_test_vec\n",
    "\n",
    "    Returns:\n",
    "        explained variance (float): The total explained variance of a fitted TruncatedSVD\n",
    "                                    with n_components=50 and random_state=seed\n",
    "        precision (float): The precision score of the spam class on the test data from a\n",
    "                           KNeighborsClassifier fitted to the SVD'd training data\n",
    "        recall (float): The recall score of the spam class on the test data from a\n",
    "                        KNeighborsClassifier fitted to the SVD'd training data\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    svd = TruncatedSVD(n_components=50,random_state=seed)\n",
    "    svd.fit(X_train_vec)\n",
    "    \n",
    "    X_train_trunc = svd.transform(X_train_vec)\n",
    "    X_test_trunc = svd.transform(X_test_vec)\n",
    "    \n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X_train_trunc,y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_trunc)\n",
    "    \n",
    "    explained_variance = np.var(X_train_trunc,axis=0).sum()\n",
    "    \n",
    "    precision = precision_score(y_test,y_pred,pos_label='spam')\n",
    "    recall = recall_score(y_test,y_pred,pos_label='spam')    \n",
    "    \n",
    "    return explained_variance, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1776b66bd0ef9237",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "var, precision_svd, recall_svd = \\\n",
    "                    get_precision_recall_of_svd(X_train_vec, y_train, X_test_vec, y_test)\n",
    "\n",
    "assert math.isclose(var, 0.16416346460759212)\n",
    "assert math.isclose(precision_svd, 0.9109947643979057)\n",
    "assert math.isclose(recall_svd, 0.7945205479452054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4388cbd2be8ec8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q5\n",
    "\n",
    "SVD got us a few points higher in recall than chi-squared, nice! Remember now some of the pros and cons of SVD and other dimensionality reduction techniques. While SVD performed better, likely because in theory it can capture information from more than 50 ngrams (whereas in chi-squared the number of features was in a direct correspondence with the number of ngrams), with SVD we lose interpretability and can't examine the most important features!\n",
    "\n",
    "Now let's try PCA.\n",
    "\n",
    "Write a function that fits sklearn's PCA with 50 components to the tfidf data from Q4, trains a KNN classifier, and outputs the total explained variance, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c323629aaea537dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_of_pca(X_train_vec, y_train, X_test_vec, y_test):\n",
    "    '''Returns the total explained variance of PCA fitted to the Tfidf-vectorized input data,\n",
    "       and the test precision and recall on the 'spam' class from a KNeighborsClassifier\n",
    "       trained on the dimensionality-reduced train data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train_vec (Series): Text data for training vectorized by Tfidf\n",
    "        y_train (Series): Labels corresponding to X_train_vec\n",
    "        X_test_vec (Series): Text data for testing vectorized by Tfidf\n",
    "        y_test (Series): Labels corresponding to X_test_vec\n",
    "\n",
    "    Returns:\n",
    "        explained variance (float): The total explained variance of a fitted PCA\n",
    "                                    with n_components=50 and random_state=seed\n",
    "        precision (float): The precision score of the spam class on the test data from a\n",
    "                           KNeighborsClassifier fitted to the PCA'd training data\n",
    "        recall (float): The recall score of the spam class on the test data from a\n",
    "                        KNeighborsClassifier fitted to the PCA'd training data\n",
    "    '''\n",
    "    dense_X_train = X_train_vec.toarray()\n",
    "    dense_X_test = X_test_vec.toarray()\n",
    "    \n",
    "    pca = PCA(n_components=50,random_state=seed)\n",
    "    pca.fit(dense_X_train)\n",
    "    \n",
    "    X_train_pca = pca.transform(dense_X_train)\n",
    "    X_test_pca = pca.transform(dense_X_test)\n",
    "    \n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X_train_pca,y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    \n",
    "#     explained_variance = 1.0*np.var(X_train_pca,axis=0).sum()\n",
    "    \n",
    "    explained_variance = pca.explained_variance_.sum()\n",
    "    \n",
    "    precision = precision_score(y_test,y_pred,pos_label='spam')\n",
    "    recall = recall_score(y_test,y_pred,pos_label='spam')\n",
    "    \n",
    "    return explained_variance, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f0560b07cb8e8132",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "var_pca, precision_pca, recall_pca = \\\n",
    "                    get_precision_recall_of_pca(X_train_vec, y_train, X_test_vec, y_test)\n",
    "\n",
    "assert math.isclose(var_pca, 0.1658261296302505)\n",
    "assert math.isclose(precision_pca, 0.9067357512953368)\n",
    "assert math.isclose(recall_pca, 0.7990867579908676)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b17e715e9161796",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q6.\n",
    "\n",
    "Now we'll change gears a bit and look into word vectors. In Learning Notebook 3 we told you that word vectors can be visualized after being projected into 2D space, and we showed you this diagram:\n",
    "\n",
    "<img src=\"./media/word-vectors-projection.png\" width=\"600\">\n",
    "\n",
    "Now we'll try to combine what you've learned about word embeddings and PCA to make our own visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.a)\n",
    "\n",
    "First, to get comfortable with spacy, get the vector for the word \"tree\" and return the sum of its elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34ae448736fdde6b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sum_tree = nlp('tree').vector.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5cdd4b59c690522a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(sum_tree, 2.3972085, abs_tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.b) \n",
    "\n",
    "Next, write a function that does a fit_transform of sklearn's PCA to a given set of word vectors, using the correct number of components. The function should return the reduced dimension word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c4ffc6d44920880",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def reduce_word_vecs(vectors):\n",
    "    '''Returns PCA-reduced word vectors of the input vectors\n",
    "    \n",
    "    Parameters:\n",
    "        vectors (np.array): Word vectors to be reduced\n",
    "\n",
    "    Returns:\n",
    "        reduced_vecs (np.array): Word vectors reduced to the number of dimensions\n",
    "                                 suitable for plotting with random_state=seed\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # The vectors that are introduced are supposed to be reduced using PCA\n",
    "    pca = PCA(n_components=2,random_state=seed)\n",
    "    pca.fit(vectors)\n",
    "    \n",
    "    reduced_vecs = pca.transform(vectors)\n",
    "    \n",
    "    return reduced_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-05bdaa52733db9e2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_vectors = np.array([[0.1, 0.2, 0.3, 0.4], [0.3, 0.5, 0.1, 0.7], [0.8, 0.6, 0.2, 0.4]])\n",
    "reduced_vecs = reduce_word_vecs(test_vectors)\n",
    "\n",
    "assert reduced_vecs.shape == (3,2)\n",
    "assert math.isclose(reduced_vecs[1][1], 0.24736592153367926)\n",
    "assert math.isclose(reduced_vecs[2][0], 0.43388622222454437)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create an array of ~100,000 of spacy's word vectors and use your PCA function to reduce them. If you're curious about using the full amount of word vectors, just remove the `if` statement in the below function, but beware it will use a lot of memory!\n",
    "\n",
    "We'll also set a list of words that we'll plot later and force our set of vectors to include these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_plot = ['banana', 'pineapple', 'mango', 'red', 'blue', 'yellow', 'woman', 'man', 'child', 'playing',\n",
    "                 'reading', 'studying', 'nintendo', 'sony', 'xbox', 'sad', 'angry', 'bored']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution: if you run this cell more than once, you may get different words than the ones\n",
    "# expected for the rest of the exercises. If that happens, restart the kernel and try again\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "full_vocab_vecs = []\n",
    "vocab_strings = []\n",
    "for tok in list(nlp.vocab.strings):\n",
    "    if tok in words_to_plot or random.random() < 0.07:\n",
    "        full_vocab_vecs.append(nlp.vocab.get_vector(tok))\n",
    "        vocab_strings.append(tok)\n",
    "\n",
    "vocab_array = np.array(full_vocab_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Word vectors shape pre-PCA: {}'.format(vocab_array.shape))\n",
    "\n",
    "full_vocab_reduced = reduce_word_vecs(vocab_array)\n",
    "\n",
    "print('Word vectors shape after PCA: {}'.format(full_vocab_reduced.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-93396749fc10fb39",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(full_vocab_reduced[100][0], -3.0945618, abs_tol=0.00001)\n",
    "assert math.isclose(full_vocab_reduced[9999][0], -0.94902855, abs_tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0feaee69a29da92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.c)\n",
    "\n",
    "Time to plot! For this, we'll limit the visualized words to a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "for word in words_to_plot:\n",
    "    idx = vocab_strings.index(word)\n",
    "    coords.append(full_vocab_reduced[idx])\n",
    "\n",
    "coords_array = np.array(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.xlim(min([x for x in coords_array[:,0]])-.25, max([x for x in coords_array[:,0]]))\n",
    "plt.ylim(min([y for y in coords_array[:,1]]), max([y for y in coords_array[:,1]])+0.25)\n",
    "plt.scatter(coords_array[:,0], coords_array[:,1])\n",
    "\n",
    "for item, x, y in zip(words_to_plot, coords_array[:,0], coords_array[:,1]):\n",
    "    plt.annotate(item, xy=(x, y), xytext=(-2, 2), textcoords='offset points', \n",
    "                 ha='right', va='bottom', color='purple', fontsize=14 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dcc429eed65a7f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The PCA seems to have worked! In the diagram we can see similar types of words closer together. But of course, take these visualizations with a grain of salt because it is practically impossible to preserve all distances in a high dimensional space in just 2 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a980c450673e4fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.d) \n",
    "\n",
    "As a final exercise, we'll look at some word similarities.\n",
    "\n",
    "Write a function that returns the next closest word in terms of cosine similarity to a given word. If there are multiple words with the same highest similarity, return all of them.\n",
    "\n",
    "Hint: you can use the already-imported `cosine_similarity` function from sklearn to compute cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72bc3a66c2a8f550",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def closest_word(input_word, words_in_vocab, word_vectors):\n",
    "    '''Returns a list of the closest word or words to the input word, based on cosine similarities\n",
    "       of the word vectors given\n",
    "    \n",
    "    Parameters:\n",
    "        input_word (string): Search for the closest words to this word\n",
    "        words_in_vocab (list): Vocabulary associated with the vectors in word_vectors\n",
    "        word_vectors (np.array): Word vectors associated with the strings in words_in_vocab\n",
    "\n",
    "    Returns:\n",
    "        closest_words_list (list): List of strings containing the closest word or words to\n",
    "                                   input_word, based on cosine similarities of the word_vectors\n",
    "    '''\n",
    "    # YOUR CODE HERE    \n",
    "    input_ = vocab_strings.index(input_word)\n",
    "    # The variable input coords holds the coordenates necessary \n",
    "    # to understand the level of similiarity to other words \n",
    "    input_coords = full_vocab_reduced[input_].reshape(1,2)\n",
    "    \n",
    "    #Now I should be able to compute the level of similarity with other words\n",
    "    similarity_dictionary = {}\n",
    "\n",
    "    for i, word in enumerate(words_in_vocab):\n",
    "        if word != input_word:\n",
    "            similarity_dictionary[word] = cosine_similarity(input_coords,word_vectors[i].reshape(1,2))[0][0]\n",
    "            \n",
    "    max_val = np.max([n for n in similarity_dictionary.values()])       \n",
    "    list_of_words = []\n",
    "    for word,value in similarity_dictionary.items():\n",
    "        if value == max_val:\n",
    "            list_of_words.append(word)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d06a90ba7d38c797",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(hashlib.sha256(closest_word('nintendo', words_to_plot, coords_array)[0].encode()).hexdigest() == \"f711da60664c04c146d7a47b722c38a8d0bf46c3f52c2084c5c8d1cb78138e73\")\n",
    "assert(hashlib.sha256(closest_word('playing', words_to_plot, coords_array)[0].encode()).hexdigest() == \"435c149cbc6a5e5cc373cd33347d4c336a22160e06b7df61092b66e56f4d55ec\")\n",
    "assert(hashlib.sha256(closest_word('pineapple', words_to_plot, coords_array)[0].encode()).hexdigest() == \"6815f3c300383519de8e437497e2c3e97852fe8d717a5419d5aafb00cb43c494\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
